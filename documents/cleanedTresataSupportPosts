username, topic_title, raw
Bug: Unable to Query Singleton Value|Steps to Recreate above error

1. Go to Validate Step of a configured product on Pre-Prod Env
2. Under Query Builder, Click on "Data Asset"
3. Using the explore option create a query by adding a condition of Single equals false
4. Click on Apply and Executed Query
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
Output Full Walkthrough|![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)
Orchestrate Full Walkthrough|![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)
Enrich Full Walkthrough|![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)
Validate Full Walkthrough|![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)
Connect Full Walkthrough|![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)
Profile Full Walkthrough|![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)
Prepare Full Walkthrough|![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)
Dashboard Full Walkthrough|![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)
Source Full Walkthrough|![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
0.
6 Set Up A Namespace On Azure|## Azure Namespace Setup 

Connecting your data to our AKS cluster

![Set_Up_Namespace_On_Azure]()



PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your Azure account. 
2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. 

STEPS:

1. Note down the Namespace UUID from the namespace creation panel
2. Make sure that the storage account has "Hierarchical namespace Enabled". 
3. In the "Networking" Tab of your storage account, under "Firewall", add the address range: 0.0.0.0/0 and hit save. 
4. Create Custom Role:
    a. In the Azure Subscription, click on "Access Control (IAM)" and click on "Add" and then "Custom Role"
    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create


{
    "id": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,
    "properties": {
        "roleName": "synapse-custom-role",
        "assignableScopes": [
            "/subscriptions/<YOUR_SUBSCRIPTION_ID>"
        ],
        "permissions": [
            {
                "actions": [
                    "Microsoft.ManagedIdentity/userAssignedIdentities/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/read",
                    "Microsoft.Resources/subscriptions/resourceGroups/write",
                    "Microsoft.Synapse/workspaces/read",
                    "Microsoft.Synapse/workspaces/write",
                    "Microsoft.ManagedIdentity/userAssignedIdentities/assign/action",
                    "Microsoft.Synapse/workspaces/operationStatuses/read",
                    "Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action",
                    "Microsoft.Synapse/workspaces/operationResults/read"
                ],
                "notActions": [],
                "dataActions": [],
                "notDataActions": []
            }
        ]
    }
}
      


4. Create Managed Identity:
    a. Go to Managed Identity in the Azure portal and click on Create. 
    b. Give relevant values for the resource group, region, name, and tags and click on Create. 
5. Attach federated Credentials:
    a. Go to the newly created Managed Identity and select federated credentials
    b. Click on Add Credentials
    c. Select Scenario as "Kubernetes accessing Azure Resources"
    d. Cluster Issuer URL = "https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/"
    e. Copy the Namespace UUID noted down from the first step - 
         Namespace = "NAMESPACE_UUID"
    f. Service Account = "default"
    g. Subject Identifier will be auto-filled as "system:serviceaccount::<namespaceUUID>:default
    h. Give relevant names to the credentials
    i. Audience = "api://AzureADTokenExchange"
    j. Click on Add
6. Add Role Assignments:
    a. In that same managed identity go to the "Azure Role Assignment" and select add role assignment
    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = "Storage Blob Data Owner" and hit "Save"
    c. In that same managed identity, again go to the "Azure Role Assignment" and select Add Role Assignment
    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role ="synapse-custom-role" (This is the custom role you 
       created above. 
7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.
8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.
9. To give the permissions to access Synapse:
   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.
   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add 
       your user as the "Synapse Administrator". (This will give you the access to add/delete role assignments.
   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse 
       Administrator.
 d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity.
0.
5 Set Up A Namespace On AWS Using Access Key/ Secret Key|## AWS Namespace Setup - Using Access Keys and Secret Keys

![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 5.0

Connecting your data to our EKS cluster

PREREQUISITES:

1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:

1. Create an IAM user and attach S3 Bucket access to that user
    a. Go to the IAM service
    b. Navigate to the "Users" and click on "Create user"
    c. Give the appropriate User Name and click on Next
    d. From the "Permissions options" select Attach Policies Directly 
    e. Click on "Create Policy"
    f. Select JSON and paste the following in the "Policy Editor"

{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    g. Review and Create the Policy and attach it to the user created in the above step
    h. Navigate to the newly created user and click on the "Security Credentials" tab. 
    i. Click on "Create Access key" select "Command Line Access" and check on the confirmation button
    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.

---
|NOTE|: Substitute these values appropriately in the JSON
| S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.

| NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.
---
0.
4 Set Up A Namespace On AWS Using Role ARN|# AWS ROLE ACCESS

![Set_Up_Namespace_On_AWS_Using_Role_ARN](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)

Screen: 4.0

Connecting your data to our EKS cluster (Recommended method)

PREREQUISITES:
1. You (ADMIN) have administrator-level privileges in your AWS account. 
2. You have an S3 Bucket where the data is stored

STEPS:
1. Note down the Namespace UUID from the namespace creation panel
2. In your AWS Account, create an Identity Provider:
    a. Go to the IAM service
    b. Navigate to the "Identity Providers" and choose "Create Provider"
    c. Select "OpenID Connect" as the provider type.
    d. Provider URL = "https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
    e. Click on Get thumbprint and AWS will generate this value. 
    f. Audience = "sts.amazonaws.com"
    g. Review the information and then click on "Add Provider"
3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data
    a. Go to the IAM service
    b. Navigate to the "Roles" and select "Create Role"
    c. Select Custom Trust Policy and add this JSON

    
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
      "Federated": "arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub": "system:serviceaccount:<NAMESPACE_UUID>:default",
                    "oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}

   - d. To attach the policy to the role, select "Create Policy" and add this JSON:


{
    "Statement": [
        {
            "Action": [
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<S3_BUCKET_NAME>"
        },
        {
            "Action": [
                "s3:GetObject",
                "s3:GetObjectTagging",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::<s3_BUCKET_NAME>/|"
        },
        {
            "Action": [
                "athena:StartQueryExecution",
                "athena:GetQueryExecution",
                "athena:GetQueryResults",
                "athena:StopQueryExecution",
                "athena:ListQueryExecutions",
                "athena:CreateWorkGroup",
                "athena:GetWorkGroup"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>"
            ]
        },
        {
            "Action": [
                "glue:GetTables",
                "glue:GetTable",
                "glue:CreateTable",
                "glue:UpdateTable"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/|"
            ]
        },
        {
            "Action": [
                "glue:CreateDatabase",
                "glue:GetDatabases",
                "glue:GetDatabase"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog",
                "arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>"
            ]
        }
    ],
    "Version": "2012-10-17"
}

|
    e. Give this policy an appropriate name and attach it to the role you are creating. 
    f. Give this role an appropriate name and click on "Create Role"
    g. Copy the RoleArn of this role and paste it under the roleArn box in the "Create a Namespace" page.

---
|NOTE|: You need to substitute these values appropriately in the JSON

| YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.

| NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. 
| YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. 
---
0.
3 Create a new Product|
Now that you have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.

|STEPS TO ADD NEW PRODUCT|

![Dashboard_Create_New_Product](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)
|Screen:3.0|

![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)
|Screen 3.1|

![Dashboard_Add_Name_Description](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)
|Screen: 3.2|

| For admin users, they can easily get started off with creating their product simply just clicking on the |Create Product| button as shown in the screen 3.0
| Next choose a category from the list above to select product type. You can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.
| After which a pop-up comes for you to provide a name for your product and a small description about it as shown in screen 3.1. After which it takes the you to the Sourcing Step.
| If you are not an admin, and just received link to sign up to the product, then you will have to wait for your admin to approve your namespace. Only approved users are allowed to create their products. So be sure to ping your admin if not approved yet.

---
|EDIT PRODUCT NAME|

![Dashboard_Edit_Product](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)
|Screen: 3.2|

In the Screen 3.2, as you can see the list of products that has been created in this particular account or you can say in the namespace. If you want to edit the name or the description of the product, you has to simply click on the |edit icon| as seen above.

![Dashboard_Naming_Best_Practice](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)
|Screen: 3.3|

The name of the product can not have any special symbols or product names can not be repeated in the given namespace.

---
|Go to Step in Product Listing|
 
![Dashboard_Product_Stage_Listing](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)
|Screen: 3.4.1|

![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)
|Screen: 3.4.2|

If the product has been created, and some steps of TRESATA has been finished. In that case, you can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button |GO| will take us to corresponding home-screen immediately 

----

|DELETE PRODUCT|

Products can be deleted easily from the Dashboard with the delete option that is present next to edit button. If you are an Admin, then you will get couple of extra features to handle the deleted products. 

More information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). 
Feel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!
0.
2 Admin Panel Actions|As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.

|MANAGE NAMESPACE|
 
Namespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  |Admin Panel| looks like. 

|Namespaces:| 

![Admin_Panel_Screen](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)
|Screen: 2.0|

This tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0

----

|Products:| 

![Admin Panel-Add_Product](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)
|Screen: 2.1.1|

![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)
|Screen: 2.1.2|

Only admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.

|Note|: 
Only one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. 

|PRODUCT ACTIONS|
| |Edit:|

![Admin Panel_Edit_Product](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)
|Screen: 2.2|

Admin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. 

There will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!

| |Delete:|

![Admin Panel_Delete_Product](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)
|Screen: 2.3|

Admin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3

![Admin Panel_Select_Multiple_product](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)
|Screen: 2.4.1|

![Admin Panel_Delete _All](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)
|Screen: 2.4.2|

Admin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2

----
|Users:| 

![Admin Panel_Add_Users](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)
|Screen: 2.5|

Only Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button |Add Users| as seen in Screen 2.5

![Admin Panel_Add_Recipients_email](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)
|Screen:2.6|

Admin then has to add the email of the list of user they wish to add in the namespace

![Admin Panel_Users_List](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)
|Screen: 2.7.1|

![Admin Panel_Invites_Sent](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)
|Screen: 2.7.2|

As shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.

|IMPORTANT NOTE|

1. When user accepts the invite, the status of the User turns from red tag |Pending| to  green tag |Accepted|
2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA

----

|CONSUMPTION|


![image|690x325](upload://orjOxekPrmDxvRCkFEatLIDkdhE.png)

|Screen: 2.8|

Admins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.

----
|DELETED PRODUCTS|

![Admin Panel_Deleted_Products_list](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)
|Screen: 2.9|

This feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.

![Admin Panel_Restore_Option](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)
|Screen: 2.10.1|

![Admin Panel_Delete_Forever_Option](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)
|Screen: 2.10.2|

The users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called |Delete Forever| to get them out of your namespace completely.

![Admin Panel_Mass_Action](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)
|Screen: 2.11|

The above screen the representation of how we can restore or delete multiple deleted products.

This is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. 

So stay Tuned...!!
0.
1 Sign Up for TRESATA||Introduction|

Welcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for you all to unlock the power of data to enrich life.

But before we dive in, let's start with the first step which is signing up to our product |TRESATA|. This post is aimed to help you to get started with the seamless sign up process.

|Steps To Sign Up:|

YOU will be able to join TRESATA product from the Tresata website, with the three options to get started, which are :
|Try for Free|, |Upload| and |Access|

![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)
 |Screen: 1.0|

Click on |Try For Free|, if you want to use our product and playaround using pre uploaded tresata files. 
Click on |Upload|, if you want to upload your files in Tresata's storage account and get started with the product.
Click on |Access|, where you can link your cloud storage with the Tresata.

|Environment Selection:|

![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)
|Screen: 1.1|

You are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, you can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made

|User Information:|

The next step is to complete sign up process by providing essential information:


![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)
|Screen: 1.2|

| Name
| Email
| Password (with specific criteria)
| Confirm Password
| Checkbox to agree to our terms and conditions.
| You can review our terms of service by clicking on the provided link.

|Verification Email:|

![image|676x500](upload://g4eE8Mubn5AkEfhWwEzidb1M869.png)

|Screen: 1.3|

Once you hit the sign-up button, you will receive an email with a verification code along with a login link. This is for two-factor authentication to confirm your account.

|Setting Up Your Namespace:|

After confirming your account, you'll need to set up a "namespace." The process differs depending on your chosen environment:

|AWS|: 

![AWS_Namespace](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)
|Screen: 1.4|

You'll be prompted to provide:

| Namespace alias name: Need to be unique and also an error message will guide you if it already exists

| Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.

| Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:

| Access Key: This is a unique identifier used to authenticate with AWS services.
| Secret Key: This key is used as part of the authentication process along with the access key.

| Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:

| Bucket Name: The name of the AWS S3 bucket.

| For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)

|Azure|: 

![Azure_Namespace_Setup](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)
|Screen: 1.5|

You'll be prompted to provide:

| Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.

| Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.

| Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:

| Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.

| Tenant ID: This is the identifier for your Azure Active Directory tenant.

| Storage Account Name: The name of the Azure Storage Account where your data sources are stored.

| Container Name: The name of the container within the Storage Account where your data resides.

|  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)

|Logging In:|

![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)
|Screen: 1.6|

Once user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:

| Provide users registered email and password to log in.
| If you've forgotten your password, we offer a password recovery option.
| New users who haven't signed up can use the provided link to join the TRESATA platform.
| Once you log in, you can start exploring and utilising the TRESATA product.

|Forgot Password|

![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)
|Screen: 1.7.1|

![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)
|Screen: 1.7.2|

Forgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then you will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!
About the Admin category|This category consists of all the relevant steps for getting you started on using our product TRESATA

| https://community.tresata.com/t/0-1-sign-up-for-tresata/565?u=tresatasupport
| https://community.tresata.com/t/0-2-admin-panel-actions/566?u=tresatasupport
| https://community.tresata.com/t/0-3-create-a-new-product/571?u=tresatasupport
| https://community.tresata.com/t/0-4-set-up-a-namespace-on-aws-using-role-arn/584?u=tresatasupport
| https://community.tresata.com/t/0-5-set-up-a-namespace-on-aws-using-access-key-secret-key/585
| https://community.tresata.com/t/0-6-set-up-a-namespace-on-azure/586?u=tresatasupport
| https://community.tresata.com/t/dashboard-full-walkthrough/667?u=tresatasupport
6.
2 Final Output||Getting Started with Output|

![Enrich_Output_Home_Screen](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)
|Screen: 2.0|

As shown in Screen 2.0, from the list of Output sources, users are given two choices to select their Output data

1. |Universal (Per Data Source)|:

![Enrich_Universal_Output](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)
|Screen: 2.1|

The Screen 2.1 shows the usage of Universal Output. In this method, users are given the option to customize the output schema for every data source. 

| |Raw Data| is your initial sourced data
| |Cleaned Data| is the data after we finished data cleaning in Prepare Step
| |Enriched Data| is the data after applying optimal rules to get Golden Records

With the above 3 options, users can select the required schema simply by clicking on the checkbox next to it as seen in Screen 2.1. The schema is source dependent and different schema can be selected for every data source

2. |Enriched|:

![Enrich_Enriched_Output](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)
|Screen: 2.2|

As seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that one output schema will be generated for all the Data Sources forming a standardized global output. And Raw Data fields can not be considered for Enriched Output. 

Users can select any schema that they wish to view output for and click on the button |Initiate Output| to get your Final Output.
6.
1 Enrich & Its Significance|One of Tresata's core capabilities is Enrich, allowing you to choose the best, most accurate values to create a |golden record| for each of your TresataIDs as well as tailor the output files to best feed downstream usage.

This post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.

|GOLDEN RECORD|

Having assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using the Enrich feature.

|STEPS TO SELECT YOUR ENRICH FIELDS|

![CLick_On_Initiate_Enrich](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)
|Screen: 1.0|

Before we start to choose fields for our Golden Record, it's important to shed light on a few sets of rules upon which the best fields can be selected by the user. In the Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.


1.|Field Preferences|:

![Enrich_Select_Canonical_Field](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)
|Screen: 1.1|

This category will allow you to select the best value for a field to populate your Golden Record.  To use this rule, the user has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. 

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.2|

| |Expression:| As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:

|While gathering information for all the bookings by |


Taking the example of |FunAirways| from before,  Prefer First name on the expression of Create Date. So the system will set Best First Name based on the latest Create Date value.

![Enrich_Count_Preference](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)
|Screen: 1.3|

| Count: This option allows the user to select a particular field on the basis of the most frequent value. A counting algorithm is run in the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. 
---
2.|Table Preferences|: 

 As the name suggests this category is used by users to choose a source as the best information for the Golden Record. We can further see 2 ways to choose best dataset.

![Enrich_Prefer_Data_Set](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)
|Screen: 1.4|

| |Prefer:| Selecting Prefer Rule is simply telling Tresata, that the selected data source is the most trusted source. Now our algorithm will assign high scores on the preferred sources and choos its field values for all the canonicals from that preference.  

![Enrich_Rank_Records](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)
|Screen: 1.5|

|  |Rank:| Just giving a preferred data source might not be enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose Rank as a criteria to sort their best data, so that those fields get prioritized. In the Screen 1.5, as you can see, users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it.

---
![Enriched_Profile_Heat_Map](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)
|Screen: 1.6|

To make sure you are applying the right logic to select your Golden Record, users can also make use of the profiled Heat map to understand data quality and get accurate canonicals for enrichment.

![Enrich_Initiate](upload://l6T637D8IlyyiQBodf80uWebED0.png)
|Screen: 1.7|

When user finalizes their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules that suites their use case. Users can also edit and delete rules before clicking on the button |"Initiate Enrich"|

This will now create Optimal Output values that contain Golden Records with the best attributes.
In the next post, let's view the final transformed data.
5.
2 How To Validate Your Results||| 

In the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.

That's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.

|OVERALL STATISTICS|

As discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the |Data Products Statistics| panel (|screen 1.0 below|).

![Screen Shot 2023-08-20 at 3.34.18 PM](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)
|Screen 1.0|

As shown above, those statistics include:

| # of Tresata IDs (TIDs)
| # of Records
| # of Data Sources
| Avg. # of Sources per TID
| Avg. # of Records per TID
| Singletons
| Spanning
| Trapped

All the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?

To begin with, the overall ||# of TresataIDs|| can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing ("loose" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:

| |As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify |how many clients| have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).|

| |As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...|

Similar way of thinking should be applied to statistics like the |Avg # of Records mer TIDS|. However, statistics like |Singletons, Trapped| and |Spanning| have some more interesting insights. As explained on the last post

| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).

As it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:

| |As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my |booking| and |ticketing| data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).| 

|DATA INTEGRITY CHECK|

All the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.

Now, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.

|CROSS-DATA SOURCE OVERLAPS|

Another interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. 

Source overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:

![Screen Shot 2023-08-20 at 4.31.41 PM](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)
|Screen 1.1|

On the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ||finance|| and ||esg||. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem.
5.
1 Validate & Its Significance|At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:

| Explain what is the Validate step and why it is important for an accurate Data Product
| List and briefly describe its most important features

Let's dive right in...

|WHAT IS VALIDATE|

Validate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a |csv| export report. 

All the above are interesting, but...

|WHY IS VALIDATE IMPORTANT|

Accurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. 

 |MOST IMPORTANT FEATURES|

Tresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:

| |Overcompression|: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by "loose" Connect logic, where the user isn't strict enough on what should link two records together. 
| |Undercompression|: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. 
| |TresataID|: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. 
| |Singletons|: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. 
| |Trapped|: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. 
| |Spanning|: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). 

Having explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:

| |Overview stats|: will include statistics such as |# of tresataIds|, |# of Records|, |# of Data Sources|, |Avg. # of Sources per TID|, |Avg. # of Records per TID|, |Singletons|, |Spanning|, |Trapped|. 
| |Counts per Data Source|: will include most of the metrics above for each data source, to provide an extra layer of granualarity.
| |Cross-Data Source Overlaps|: will include |Data Source Combinations|, |# of Records| and |% of Records Overlapped|. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. 

The above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!
3.
3 Commonly used Regular Expressions||SUMMARY|

Regular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality


|LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS|
|Name|Regex Expression|Description|Example|Output
|---|---|---|--|--
|Accept Numbers | ^[0-9]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345
||||ab123| None
Exclude Special Characters|^[A-Za-z0-9]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123
||||hello@me| None
Accept Valid Email ID|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$| [A-Za-z0-9_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com
||||user@gamil|None
Phone Number Validator|^[+]{1}(?:[0-9-()/.]s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456
||||+1 7875 1234 | None
Credit Card Validation|^(d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345
||||12345  | None
Alpha Numerals||^[a-zA-Z0-9]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123
||||johnjacob|&1  | None
Accept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-|/%&]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123
 Scrub first 4 characters in a string|^[sS]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE
||||123456 | 1234
Scrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks
||||123456 | 456
Scrub only first name from Full Name|^[^,-. ]||This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex
Scrub only last name from Full Name|(?<=[,-. ]).||This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano
ZIP code Validator|^d{5}[-s]?(?:d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789
||||1234| None
SSN Number Validator|^d{3}-d{2}-d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789
||||1234| None
Accept Valid Date|MM/DD/YYYY format - ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)d{2}$| The pattern  (0[1-9]|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003
||DD/MM/YYYY format - ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)d{2}$|( 0[1-9]|[12][0-9]|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None
||YYYY/MM/DD format - ^(19|20)d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$|(19|20)d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22

---
|CHEAT - SHEET|
|REGEX|DESCRIPTION|
|---|---|
|  .| Matches Any Character
|d|  Digit (0–9)
|D| Not a digit (0–9)
| w | Word Character (a-z, A-Z, 0–9, _)
| W| Not a word character
| s| White space (space, tab, newline)
| S| Not white space (space, tab, newline)
| b| Word Boundary
| B| Not a word boundary
| ^| Beginning of a string
 $| End of a String
 [  ]| matches characters or brackets
 [^ ]| matches characters Not in brackets 14. | = Either Or
 ( )|  Group
 ||  0 or more
 +| 1 or more
 ?|  Yes or No
 {x}|  Exact Number
{x, y}| Range of Numbers (Maximum, Minimum)
4.
4 Executing Connect Step|||

Let's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.

In this post, we shall look into the criteria on which the selected fields are coming along together.

|Configure Connector Type|

![Connect_Configure_Connector_Type](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)
|Connect: 4.0|

Now that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.
Yes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.

1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider |null| values.

2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.

3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.

4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.

Users can choose one of the Matcher type from the above mentioned options to match their records.

|Initiate Connect|

![Initiate_Connect](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)
|Connect: 4.1|

We have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.

This concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.

Cheers..!!!
4.
3 Configure Connect Logic|Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.

|CREATE CONNECT CONFIGURATION|

The first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:
| |Resolve|: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a |Tresata Id|.
| |Against|: Directional linkage, it connects one or more records from the selected sources on the "From" directory to at most one from the "To" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.

The choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the |resolve| step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the |against| step and specify |"from"| as the transactional sources and |"to"| as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.

|Source Selection Process|

 In |screen 3.0| the source selection process is shown for a |resolve| step. 

||PRO TIP||: |Remember, the |against| step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.|

![Connect_Add_Data](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)
|Connect: 3.0|

As shown in |screen 3.0|, the user can select the required dataset by clicking on the |(+)| in the |"Add Data Sources"| section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for |resolve|, two for |against| - |from| and |to|) the rest of the configuration panel is in a disabled state.

-------------------------------
|Creating Connect Logic|

Upon selecting data sources, the canonicals included to |at least one| of the selected sources gets enabled on the |Canonicals| right panel. The panel includes:

| ||Selected Sources (#)||: The canonicals included in |at least one| of the selected sources for this step.
| ||Others (#)||: The canonicals not included in any of the selected sources for this step.
| ||Color Coded Indication||: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In |screen 3.0| the source selection process is shown for a |resolve| step.

Hovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.

||PRO TIP||: |Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.|


![Connect_Analyse_Canonicals](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)
|Connect: 3.1|

--------------------------------------------------------------------

Now, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. 

The CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:

| ||Connecting people||: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.
| ||Companies||: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. 

So for the first case, |name+phone| can identify a unique individual and so does |name+email| or just |passport id|. In the second case, |name+address| can identify an individual company and so does |name+date_of_incorporation| or just |tax_code|. So there can be multiple such combinations which identify a unique entity. This is where the |Add Logic| option is used to add as any such combinations as possible to identify more and more connections. 

Of course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? 

As shown in |screen 3.2|, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. 


| |Add Fields to Resolve:|

![Connect_Add_Fields_for_Resolution](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)
|Connect: 3.2|

As we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.

| |Parentheses Action:|

![Connect_Parentheses_Action:](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)
|Connect: 3.3.1|

![Connect_Parentheses_Action:](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)
|Connect: 3.3.2|

Adding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.

The above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.

| |Adding another Step of Resolution:|

![Connect_Add_Step_2](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)
|Connect: 3.4|

Click on the |Plus button| to add a new step as shown in the screen 3.4

![Connect_Against_Method](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)
|Connect: 3.5|

Let's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.


---------------------------------------------------------------------
| |Using Shields|
As explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), |Shields| are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:

Shields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection |should not| be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what |should| match, Shields define what |should not| match.

A user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If |any| of those fields is inconsistent, then the connection will not be formed. 

||PRO TIP||: |Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on |Others(#)| the user can find the rest of the canonicals and still use them as Shields in their configuration.|

![Connect_Add_Shield_Fields](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)
|Connect: 3.6|

If the user attempts to proceed to |Build Pipeline| without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.

![Connect_Shield_Pop_Up](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)
|Connect: 3.7|
4.
2 Getting Started with Connect|||

In the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!

|CONNECT|

Starting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.
![Connect_HeatMap_Button](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)
|Connect: 2.0|

![Connect_HeatMap_View](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)
|Connect: 2.1|

As shown in screen 2.0, click on the "View HeatMap" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.

![Connect_Screen_Highlights](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)
|Connect: 2.2|

As seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:

1.  |Resolve:|  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.

| |For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  "|Resolve|" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.|

2. |Against:| This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as "||from||") to at most on record of another pool of source(s) (mentioned as "||against||"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.

| |After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ||against|| resolution step.|

3. |Steps:| CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.

4. |Add Data Sources:| Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. 

5. |Canonicals:| Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. 

6. |Add Resolution Logic:| That is the place where the connect logic is specified. As should in |screen 2.2|, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. 

7. |Add Link SHIELDS:| In order to avoid incorrect connections, Tresata allows the user to define "shields", i.e. fields that if they have different values, the connection is incorrect and should be broken. 

| |Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use |name, email, phone, and citizenship|. According to that logic, one passenger, John Doe, seems to have connected with the list of |flagged| people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a |SHIELD| would prevent that connection and improve the accuracy of the CONNECT step.|

8. |Build Pipeline:| To proceed in CONNECT, the user has to first click on the "|Build Pipeline|"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.


With this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!
4.
1 Connect & its Significance||SUMMARY|

After the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. 

|Seems simple, right? Not quite...|

When dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. 

But why is this important and how does this help businesses grow? This post will aim to answer those two questions!

|WHAT IS CONNECT|

To better understand the power of CONNECT, it may be easier to describe the problem using a real world example:

Helen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). 

That amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. 

Now, let's make this problem even |more| difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the 

How can FunAirways:

| Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?
| Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?

CONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.

|WHY IS CONNECT IMPORTANT|

By creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back.
7.
2 Monitoring Orchestrated Workflow||SUMMARY|

As we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.

|USER ACTIONS|

![Orchestrate_Progress Bar](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)
|Orchestrate: 2.0|

| |Progress Bar:|
 Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0

![Orchestrate_Restart_Workflow](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)
|Orchestrate: 2.1|

![Orchestrate_Restart_Workflow_Approve](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)
|Orchestrate: 2.2|

| |Restart Workflow:|
When there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2
 

![Orchestrate_Cancel_Workflow](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)
|Orchestrate: 2.3|

| |Cancel Workflow:|
 Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.

| |Start, Edit and Delete Methods|

![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)
|Orchestrate: 2.4|

1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4

![Orchestrate_Edit_Workflow](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)
|Orchestrate: 2.5|

2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5

![Orchestrate_Delete_Workflow](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)
|Orchestrate: 2.6|

3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.

|THINGS TO REMEMBER IN ORCHESTRATION|

There are few important points to remember while setting up the workflow to orchestrate

| The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.
|  The name of the workflow can not have any special characters or can not have duplicated names.
| Scheduled workflow can be changed and deleted at any point of time.

Finally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue..
7.
1 Workflow Orchestration|Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post

|ORCHESTRATE THE WORKFLOW|

After creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on |Automate Workflow| button to get started with Orchestration

![Orchestrate_Automate_Workflow](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)
|Orchestrate: 1.0|

There are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.

| MANUAL ORCHESTRATION

![Orchestrate_Manual_Method](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)
|Orchestrate: 1.1|

The screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.

Now select the schedule type to be |Manual| and then clicking on |Confirm & Orchestrate| button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.

| AUTOMATIC ORCHESTRATION

Unlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.

![Orchestrate_Automatic_Method](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)
|Orchestrate:1.2|

Selecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. 

1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met
2. Time - Users can set hours and minutes at which they want to execute the workflow.
3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically
4. User has to be sure to know the steps that they want to run automatically.

Just like we confirmed in Manual method, go ahead and click on  |Confirm & Orchestrate| to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.

The more information about monitoring the scheduled runs will discussed in the next posts...!! 
Stay Tuned..!!
3.
2 Cleaner Actions||SUMMARY|

Once all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). 

Now, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.

|TYPES OF CLEANERS|

There are two standard types of cleaner that TRESATA offers as listen below.

| |GLOBAL CLEANER|:

![Prepare_Global_Cleaner](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 2.0|

 As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.

| |FILED SPECIFIC CLEANER|

![Prepare_Individual_Cleaners ](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)
 |Prepare: 2.1|

TRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier, our SAM suggests a few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit the "APPLY" button so that the selected cleaners will be applied automatically for the chosen canonical fields. 

|CUSTOM CLEAN YOUR FIELDS|

First, choose the field that should be cleaned from the list of canonical fields on the left. 

![Prepare_Select_Canonical_Field](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)
|Prepare: 2.2|

From the list of available cleaners, select the most appropriate based on the nature of your canonical. 

|TIP:| Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. 

Once you decide on a cleaner, just click on, "Add Cleaner To Field" to apply it to the canonical.

![Prepare_Add_Cleaner](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)
|Prepare: 2.3|

If you wish to edit the cleaner or add additional cleaners to the same field, simply use the "Edit" option as shown below. You can also view all the cleaners that were applied by clicking on the radio button "View Applied Cleaners" and check how the cleaners work together by providing a sample test string. Additionally, if a cleaner is no longer required, you are able to use this same option to delete the unwanted cleaner.

Each time a cleaner is applied successfully, a notification will appear on the top of the screen. Once you feel happy about all the cleaned fields, you can click on "|Mark Complete|" so that you can move to the next data source for cleaning messy data.

![Prepare_Edit_Cleaner](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)
|Prepare: 2.4|

As you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the "View Applied Cleaners" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied to a field. Additionally, use the "View Heatmap" button on the top left to get a view of your tagged fields across sources. When you are happy about the cleaners used, remember to click |Mark Complete| to make sure the cleaning has been completed for the selected Data source. 

![Prepare_View_All_Cleaners](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)
|Prepare: 2.5|

Check out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!

The final step of Prepare is to click on |Initiate Data Preparation| to apply all the cleaning that you just selected.
3.
1 Prepare & its Significance||SUMMARY|

After using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. 

By cleaning and standardising the data, you enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.

|CANONICAL FIELDS|

Canonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).

For example, let's say you have two sources (Table-1 and Table-2). 

| Table-1 contains the raw field name "|Full_Name|", representing customer names 
| Table-2 contains the raw field name "|Cust_Name|",  representing customer names

Since both fields represent the same attribute, you would tag each field as ||name||, resulting in the creation of a single, universal canonical field called "name" (refer to the Profile step for further information on tagging). 

||Note:|| The tags you assigned in the Profile step are |automatically populated as canonical fields| during the Prepare step. 

---
|GLOBAL CLEANERS|
![Prepare_Global_Cleaner_By_SAM](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)
|Prepare: 1.0|

In Screen 1.0, you can see an example of the canonical list for the data source |boromir-gb-1|. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, you can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once you have confirmed the canonical list is complete, they will dive deeper into field-level cleaning.

The next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!
2.
5 Profile Heatmap|Heatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by you, making it a one place destination to have complete view of your data.

|REVIEW HEATMAP|

![Profile_Review_Heatmap](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)
|Profile: 5.0|

When you have completed tagging their critical fields, you will get access to Heatmap.
As Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. 

As shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap.

![Profile_Heatmap_view](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)
|Profile: 5.1|

Screen 5.1 shows the resultant Heatmap that is created for the profiled data sets.
| |List of Tags:| List of Tags tab shows all the tags that was assigned by you. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. 
| |Sources:| In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated.
| |Field Names:| This is the raw names of the fields that was profiled.
| |# of Uniques:| Shows the total count of unique values in the field. This details will help to identify the diversity of the data.
| |% of Populated:| This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. 
More populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population.

|HEAT MAP ACTIONS|

![Profile_Heatmap_Sort_Filter_action](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)
|Profile: 5.2|

| The Heatmap allows you to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.
| Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made.
| The lock button is used to make any data source to remain unchanged.  
| Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.
| After making the changes, simply click on Apply button to update the heatmap.

|HEATMAP HOVER ACTIONS|

![Profile_Heatmap_Hover](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)
|Profile: 5.3|

You can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag

![Profile_Heatmap_Hover_FiledNames](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)
|Profile: 5.4|

When hovered on the name of the field, you can view the Top values with the total percentage of that value present in the data.

![Profile_Heatmap_Hover_FiledNames](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)
|Profile: 5.5|

In addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.

![Profile_Heatmap_Hover_Population](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)
|Profile: 5.6|

Screen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code

|NEXT STEP|
When you are happy about the over all heatmap, you will be able to export them to a csv in just one click for further reference of their holistic profiled data.

This brings us to the end of Profiling step. If you are experiencing any problems, reach out to our community team or post your quetsions here on [Tresata Support](https://community.tresata.com/c/tresata-product-support/94)
2.
4 Add Tags to Fields|After a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on your data. In order to do that, TRESATA offers a special feature known as "Tags." These Tags are essentially keywords or labels that you can assign to critical fields, such as name, email, phone, ID, and more.

When these Tags are assigned, they serve as canonical fields, enabling you to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.

|ADDING NEW TAG|

![Profile_Add_New_Tag](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 4.0|

As shown on screen 4.0 above, to all the critical fields, you can easily add a new tag simply by clicking on the add button shown at the bottom of the screen

![Profile_Save_Tag](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)
|Profile: 4.1|

When you have finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1

![Profile_Edit_Tag](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)
|Profile: 4.2|

![Profile_Delete_Tag](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)
|Profile: 4.3|

The screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, you can also remove the tags when found otherwise.

|Best Practices To Add a Tag|

![Profile_Tags_with_no_special_characters](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)
|Profile 4.4|

| |No Special Characters|: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names 

| |Only One tag per Field|: Each field can have only one Tag assigned to it.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

2.
3 Field Level Investigation|Looking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit your need and requirements. To do all that, you will want to have the understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling you to get a holistic understanding of the data.

|LEFT SIDE PANEL|

![Profile_field_landing_correct](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)
|Profile: 3.0|

As shown on screen 3.0 above, in order for you to investigate a source per field, you need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:

| |Tags|: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this |crucial| tagging step ||here||. 
| |% Populated|: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.
| |# of Uniques|: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.

NOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).

Usually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ||here||.

While those metrics are good for high level field understanding, there still could be problems that you can't identify unless seeing the actual values within a field. Imagine a scenario where a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows you to go |even more granular| to the actual values within the fields. 

---
|FIELD LEVEL INFORMATION|

As shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.1|

Looking at the value based information, you can see:

| |Top Values|: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).
| |Top Patterns|: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).
| |Top Formats|: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. 

Knowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, you can see the actual percentage those values hold within the data. Additionally, you can in real-time filter those values out of your data to identify how that would affect the overall statistics of that field as shown below:

![Profile_field_unselect](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)
|Profile: 3.2|

![Profile_Field_Level_Hover_Over](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)
|Profile: 3.3|

Investigating that middle panel is a |very important step for a successful TRESATA workflow|, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process you can identify Data Quality problems, trend and more.

---

|SAM : SIMPLE AUGMENTED MODE|

Tresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When you land on the profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.

![Profile_SAM_Tag_Suggestion](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)
|Profile: 3.4|

The screen above is the representation on how SAM is suggesting most critical fields, where you can tag them from your raw data. Additionally, completely understanding the data is important for |tagging|, which is what powers the remaining TRESATA steps and will be explained ||[here](https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport)||.
2.
2 Profiled Overview Statistics|The first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables you to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.

|PROFILE LANDING STATISTICS|

![Profile_Overview_Statistics, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)
|Profile: 2.0|

As you proceed from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:

| All of your sources are successfully profiled and the left "Profiled Data Sources" reflects all of them.
| Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.

Notice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of |everything| that has been processed so far, across all the completed sources. Available metrics:

| Number of Sources (Tables)
| Number of Records (Rounded Up)
| Number of Fields (Columns) 

By looking at those statistics, you can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.

![Profile_Overview_Per_Source](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)
|Profile: 2.1|

Notice that to get to that view, you have enabled the "Selected All" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:
| |Number of Records|: Records (rows) for this source.
| |Number of Columns|: Columns (fields) for this source.
| |Path|: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the "Copy to Clipboard" functionality is present.
| |Profiled On|: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.

To enable you quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, you can see a list of all your sources (the ones selected from the left panel) and decide which one they want to investigate.

Looking at those statistics on a source level makes it easier to identify |if| anything is missing and |where| it is missing from. Once you identify that nothing is missing, you can proceed to have an even more granular investigation of the data, on a field level (checkout [this](https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport) post to learn more).
2.
1 Profile: What, Why & How|PROFILE is a crucial step in the flow. In this post, we will identify:

| |WHAT| is the PROFILE step
| |WHY| is it important
| |HOW| should it be interpreted

|WHAT|

Profiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:

1. |Structure|: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). 
2. |Content|: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. 

|WHY|

Everything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? 

Gaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder "bad" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:

||SCENARIO|: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value "test" for customer_name field and "test@airline.com" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?|

For the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, you should identify that:

| There is an unusual 4-letter pattern for full name
| Among the top values for email, test@airline.com is there and needs to get cleaned

With that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.

|HOW|

So, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?

For you, the most powerful way to use PROFILE output is:

| Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.
| Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (|"Notes" icon on top right of your screen|) in order to not lose those findings. 
| Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (|PRO TIP:| There are some pre-configured common tags at your disposal to use). tagging is |essential| for a successful workflow
| Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps.
1.
3 Add Source And Continue To Profile||SUMMARY|

At this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.

|ADDING A SOURCE TO CART|

To add a source to the Cart, the user has to complete the whole journey of:

| Choosing a format and checking the schema
| Checking at least one field for the First 10 values

Once both those actions are done, the |"Add To Selected Data Sources"| button will be enabled for the user to use as shown below:

![SOURCE-values-filled](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)
|Screen: 3.0|

Clicking on the |"Add To Selected Data Sources"| button, will reflect on the counter seen next to the "Continue to Profile", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:

![SOURCE-CART-remove-one](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)
|Screen: 3.1|

While in the cart, the user has the ability to perform specific actions:

| |Individual removal|:
If the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.

![SOURCE-remove-one-messages](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)
|Screen 3.2|

| |Mass removal|:
When the user click on the |Select All| check box besides |"Data Source(s)"|, a "|Remove Selected|" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.

![SOURCE-remove-all-button](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)
|Screen: 3.3|

![SOURCE-remove-all-pop-up](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)
|Screen: 3.4|

| |Continue to PROFILE|:

When selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).

![SOURCE-proceed-profile](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)
|Screen: 3.5| 

![PROFILE-job-status](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)
|Screen: 3.6|

That concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!
1.
2 Get Information About Your Source||SUMMARY|

Looking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:

| Easy access to the source schema to verify the fields included
| Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!

But how to do all that?

|GET SOURCE SCHEMA|

In Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:

![Sourcing-nested-expanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Screen: 2.0|

Notice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the |APPLY| button. The list of the available formats are:

|Formats|: |parq|, |csv|, |csvu|, |bsv|, |bsvu|, |tldsv|, |tldsvu|, |casv|, |casvu|, |avro|, |json|, |orc|

The user can use the applicable to their source format as shown below:
![SOURCE-schema-apply](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)
|Screen: 2.1|

Once the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:

![SOURCE-schema-fields](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)
|Screen: 2.2|

|NOTE|: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.

This is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...

|GET FIRST TEN|

Now the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.

To check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:

![SOURCE-search-selection](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)
|Screen: 2.3|

Notice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now |APPLY| the selection and the screen will look like this:

![SOURCE-values-filled](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)
|Screen: 2.4|

At this point, the user can take multiple actions:

| |Unselect a field|: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the |APPLY| button is clicked. The user can now pick another field(s).
| |RESET|: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the |APPLY| button is clicked. 
| |Expand|: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.

![SOURCE-values-expanded](upload://ddyPaynyQveShjQB9p4564tUW1D.png)
|Screen: 2.5|

At this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the |Continue to Profile| flow.
1.
1 Navigate & Select Your Source||SUMMARY|

Navigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. 

TRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.

In the following post we'll give a walk through of how TRESATA enables the user to:

| View all objects (folders and files) existing on a user's back-end
| Navigate quickly through a given file structure 
| Search a directory for a file path
| Star important files and selectively access them


|VIEW FILE SYSTEM|

![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)
|Sourcing: 1.0|

Looking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: |"usecases"|, |"tresata-generated"|, |"user"|, |"ascii-directory"|, |"tmp"| and |"demos"|.  

Further breaking down the left side |"Objects"| panel:

1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.
2. Each directory will have an |expand| indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)

|NAVIGATE FILE SYSTEM|

Once the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. 

![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)
![SOURCE-no-sources-added](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)
|Sourcing: 1.1|

In Screen 1.1, we can see that |usecases| has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. 

![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)
![Sourcing-nested-epanded-final](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)
|Sourcing: 1.2|

|Tip:| To return to the structure where |usecases| is shown as the parent directory, the user has to click to "|Go to usecases|" and the subdirectories under |usecases| will be displayed.

Given the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. 

![SOURCE-truncated](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)
|Sourcing: 1.3|

Aside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.

![Source-clipboard](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)
|Sourcing: 1.4|


|ADVANCED SEARCH|

TRESATA has a universal search box with a "Search using file path" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side |Objects| panel.

|NOTE|: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.

|STAR RELEVANT FILES / DIRECTORIES| 

Users may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.

To star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.

Now for displaying only starred files, click on the toggle next to |"Objects"| that has "Show Starred only" written beside it as shown on screen 1.6.
About the Profile category|This section contains the documents on the Profile step of the TRESATA. 

| https://community.tresata.com/t/2-1-profile-what-why-how/498?u=tresatasupport
| https://community.tresata.com/t/2-2-profiled-overview-statistics/503?u=tresatasupport
| https://community.tresata.com/t/2-3-field-level-investigation/504?u=tresatasupport
| https://community.tresata.com/t/2-4-add-tags-to-fields/521?u=tresatasupport
| https://community.tresata.com/t/2-5-profile-heatmap/527?u=tresatasupport
| https://community.tresata.com/t/profile-full-walkthrough/669?u=tresatasupport
About the Source category|This category consists of all the relevant documentation that is required to complete the |"Source"| step in the process of creating a data product. This includes:

1.1 Navigate & Select Your Source
&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end
&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure
&nbsp;&nbsp;1.1.3 Search a directory for a file path
&nbsp;&nbsp;1.1.4 Star important files and selectively access them

1.2 Get Information About Your Source
&nbsp;&nbsp;1.2.1 Get Source Schema
&nbsp;&nbsp;1.2.2 Get First Ten Records

1.3 Add Source And Continue To Profile
About the Tresata category|(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)

Use the following paragraphs for a longer description, or to establish category guidelines or rules:

- Why should people use this category? What is it for?

- How exactly is this different than the other categories we already have?

- What should topics in this category generally contain?

- Do we need this category? Can we merge with another category, or subcategory?

GUIDELINES|<a name="civilized"></a>

## [This is a Civilized Place for Public Discussion](#civilized)

Please treat this discussion forum with the same respect you would a public park. We, too, are a shared community resource &mdash; a place to share skills, knowledge and interests through ongoing conversation.

These are not hard and fast rules. They are guidelines to aid the human judgment of our community and keep this a kind, friendly place for civilized public discourse.

<a name="improve"></a>

## [Improve the Discussion](#improve)

Help us make this a great place for discussion by always adding something positive to the discussion, however small. If you are not sure your post adds to the conversation, think over what you want to say and try again later.

One way to improve the discussion is by discovering ones that are already happening. Spend time browsing the topics here before replying or starting your own, and you’ll have a better chance of meeting others who share your interests.

The topics discussed here matter to us, and we want you to act as if they matter to you, too. Be respectful of the topics and the people discussing them, even if you disagree with some of what is being said.

<a name="agreeable"></a>

## [Be Agreeable, Even When You Disagree](#agreeable)

You may wish to respond by disagreeing. That’s fine. But remember to _criticize ideas, not people_. Please avoid:

| Name-calling
| Ad hominem attacks
| Responding to a post’s tone instead of its actual content
| Knee-jerk contradiction

Instead, provide thoughtful insights that improve the conversation.

<a name="participate"></a>

## [Your Participation Counts](#participate)

The conversations we have here set the tone for every new arrival. Help us influence the future of this community by choosing to engage in discussions that make this forum an interesting place to be &mdash; and avoiding those that do not.

Discourse provides tools that enable the community to collectively identify the best (and worst) contributions: bookmarks, likes, flags, replies, edits, watching, muting and so forth. Use these tools to improve your own experience, and everyone else’s, too.

Let’s leave our community better than we found it.

<a name="flag-problems"></a>

## [If You See a Problem, Flag It](#flag-problems)

Moderators have special authority; they are responsible for this forum. But so are you. With your help, moderators can be community facilitators, not just janitors or police.

When you see bad behavior, don’t reply. Replying encourages bad behavior by acknowledging it, consumes your energy, and wastes everyone’s time. _Just flag it_. If enough flags accrue, action will be taken, either automatically or by moderator intervention.

In order to maintain our community, moderators reserve the right to remove any content and any user account for any reason at any time. Moderators do not preview new posts; the moderators and site operators take no responsibility for any content posted by the community.

<a name="be-civil"></a>

## [Always Be Civil](#be-civil)

Nothing sabotages a healthy conversation like rudeness:

| Be civil. Don’t post anything that a reasonable person would consider offensive, abusive, or hate speech.
| Keep it clean. Don’t post anything obscene or sexually explicit.
| Respect each other. Don’t harass or grief anyone, impersonate people, or expose their private information.
| Respect our forum. Don’t post spam or otherwise vandalize the forum.

These are not concrete terms with precise definitions &mdash; avoid even the _appearance_ of any of these things. If you’re unsure, ask yourself how you would feel if your post was featured on the front page of a major news site.

This is a public forum, and search engines index these discussions. Keep the language, links, and images safe for family and friends.

<a name="keep-tidy"></a>

## [Keep It Tidy](#keep-tidy)

Make the effort to put things in the right place, so that we can spend more time discussing and less cleaning up. So:

| Don’t start a topic in the wrong category; please read the category definitions.
| Don’t cross-post the same thing in multiple topics.
| Don’t post no-content replies.
| Don’t divert a topic by changing it midstream.
| Don’t sign your posts &mdash; every post has your profile information attached to it.

Rather than posting “+1” or “Agreed”, use the Like button. Rather than taking an existing topic in a radically different direction, use Reply as a Linked Topic.

<a name="stealing"></a>

## [Post Only Your Own Stuff](#stealing)

You may not post anything digital that belongs to someone else without permission. You may not post descriptions of, links to, or methods for stealing someone’s intellectual property (software, video, audio, images), or for breaking any other law.

<a name="power"></a>

## [Powered by You](#power)

This site is operated by your [friendly local staff](/about) and |you|, the community. If you have any further questions about how things should work here, open a new topic in the [site feedback category](/c/site-feedback) and let’s discuss! If there’s a critical or urgent issue that can’t be handled by a meta topic or flag, contact us via the [staff page](/about).

<a name="tos"></a>

## [Terms of Service](#tos)

Yes, legalese is boring, but we must protect ourselves &ndash; and by extension, you and your data &ndash; against unfriendly folks. We have a [Terms of Service](/tos) describing your (and our) behavior and rights related to content, privacy, and laws. To use this service, you must agree to abide by our [TOS](/tos).
Community Standards and Guidelines|Community is the heart of Tresata. Community guidelines are a crucial aspect of any online platform or community.  To ensure a respectful and safe environment for all Tresata Community Members, we have set the following guidelines:

| Engage respectfully, professionally, and with integrity at all times
| Describe the situation & context, not specific details
| Never share sensitive or revealing information (related to people, products, or clients)
| Keep informal conversations outside of the community
| Use Tresata Community for all technical topics/ threads
| When responding to a post, attempt to solve or progress the conversation productively
| Search for duplicates before posting
| Use proper grammar and spelling
| Facts > opinions, post thoughtfully
| If you think something contributes to the conversation or is the right answer, upvote it and vice-versa
| Zero-tolerance for inappropriate, hurtful, or negative content
| [Reminder] - This is an open community so these guidelines apply to both internal & external user engagement, act accordingly
Welcome to Tresata Community!|Welcome and congratulations on becoming a member of the |Tresata community!| 

Tresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. 

You will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community. 

Know about using the portal here:
![image|690x329](upload://n72U8QITc9VRqqzBs68ZRcAgwT5.png)






You will see personalised cards displayed on the [landing page](https://community.tresata.com/) covering multiple topic categories with relevant posts, specifically for you. The |GET STARTED| section will help you use the downloaded product easily.

You can leverage the robust |Search| functionality to find topic you are interested and leave comments, vote for the topic, or even create a new topic if needed.

For any support required, do reach out to support@tresata.com
GUIDELINES|<a name="civilized"></a>

## [This is a Civilized Place for Public Discussion](#civilized)

Please treat this discussion forum with the same respect you would a public park. We, too, are a shared community resource &mdash; a place to share skills, knowledge and interests through ongoing conversation.

These are not hard and fast rules. They are guidelines to aid the human judgment of our community and keep this a kind, friendly place for civilized public discourse.

<a name="improve"></a>

## [Improve the Discussion](#improve)

Help us make this a great place for discussion by always adding something positive to the discussion, however small. If you are not sure your post adds to the conversation, think over what you want to say and try again later.

One way to improve the discussion is by discovering ones that are already happening. Spend time browsing the topics here before replying or starting your own, and you’ll have a better chance of meeting others who share your interests.

The topics discussed here matter to us, and we want you to act as if they matter to you, too. Be respectful of the topics and the people discussing them, even if you disagree with some of what is being said.

<a name="agreeable"></a>

## [Be Agreeable, Even When You Disagree](#agreeable)

You may wish to respond by disagreeing. That’s fine. But remember to _criticize ideas, not people_. Please avoid:

| Name-calling
| Ad hominem attacks
| Responding to a post’s tone instead of its actual content
| Knee-jerk contradiction

Instead, provide thoughtful insights that improve the conversation.

<a name="participate"></a>

## [Your Participation Counts](#participate)

The conversations we have here set the tone for every new arrival. Help us influence the future of this community by choosing to engage in discussions that make this forum an interesting place to be &mdash; and avoiding those that do not.

Discourse provides tools that enable the community to collectively identify the best (and worst) contributions: bookmarks, likes, flags, replies, edits, watching, muting and so forth. Use these tools to improve your own experience, and everyone else’s, too.

Let’s leave our community better than we found it.

<a name="flag-problems"></a>

## [If You See a Problem, Flag It](#flag-problems)

Moderators have special authority; they are responsible for this forum. But so are you. With your help, moderators can be community facilitators, not just janitors or police.

When you see bad behavior, don’t reply. Replying encourages bad behavior by acknowledging it, consumes your energy, and wastes everyone’s time. _Just flag it_. If enough flags accrue, action will be taken, either automatically or by moderator intervention.

In order to maintain our community, moderators reserve the right to remove any content and any user account for any reason at any time. Moderators do not preview new posts; the moderators and site operators take no responsibility for any content posted by the community.

<a name="be-civil"></a>

## [Always Be Civil](#be-civil)

Nothing sabotages a healthy conversation like rudeness:

| Be civil. Don’t post anything that a reasonable person would consider offensive, abusive, or hate speech.
| Keep it clean. Don’t post anything obscene or sexually explicit.
| Respect each other. Don’t harass or grief anyone, impersonate people, or expose their private information.
| Respect our forum. Don’t post spam or otherwise vandalize the forum.

These are not concrete terms with precise definitions &mdash; avoid even the _appearance_ of any of these things. If you’re unsure, ask yourself how you would feel if your post was featured on the front page of a major news site.

This is a public forum, and search engines index these discussions. Keep the language, links, and images safe for family and friends.

<a name="keep-tidy"></a>

## [Keep It Tidy](#keep-tidy)

Make the effort to put things in the right place, so that we can spend more time discussing and less cleaning up. So:

| Don’t start a topic in the wrong category; please read the category definitions.
| Don’t cross-post the same thing in multiple topics.
| Don’t post no-content replies.
| Don’t divert a topic by changing it midstream.
| Don’t sign your posts &mdash; every post has your profile information attached to it.

Rather than posting “+1” or “Agreed”, use the Like button. Rather than taking an existing topic in a radically different direction, use Reply as a Linked Topic.

<a name="stealing"></a>

## [Post Only Your Own Stuff](#stealing)

You may not post anything digital that belongs to someone else without permission. You may not post descriptions of, links to, or methods for stealing someone’s intellectual property (software, video, audio, images), or for breaking any other law.

<a name="power"></a>

## [Powered by You](#power)

This site is operated by your [friendly local staff](/about) and |you|, the community. If you have any further questions about how things should work here, open a new topic in the [site feedback category](/c/site-feedback) and let’s discuss! If there’s a critical or urgent issue that can’t be handled by a meta topic or flag, contact us via the [staff page](/about).

<a name="tos"></a>

## [Terms of Service](#tos)

Yes, legalese is boring, but we must protect ourselves &ndash; and by extension, you and your data &ndash; against unfriendly folks. We have a [Terms of Service](/tos) describing your (and our) behavior and rights related to content, privacy, and laws. To use this service, you must agree to abide by our [TOS](/tos).
Community Standards and Guidelines|Community is the heart of Tresata. Community guidelines are a crucial aspect of any online platform or community.  To ensure a respectful and safe environment for all Tresata Community Members, we have set the following guidelines:

| Engage respectfully, professionally, and with integrity at all times
| Describe the situation & context, not specific details
| Never share sensitive or revealing information (related to people, products, or clients)
| Keep informal conversations outside of the community
| Use Tresata Community for all technical topics/ threads
| When responding to a post, attempt to solve or progress the conversation productively
| Search for duplicates before posting
| Use proper grammar and spelling
| Facts > opinions, post thoughtfully
| If you think something contributes to the conversation or is the right answer, upvote it and vice-versa
| Zero-tolerance for inappropriate, hurtful, or negative content
| [Reminder] - This is an open community so these guidelines apply to both internal & external user engagement, act accordingly
Welcome to Tresata Community!|Welcome and congratulations on becoming a member of the |Tresata community!| 

Tresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. 

You will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community. 

Know about using the portal here:
![image|690x329](upload://n72U8QITc9VRqqzBs68ZRcAgwT5.png)






You will see personalised cards displayed on the [landing page](https://community.tresata.com/) covering multiple topic categories with relevant posts, specifically for you. The |GET STARTED| section will help you use the downloaded product easily.

You can leverage the robust |Search| functionality to find topic you are interested and leave comments, vote for the topic, or even create a new topic if needed.

For any support required, do reach out to support@tresata.com
GUIDELINES|<a name="civilized"></a>

## [This is a Civilized Place for Public Discussion](#civilized)

Please treat this discussion forum with the same respect you would a public park. We, too, are a shared community resource &mdash; a place to share skills, knowledge and interests through ongoing conversation.

These are not hard and fast rules. They are guidelines to aid the human judgment of our community and keep this a kind, friendly place for civilized public discourse.

<a name="improve"></a>

## [Improve the Discussion](#improve)

Help us make this a great place for discussion by always adding something positive to the discussion, however small. If you are not sure your post adds to the conversation, think over what you want to say and try again later.

One way to improve the discussion is by discovering ones that are already happening. Spend time browsing the topics here before replying or starting your own, and you’ll have a better chance of meeting others who share your interests.

The topics discussed here matter to us, and we want you to act as if they matter to you, too. Be respectful of the topics and the people discussing them, even if you disagree with some of what is being said.

<a name="agreeable"></a>

## [Be Agreeable, Even When You Disagree](#agreeable)

You may wish to respond by disagreeing. That’s fine. But remember to _criticize ideas, not people_. Please avoid:

| Name-calling
| Ad hominem attacks
| Responding to a post’s tone instead of its actual content
| Knee-jerk contradiction

Instead, provide thoughtful insights that improve the conversation.

<a name="participate"></a>

## [Your Participation Counts](#participate)

The conversations we have here set the tone for every new arrival. Help us influence the future of this community by choosing to engage in discussions that make this forum an interesting place to be &mdash; and avoiding those that do not.

Discourse provides tools that enable the community to collectively identify the best (and worst) contributions: bookmarks, likes, flags, replies, edits, watching, muting and so forth. Use these tools to improve your own experience, and everyone else’s, too.

Let’s leave our community better than we found it.

<a name="flag-problems"></a>

## [If You See a Problem, Flag It](#flag-problems)

Moderators have special authority; they are responsible for this forum. But so are you. With your help, moderators can be community facilitators, not just janitors or police.

When you see bad behavior, don’t reply. Replying encourages bad behavior by acknowledging it, consumes your energy, and wastes everyone’s time. _Just flag it_. If enough flags accrue, action will be taken, either automatically or by moderator intervention.

In order to maintain our community, moderators reserve the right to remove any content and any user account for any reason at any time. Moderators do not preview new posts; the moderators and site operators take no responsibility for any content posted by the community.

<a name="be-civil"></a>

## [Always Be Civil](#be-civil)

Nothing sabotages a healthy conversation like rudeness:

| Be civil. Don’t post anything that a reasonable person would consider offensive, abusive, or hate speech.
| Keep it clean. Don’t post anything obscene or sexually explicit.
| Respect each other. Don’t harass or grief anyone, impersonate people, or expose their private information.
| Respect our forum. Don’t post spam or otherwise vandalize the forum.

These are not concrete terms with precise definitions &mdash; avoid even the _appearance_ of any of these things. If you’re unsure, ask yourself how you would feel if your post was featured on the front page of a major news site.

This is a public forum, and search engines index these discussions. Keep the language, links, and images safe for family and friends.

<a name="keep-tidy"></a>

## [Keep It Tidy](#keep-tidy)

Make the effort to put things in the right place, so that we can spend more time discussing and less cleaning up. So:

| Don’t start a topic in the wrong category; please read the category definitions.
| Don’t cross-post the same thing in multiple topics.
| Don’t post no-content replies.
| Don’t divert a topic by changing it midstream.
| Don’t sign your posts &mdash; every post has your profile information attached to it.

Rather than posting “+1” or “Agreed”, use the Like button. Rather than taking an existing topic in a radically different direction, use Reply as a Linked Topic.

<a name="stealing"></a>

## [Post Only Your Own Stuff](#stealing)

You may not post anything digital that belongs to someone else without permission. You may not post descriptions of, links to, or methods for stealing someone’s intellectual property (software, video, audio, images), or for breaking any other law.

<a name="power"></a>

## [Powered by You](#power)

This site is operated by your [friendly local staff](/about) and |you|, the community. If you have any further questions about how things should work here, open a new topic in the [site feedback category](/c/site-feedback) and let’s discuss! If there’s a critical or urgent issue that can’t be handled by a meta topic or flag, contact us via the [staff page](/about).

<a name="tos"></a>

## [Terms of Service](#tos)

Yes, legalese is boring, but we must protect ourselves &ndash; and by extension, you and your data &ndash; against unfriendly folks. We have a [Terms of Service](/tos) describing your (and our) behavior and rights related to content, privacy, and laws. To use this service, you must agree to abide by our [TOS](/tos).
Community Standards and Guidelines|Community is the heart of Tresata. Community guidelines are a crucial aspect of any online platform or community.  To ensure a respectful and safe environment for all Tresata Community Members, we have set the following guidelines:

| Engage respectfully, professionally, and with integrity at all times
| Describe the situation & context, not specific details
| Never share sensitive or revealing information (related to people, products, or clients)
| Keep informal conversations outside of the community
| Use Tresata Community for all technical topics/ threads
| When responding to a post, attempt to solve or progress the conversation productively
| Search for duplicates before posting
| Use proper grammar and spelling
| Facts > opinions, post thoughtfully
| If you think something contributes to the conversation or is the right answer, upvote it and vice-versa
| Zero-tolerance for inappropriate, hurtful, or negative content
| [Reminder] - This is an open community so these guidelines apply to both internal & external user engagement, act accordingly
Welcome to Tresata Community!|Welcome and congratulations on becoming a member of the |Tresata community!| 

Tresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. 

You will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community. 

Know about using the portal here:
![image|690x329](upload://n72U8QITc9VRqqzBs68ZRcAgwT5.png)






You will see personalised cards displayed on the [landing page](https://community.tresata.com/) covering multiple topic categories with relevant posts, specifically for you. The |GET STARTED| section will help you use the downloaded product easily.

You can leverage the robust |Search| functionality to find topic you are interested and leave comments, vote for the topic, or even create a new topic if needed.

For any support required, do reach out to support@tresata.com
