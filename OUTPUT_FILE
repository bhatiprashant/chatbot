"Even I faced this issue. \nThe thing over here is the jobs are running fine in the backend but its not reflecting on the UI.  \nI kept getting these pop ups shown in the screenshot below, checkd the network to find some status code error.\n![Screenshot from 2024-04-30 09-39-04|690x388](upload://3YmD59Q2gZuiJh1WB10XleEGEfr.png)\n![Screenshot from 2024-04-30 09-41-44|690x388](upload://6Y5LTmsxqke75KGgH8Cs1hzQ2l5.png)"
"I would look at the regexFilter cleaner. You can refer to this post too - https://community.tresata.com/t/need-for-a-tail-head-scrubber/777/3"
"Hey everyone,\nin the cleaners we have a cleaner\nCleaner Type: RegexReplace\nDescription : Scrubber that substitutes matches of the given Regular Expression with the replacement string\n\nExample: I want to use only alphabets from a alphanumeric string. \nso in general we use  '\\d+' or [^a-zA-Z ] to remove the digits or keep only the alphabets.\n\nbut how do we use it to replace the string with nothing or a space in this cleaner of ours ? \nIf I leave it  blank, it save that input is missing.\n\nIn general we use '' or ' ' for nothing and space. Alternative option is to use \"\" or \" \" . \nfor this try,\n![image|690x229](upload://q0n8iAf3q2XbX6F2dfBLuto4eP3.png)\nwe get the following result \n![image|690x281](upload://YFpyzAEwrIHtEi7JgJJBKCThLS.png)\n\n How can I get the desired result here ? Or do we have any other cleaners or workaround for this ?"
"Impact:  hack - test case impacted\n\nProduct Step: Connect\nProduct Name: CustomerLoyalty\nDescription:  When I first ran the connect logic, it resolved to some 139m tresata Ids. In order to improve the results I tried to playaround and modify the connect logic but due to some strange reasons I don't seems to resolve anything at all. \n\nAdding the below screenshots of my last two iteration for the reference .\ncase 1:\n![Screenshot from 2024-04-30 23-19-01|690x388](upload://9wfos2YjplDC1VbUNa6apgrRopx.png)\n![Screenshot from 2024-04-30 23-19-05|690x388](upload://lNvHW30D7gjlOrjeLLavRR7LULn.png)\n\ncase 2: \n![Screenshot from 2024-04-30 23-20-22|690x388](upload://y9JYvsvIuqEKLPyujtICm2ElDQn.png)\n![Screenshot from 2024-04-30 23-20-48|690x388](upload://H7t2onfYxh9IUsfOQjhpARM143.png)\n![Screenshot from 2024-04-30 23-42-31|690x388](upload://m1b53nXUNOMtBCX1nEPaBsryHbL.png)\n\nAdditional Details:\nSystem: Linux, Brower: Chromium\nNamespace: 44da0b3d15f94ffeb16b281ef91ab0b3\nProduct Id : 745774696"
"Impact:  hack - test cases impact\nProduct Step: Profile\nProduct Name: LegalEntityHiddenNetworks\nDescription:  I get a \"something is wrong\" pop up on my screen. It happens to appear on all the pages of the product. The profile steps keep on in the running status for hrs and doesn't complete and when cancel it goes to the pending state.\n\nAdditional Details:\nSystem: Linux, Browser: Chromium. \nAdding the below screenshots :\n![Screenshot from 2024-04-30 09-39-04|690x388](upload://3YmD59Q2gZuiJh1WB10XleEGEfr.png)\n![Screenshot from 2024-04-30 09-41-44|690x388](upload://6Y5LTmsxqke75KGgH8Cs1hzQ2l5.png)"
"Would be helpful if descriptions for cleaners contained not just the Cleaner Type and Description, but also contains an example. The reason for the example would be it would help first time users that are not familiar with the description (or the language within the description) and wants to know the effect of applying the cleaner. \n\nThe value for first time users would be that when they go to test and do not know what to test for, the first time user could be given an example of what the cleaner should be used for. Even if it is just an information button that the user can hover over :slightly_smiling_face:"
"Thank you for that clarity Ben! :slightly_smiling_face:"
"I don't see any running jobs. Also, all the past profiling jobs don't have any runtimes over 5 minutes. Are you still facing this issue?"
"Hello Ben! Thank you for that clarity but with certain cleaners I am not sure what to test for due to having no familiarity with the cleaner's name or some of the language within the cleaner description. I disregarded certain cleaners due to not knowing what to test for and therefore skipping :confused:"
"Namespace: dfb43ee38a2a4dfc902809e41facf03a\nProduct: Usecase7"
"Hi Everyone!\nFor the Validate step - when I run a query, click on a TresataID, and visit the Graph Inspector - I am typically looking to expand everything for each node to view every connection. I find that this is the case for many cases. Is there a way to have the Knowledge Graph automatically expanded with the option to deselect the \"all\" fields on each node?"
"Can you give more context behind these screenshots? Do they correspond to 3 separate runs? What did you change in your Connect logic between each of them?\n\nCan you share your namespace ID and your product name? I can try to confirm the actual pipelines used for each run on the backend"
"Can you share your namespace and product name? I will check on the backend"
"I am running the profile job on the Usecase-7 and it is running for like 3+ hours. This issue exists even when I cancel and re-profile the job.\n![image|690x343](upload://xUuySvaoUSHGhimHrMEj6Liq8zq.png)"
"in order to improve the results for use case 1, I tried to update the logic multiple times but it does not seem to work and I hardly get any connections.\n![Screenshot from 2024-04-30 13-40-05|690x388](upload://2JJFgOpRBp6v74jCvsVCpO7EUOW.png)\n\n![Screenshot from 2024-04-30 14-10-46|690x388](upload://yfrY8Wp5oebEutlNKlvbezHzUyw.png)\n![Screenshot from 2024-04-30 11-25-21|690x388](upload://dXfRuNtkZwfGBsr7SoH3xpEnydn.png)\n![Screenshot from 2024-04-30 13-39-59|690x388](upload://AkrgHBviluDUuZ9u25iJoQzsQiG.png)"
"I'll just add that determining if your resolution logic is \"correct\" is really hard at this scale. A few other approaches that are helpful are looking at your Top Tresata IDs and Compression Statistics. \n\nTop Tresata IDs can help identify \"hairballs\" - a form of severe over-compression in which loose matching logic allows many records to collapse into one entity because they share a common feature, such as a phone number or address (often times “filler” values such as 123 Main St. and 123-456-7890).\n\nCompression Statistics helps identify further over-compression. Using the Query Builder and Graph Inspector, it is worth looking into Tresata IDs that have multiple names, phone numbers, email addresses, birthdates, genders, and other relevant PII elements.\n\nLastly, I'll emphasize that this is an iterative approach. These different features in Validate help surface resolution issues. For example, after resolving the problematic Top Tresata IDs, a few more might appear with your next run to further investigate. It is also possible that addressing one issue could present others as well, so it is important to size the issues before and after fixing"
"This looks a little higher than a Low impact! Thank you for raising - losing work is not good. I'm having trouble replicating though. Can you provide more detail on these steps? \n\nThis was in Prepare? What do you mean by adding/removing sources? Which sources? What work did you do? What do you mean by dropping a field outside the box? Did you mean Connect?"
"![image|690x372](upload://iM2rVMSxlrllVaTx5owMwTDpGRJ.png)\n\n@anish.kutty_1 After taking a look at your resolution logic, I am confused how any source overlap would exist in your connect output. The above config shows that you are only identifying connections within the booking source.\n\nRegarding the logic itself:\n\n1. name + phone - assuming the data has been cleaned well, this is reasonable logic. generally, someone with the same name and same phone number is the same person. I would consider adding birthdate to your Shields, to avoid scenarios where a father/son, mother/daughter with same name and using the same phone are connected.\n2. email - i would consider this to be 'loose' logic, and would consider supplementing with additional PII. this clause means that any two records which share the same email will be connected, given they do not have conflicting values for your \"gender\" field. similar to the name+phone scenario, contact information can be re-used by other individuals in the same household. \n3. customerID - same as email. family or friends might re-use or share the same loyalty id when making bookings - i would supplement with other PII. however, when resolving the loyalty source, which is a more curated dataset of customer ids and the persons who are actually registered to those ids, you could probably safely use this logic."
"Hey @JustyceL , can you quickly check on PREPARE and ensure the canonicals are there? \n\nIf canonicals are missing, I believe your PROFILE tags might not be in-sync with your PREPARE canonicals (that could be because of moving to PREPARE and then adding more tags on PROFILE)."
"@anish.kutty_1 The overlap statistics help show us how connected our various sources are, and is useful as a common sense check against the relationships we might expect to see between sources. More specifically, the # of Records for a given data source combination is the *number of records that share a tresataID with a record in each listed source*, and will always be greater than or equal to the number of tresataIDs that have a record in each listed source.\n\nApplied to your use case, we should likely expect a bookings dataset and a ticketing dataset from the same airline to have a very high degree of overlap, since bookings and tickets are generated from the same event / purchase. Stated differently, we should expect many, if not all, of the unique customers identified in the ticketing source to also exist in the booking source, since all customers who receive a ticket when making a purchase should also receive a booking number. \n\nWe may expect a slightly lower degree of overlap between a tickets source and loyalty source, or bookings source and loyalty source, since not everyone who purchases a ticket with an airline is necessarily registered in that airline's loyalty program.\n\nI would recommend investigating those tresataIds which were only assigned to records in loyalty. If I remember correctly, the total number of records in loyalty is 190k, which means about half of the loyalty records did not connect with a record from tickets or booking.  \n\nAdditionally, I am not sure that the number of total records you have (~900k) is correct, as it is far fewer than the number of records overlapping between all three sources (~1.8M). It would be helpful if you would submit a ticket to Tresata Support for further investigation."
"I would have really like to see all the sources as pills, as there is space on the screen to display them. Instead I see a +1 and I have to go through each one to find out what it is\n![image|690x170](upload://4MgW8FmJyhxbxV8sz1ZmBs2dRiF.png)"
" After I pull in the data, tag and use cleaners, I am having trouble with configuring steps to connect the data. In the demo there seem to be over 10 selected sources and I have 0 selected sources. To be clear, I have followed the demo tagging and cleaners step-by-step so I am not clear on why I have no selected sources once at the configuration step (Please see Screenshots).\n\n![DemoLoyaltyProduct|690x341](upload://vbBHUvQvmGAYUaxnSXY8wy149OB.jpeg)\n![MyLoyaltyProduct|690x351](upload://yFlQOdU9kfauo2g4cm7UmM7VMkM.png)"
"**Impact**:   *Low*\n\n****Product Step:****   *Prepare*\n**Product Name:**  *CustomerLoyalty*\n**Description**:  *Lost all progress to an error page the refreshed using button in ui but to a previous saved state*\n\n**Additional Details:**\n*  *Mozilla Firefox 124.0.2 - Ubuntu 20.04 LTS*\n\n* Add sources\nDo some work\n* Remove sources\n* all work is automatically cleared\n* start adding your field again *possibly try dropping it in an area a tiny bit outside the box a few times to reach the following potential error codes *\n*Reload page and it will come back to previous work from work performed ** however this work comes back to a previous state that we're no longer at, totally losing all work done before error code.**\n\n\n![image|690x458](upload://jqcL1RCwUqpHt4m1gwvXyhaExHW.jpeg)\n![image|690x441](upload://1ynfLueYCTObW2wDwtUO06XNwIV.jpeg)\n![image|690x473](upload://uL5BlB654mNVWR2VUATLY9F5OoR.jpeg)"
"It would be really helpful to see a few top values for each field when I am selecting cleaners for it.\nCurrently I have to view heatmap, find the field I'm interested in and use the tooltip icon to see top values for that field."
"Hello Akhila,\nIf you would refresh the page, it should take you back to connect page. Also, could you please mention the exact steps, so I am able to recreate it."
"@willbaldwin , My logic view\n![Screenshot 2024-04-30 at 10.43.11 AM|690x372](upload://fBpKCBBbmA43zOo1tGMGLhUhHwr.png)"
"How do a common user know there is something wrong with the added\n![Screenshot 2024-04-30 at 10.30.10 AM|690x373](upload://qcDjeGJQ2J5MOtdTeJ1ScCCdaY7.png)\n resolution logic?"
"In Resolve and Against Step, I first added Against then tried to add resolve - where i got this page. It doesn't really specifies what should I do next, or what did I do wrong. \n![image|690x378](upload://HRNDrA1A6FWSAzPRCm6RxNRLQ8.jpeg)"
"Hi Karan, thanks for raising. That is very strange. I followed your steps, but I am not able to reproduce it. I was able to go back and add the zip code field to the address parser - \n\n![image|432x196](upload://wiP5B3A5zEdFdQeUkbDUHeJWfm9.png)\n\nIs this still an issue for you? Do others on your team get this error too?"
"Hi Naman, this is something our team is currently developing a change for so that the canonical field will automatically appear here once the tag has been added, after the first time tags are added. For now, you can select the Add Canonical Field button for a source that is not marked completed and then add the raw field and name the canonical, but keep an eye out for this change in the product going live in the near future."
"I went back to add more tags to the fields after running the connect step. After I have successfully added a tag I am able to see that on the heat map but not able to see on the prepare step when I want to add cleaners to it.\n\nHere I have added name tag to the tickets data set where Pax_Full_Nm is tagged as name. You can see it in the image below.\n![image|690x323](upload://n9BPDSbljMMCGs4HW9pVehpNOAW.png)\n\nBut the tag is not reflected on the prepare step to add cleaner. You can see it in the image below.\n![image|690x327](upload://fWiZXXbUl76PA53oDlBJHvmkfBL.png)"
"Hi Naman, when adding the canonical tag to the previous steps, did you also add the canonical as a new canonical in the Prepare step? Currently, the software requires you to add it there after performing an initial translation of all tags into canonical fields, however a change to this to constantly synchronize the tags and canonical fields is coming to the product very shortly."
"I added new tag to the `name` field for two datasets for which I forgot to add in the beginning (after running the pipeline).  After successfully adding the tag and running prepare, I reran the connect step without changing the logic in it. It did not rerun the step and just gave me the same old result. As I had added new tag to the two data sources It should have reran the step which it did not.  Its also better to make a rerun button for each step so user can rerun a step."
"Great point of feedback Justyce, I think clarifying the purpose of that piece of information to the user would be a good add to the current cleaner. As this is a parser (in this case for name), the software is taking the data from the name column (i.e. \"John Adam Smith\") and parsing it into **firstName = John** and **lastName = Smith**, and then allowing you to create those fields. The canonical prefix is simply asking you to give some more context to the parsed name - so when you apply the parser, you should see **\\<your canonical prefix\\>_first_name** and **\\<your canonical prefix\\>_last_name** added to your canonical fields (assuming you had just first name and last name selected in the blue box to the right)."
"Hello Ben! I am trying to apply the Name cleaner when running into that \"issue\". My question is, as a user that does not understand software on that level, I am not sure why that portion is necessary for me to answer."
"Great find, thanks for raising! Turns out this is an issue with Athena. The \"singleton\" field is a boolean, so we can't apply string operations to it.\n```\nThe Amazon Athena query failed to run with error message: TYPE_MISMATCH: line 12:18: Cannot apply operator: boolean = varchar(5)\n```"
"Clever solution. However, it sounds like a more robust solution would be the ability to create a new canonical from either an existing canonical field or a raw field that's already been tagged. Is that accurate?"
"To clarify, you're asking if zip should disappear from Suggested Tags after the user clicks and adds it?\n\n![image|690x94](upload://qbxFHle4smb9DRrEoKoxq2MdWw5.png)"
"Hi Akhila, is there anyone else in your namespace that has added that tag?"
"Sure, \"resolve\" will use your connect logic to link ALL records across ALL sources (i.e. many-to-many). On the other hand, \"against\" will use your connect logic to link ALL records from your \"From\" sources to ONE record from your \"To\" source.\n\nA common use case for using resolve and against is when you're resolving entities in a customer reference source and a transactions source. It is critical to first \"resolve\" the customer reference source correctly, so multiple records for the same customer will receive the same Tresata ID. Then, you can resolve your transaction source \"against\" your customer reference source. Even though a transaction will link to only one record, the customer reference source will have already been resolved, so all records for the same entity will receive the same Tresata ID.\n\nHere's an example - \n\nCustomer Reference Source\n\n1. Ed Smith, ed@gmail.com\n2. Edward Smith, ed@gmail.com\n\nTransaction Source\n\n1. Ed Smith, $40, ...\n2. Edward Smith, $100, ...\n\nIncorrect Approach: If we resolve the Transaction Source against the Customer Reference Source on name, then the first record of each source will match, and the second record of each source will match separately, resulting in 2 Tresata IDs for the same entity.\n\n![image|690x242](upload://pjcgIL2XOhLvdDfPxEUKJnypC6v.png)\n\nCorrect Approach: If we first resolve the Customer Reference Source on email, then both records will receive the same Tresata ID. Then, we can resolve the Transaction Source against the Customer Reference Source on name. Since the Tresata ID in the Customer Reference Source contains both names, both Transaction Source records will match and receive the same Tresata ID.\n\n![image|690x98](upload://9R2IPg5wogOxU46b2D1mvZ7vMSR.png)"
"Hi Vijay, I can't see your screenshot, but that's great feedback. The Group By functionality is currently limited in the Query Builder. While we work to enhance that, an alternative approach is querying the Tail Stats source. It has a field \"numSources\" that you can filter on. For example, numSources > 1 will return Tresata Ids that span multiple datasets."
"**Impact**:   *LOW*\n\n****Product Step:****   *Prepare*\n**Product Name:**  *SaketMediaStream*\n**Description**:  *While adding cleaners, The hints (With bulb) overlaps the buttons scrolling up and down*\n\n**Additional Details:**\n![image|425x500](upload://tg97OIR6hZHi39gahZIzW4Me2yF.png)"
"Thanks for raising, Saket. I was able to reproduce. Our team will file a ticket for this"
"I am unable to get any streams for disney plus in this dataset. How would I be able to use the software to query to get a trestaaID that spans across the datasets, so I could investigate?\n\n\n[Screenshot 2024-04-29 at 17.18.55|690x383](upload://fSWZUBvCSQe2FQDWYJRYzWnn4aE.png)"
"Steps to Recreate above error\n\n1. Go to Validate Step of a configured product on Pre-Prod Env\n2. Under Query Builder, Click on \"Data Asset\"\n3. Using the explore option create a query by adding a condition of Single equals false\n4. Click on Apply and Executed Query"
"Hi Abhilasha, \nI have already done a mail, it's not as descriptive but you should be able to figure out what it is."
"Could you please mail this on support@tresata.ai @Mukul"
"**Impact**:   *Low*\n\n****Product Step:****   *Prepare*\n**Description**:  *Adding coalesce cleaner allows user to put in a single value as the input field for coalescing. For example, I can have name and then apply coalesce on just name to get new_name. But on refreshing the page, Only new_name is left, and name canonical is lost.*\n\n**Additional Details:**\n*  *Browser & Version: Chromium*"
"Andy, thanks for your feedback - this is something we have noticed users are unsure of in the past, and would be good to make more clear."
"Could I please get an explanation between the difference of resolve and against on connect step?"
"Could I please get an explanation between the difference of resolve and against on connect step?"
"**Impact**:  LOW\n\n****Product Step:****   *Profile*\n**Product Name:**  *SaketMediaStreamingServices*\n**Description**:  *Tags are not seen when the similarity index is changed. \nSteps to reproduce the issue:*\n*  *Go to Profile step*\n*  *Select the field you want to tag*\n*  *Change the similarity Index*\n*  *Add a tag to the fields and propagate*\n*  *Observe the tags got added on the fields* \n*  **\n![image|425x500](upload://n9vyCqjA0acvmqBInJQvTip0XCT.png)\n*\n*  **\n![image|425x500](upload://bYe4LvShfOoEDheBrkasOzMFKj1.png)\n*\n*  *Change the similarity index again* \n*  *Observe the tags no longer shown for the fields in the sources*\n* *Observe the tags present on the fields in respective sources*"
"Hi Justyce, which cleaner are you using where you are running into this issue? If it is a parser, this is because the software is going to break down the components of your selected field into new canonical fields, and needs a prefix to apply uniquely name the new fields."
"When adding a cleaner, it would be helpful to know the reason for ‘Canonical Field Name’ and why it must be added to apply the cleaner. Also, it seems like if you do not choose a prefix for those ‘new canonicals’ it will not allow you to proceed with the cleaner. This is complicated as a first time user because this is my first time seeing the word canonical."
"Why does it show \"address_2\" as suggested tag when I select zipcode. I haven't created that tag before for any of the other sources. \n![image|690x336](upload://6q93BmffDzM3oim6aRdlr9kNw5A.jpeg)"
"You can copy the following paths on the search bar on source page\nUsecase 1: s3a://tresata-dune/usecases/UC1_CustomerLoyalty\nUsecase 2: s3a://tresata-dune/usecases/UC2_LegalEntityHiddenNetworks\nUsecase 3: s3a://tresata-dune/usecases/UC3_HealthCarePatientPopulation\nUsecase 4: s3a://tresata-dune/usecases/UC4_HealthCareProviderPayer\nUsecase 5: s3a://tresata-dune/usecases/UC5_FinancialService\nUsecase 6: s3a://tresata-dune/usecases/UC6_StreamingServices\nUsecase 7: s3a://tresata-dune/usecases/UC7_OnlineRetailer\nUsecase 8: s3a://tresata-dune/usecases/UC8_TelecomCustomerChurn"
"@archana it would be good to check on heatmap, the type of values singleton has because it would be the case that 'false' is not value of singleton"
"Two interpretations for a user trying to profile all three sources in the attached image:\n1. Just click \"Continue To Profile\" to profile all sources\n2. Click all three checkboxes next to the sources, then click \"Continue To Profile\". Otherwise none will be profiled\n\nI think it would be clearer to the user that scenario 1 is correct if the column with the checkboxes had a header indicating that a checkmark will mark the source for deletion. Then it would be more clear that checking is not necessary to profile the sources.\n\nFurthermore, I think the button labeled \"Continue To Profile\" could clear up any of this confusion if it the label was more clear that all sources will be profiled, regardless of what we do with the checkboxes. Perhaps something like \"Profile All (3) Sources\"?\n\n![Screenshot 2024-04-29 at 4.25.54 PM|690x266](upload://97P2KxLD81kC0S1ANBR63wbzNxS.png)"
"Hi All!\n\nWhile creating a data product, a major part of the profile step is to tag fields across the data sources. But what's the real motivation behind tagging and how to choose the \"right\" fields for it?\n\nSo for the first part: \n\nTagging is the process by which we can standardize the field names of the column containing the same information across the different sources. Consider 3 sources containing customer information that we want to resolve. Let's say, the field that contains the name information of the customer in 3 data sources are \"FULL_NAME\", \"full_name\" and \"Name\". While building the resolution logic during the connect step, we'll need to apply some combination of fields to resolve the entities.\n\nLet say, one of the steps is: name + email + phone\n\nTo use this name information, we should have a standard field that consists of name information across the sources. Here's where tagging plays a crucial role. With the help of tagging, we create a standardized name for each of the field across sources so that we can use that tagged field (canonical) further for building the pipeline.\n\nNow for the second part, i.e. how to choose the right fields to be tagged?\n\nAll the fields that we want to apply cleaning to and want to use to build the resolution logic should be tagged. These will then become the canonical fields."
"Hi @anish.kutty_1, I'm not sure if there's a \"formula\" so to speak, but use cases usually revolve around identifying either unique companies or individuals. I think a helpful strategy is to think about the sources that are most likely to contain personally identifiable information that pertain to those companies or individuals. Often the best way to do that is by first profiling your data and then looking for tables that contain well populated name fields, or address fields, or contact information fields."
"Hey Justyce, that is great feedback. We will look into adding examples like that to make it more clear to users how each scrubber is meant to work. In the meantime, you can try them out by selecting the \"View Added Cleaners\" toggle at the bottom of the screen on Prepare when you have a field selected. This will allow you to enter a test value and see the effect of each scrubber you have applied."
"Would be helpful if descriptions for cleaners contained not just the Cleaner Type and Description, but also contains an example. The reason for the example is that it would help first time users that are not familiar with the description and wants to know the effect of applying the cleaner."
"As we tag the fields with the suggested tag name, shouldnt those tag names disappear from the list ?"
"Hi Archana, I am having difficulty replicating the error you showed. Could you try refreshing the page and rerunning the query, and letting me know the exact steps you followed if the issue persists?"
"Create coalesce of full_name (full_name_2), extract first and last name from both of them, now we have four fields -\n1. full_name_First_name\n2. full_name_Last_name\n3. full_name_2_First_name\n4. full_name_2_Last_name\nthen apply regex on full_name_2_First_name and full_name_2_last_name to extract first four characters from each, this way we can extract all the fields required. This method allows us to extract numerous features."
"Hi Anish, great question. Currently, we don't have a tool or service that specifically enables this, but searching through your cloud storage in the Sourcing step is your best bet to see what data tables you have available for your use case. Given the unique circumstances many of the use cases our users need to tackle with the product, it can be a bit tricky to give a one-size-fits-all solution to picking the right data."
"Currently we don't have a functionality to view more than top 7 values but that could be something we can bring in future versions."
"[quote=\"ben, post:2, topic:777\"]\n.{4}$\n[/quote]\n\nThanks a lot, this does the trick."
"I am able to query all other fields on Query Builder but not on Singleton, as I am getting \"Error occurred while fetching the data\"\n\n![image|690x336](upload://oaCfQ0AtN5jC2ISWajCPNYLICWp.png)"
"Is there is a way where user(s) can use a universal formula(trick/steps) to arrive at the right source for any given usecase?"
"How do I view more than the 7 top values and patterns in Profile output? That would be very helpful full getting a better understanding of the quality of my data fields during the Profile step. \n\n![Screenshot from 2024-04-29 06-46-10|690x305](upload://uWULgveOLAiMkPqR5N0N0LRDhYD.png)"
"Hey Mukul, thanks for your post. Try using the RegexFilter scrubber and use the following regex: \n\n.{4}$\n\nThis should return back the last 4 characters of the string. Hope this helps!"
"While attempting to connect two datasets which differ in the number of digits shown for an account number, I faced a blocker. The transaction data only shows last 4 digits of the account number, while accounts reference data shows the entire account number. If I could take the last 4 digits out of the reference data, I could potentially use this for trustworthy PII.\n![image|290x78](upload://1dbXJIsleBk6qeo2A8cy98Y6S7g.png)\n\n![image|313x127](upload://qrUFZCR0oQ1xU328VnTVSDgK8DF.png)"
"The selecting upto 4 fields during the source step is ONLY FOR VIEW PURPOSE\nWhen a file has is selected and upto 4 fields are chosen and clicked \"Add to selected Data Source(s)\" - The entire file will be profiled. The upto 4 fields option is just to see the first 10 records and confirm if correct file is chosen and then the entire file is profiled by clicking \"Continue to Profile\""
"I’ve come across a strange issue/bug with the “Address” cleaner on the Prepare Step. After I add this cleaner, if I go back and try to select another parsed field to add, it gives me this error message saying - “Tag: parsed_first_name is already used in Source: Tag: parsed_first_name is already used in Source: amazonprime_streams.”.\n\nHere are the steps to reproduce it,\n\n1. Choose a canonical address field from a source\n2. Select a couple options from hint box - for example, “City” and “State”\n3. Add the Canonical Field Name\n4. Press on “Add Cleaner to Field”\n5. Go back and add select “Postcode”\n6. And press on “Add Cleaner to field\"\n\n![Bug Duplicate Field Error in Address Cleaner - screenshot|690x412](upload://mSlKEtmWaCd52qd0NHdweHDTN5h.jpeg)"
"***WARNING - DO NOT SHARE, FOR INTERNAL USE ONLY!***"
"\n  * My email was created before 02/01/24, how do I change to tresata.ai emails? \n    * Follow the guide [here](https://apps.google.com/supportwidget/articlehome?article_url=https%3A%2F%2Fsupport.google.com%2Fa%2Fanswer%2F7504451&assistant_id=generic-unu&product_context=7504451&product_name=UnuFlow&trigger_context=a) to begin using tresata.ai email\n  * My email was created after 02/01/24, am I required to change anything?\n    * No, No action is required.\n  * Why can’t I use my tresata.ai email to sign in?\n    * tresata.ai is an alias set on your real address allowing you to send from it. Your true email and username is still [johndoe@tresata.com](mailto:johndoe@tresata.com). For this reason you cannot use your alias to sign in.\n  * I was able to sign up for a platform with my tresata.ai email. How does the above not apply?\n    * The above rule only applies if you are using sso to sign into the account. tresata.ai emails can however be used to sign up directly within other platforms as long as we are not using an identity provider connection.\n\n* What if I want to use my tresata.ai email for all platforms?\n  * This is possible however we will not be transferring your entire domain over to .ai due to it being too costly in terms of connections breaking to your current email. *"
"You can follow the steps given below and start using your following email IDs to respond or send new emails:\n\nThe steps to follow are:\n\n* In your Gmail, click on Settings icon in the top right corner and then click on 'See all settings’\n\n![|170x69](upload://kJrc3FOIQlfRmFYgC4ITxxraMH3.png)\n\n![|309x85](upload://bINY2gI4Bk9AzejYMoLHPYaFqz2.png)\n\n* Go to 'Accounts' tab and in the 'Send mail as' section, click on 'Add another email address' link\n\n![|627x64](upload://5drIQOZg66dIla6xck0m6GrUrXC.png)\n\n* Enter your tresata.ai email IDs in the 'Email address' section and click the 'Next Steps' button.\n  * Example of this would be johndoe@tresata.com would now be able to use johndoe@tresata.ai\n* You should be able to see your tresata.ai email address listed, click on 'make default' option for that to be the email address for new emails or responses to any emails.\n* Please ensure you change your email signature as well in the 'General' tab.\n\n![|624x267](upload://k2qfJebC3dIgPrwkkAEWlxhw6rS.png)"
"Ideally the Cross-Account Azure Storage Access using Managed Identities caters to below categories\n1)Technical Audience:\na)Granting Cross-Account Azure Storage Access with Managed Identities in \nb)Leveraging Managed Identities for Secure Cross-Account Storage Access in \n2)Audience New to Managed Identities:**\na)Unlocking Secure Cross-Account Storage Access in Azure with  (using Managed Identities)\nb)Simplify Secure Azure Storage Access Across Accounts in  with Managed Identities\n3)Benefit-Driven:**\na)Streamline Secure Data Collaboration in Azure Storage with  and Managed Identities\nb)Boost Security and Efficiency: Cross-Account Azure Storage Access in  via Managed Identities**"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"![output_final|video](upload://9wkAkzQd1PuzMV10royH7On0iPb.mp4)"
"![Orchestrate_Final|video](upload://6bcTOYRkdzXhIKujWMKym7h0MJI.mp4)"
"![Enrich_Full_Video|video](upload://asTLKWyN4EAQ5iXMKsJs3EJ7Rl7.mp4)"
"![full_validate|video](upload://qsTWBSZMCEpaP4E5cPo8u2j3HIG.mp4)"
"![trimmed_connect|video](upload://fKgFYFURX58QDTc1vWX6zjmylII.mp4)"
"![Profile|video](upload://3xtilRJHr9ngxeK8kPW5xDaOMIN.mp4)"
"![trimmed_prepare|video](upload://mQzU4wiHRmQGzoXtPgCplr2KGnd.mp4)"
"![Dashboard_Final|video](upload://5lg7mi3wS4ZIhoTyX304JuG25ky.mp4)"
"![Source|video](upload://97l4dKToJMqNMmgirC9dQMToZGS.mp4)"
"Happy Thursday Community, looking forward to hearing your thoughts & questions around what we just shared :arrow_up: to keep the discussion going."
"**Q. Does the product offer on-premise hosting support?**\nCurrently we do not support on premise implementation of our software, we provide AWS and Azure as an out of the box options and we do have plans to extend it to other cloud providers like GCP. \n\n**Q. How are privacy concerns addressed?**\nWe do not collect or store any user data. Instead, our application integrates with users' cloud storage accounts, where their data remains securely stored. Our software is designed to run directly on this stored data, facilitating efficient and secure data transformation processes without ever compromising the integrity or privacy of the user's information.\n\n**Q.How does the product function on my cloud without data transit and extraction? What resources are deployed onto my cloud?**\nOur application uses your cloud storage (e.g., Amazon S3 Bucket) directly for file operations. All other application components, including code, APIs, and frontend, are hosted within our cloud environment\n\n**Q. How is ingress traffic managed if the data is kept private and off the internet?**\nWe don't employ private network integration between two cloud accounts. Instead, we prioritize security through industry best practices, including encryption, and access control services like AWS IAM and Azure Entra ID for secure data access.\n\n**Q. Can other resources from my environment be leveraged, apart from storage?**\nCurrently we do not user any resources from user accounts, Software is deployed only on Tresata's cloud account and serves as SaaS platform\n\n**Q. How is live stream data handled by the product?**\nTresata currently utilizes AWS S3 and Azure Storage accounts for storage, which aren't primary sources of live data but support updates reflected in the UI. For real-time data streams, we are exploring other cloud service options such as Kinesis.\t\n\n**Q. How does the product detect anomalies in data?**\nOur product offers statistical information about the data in the Profile step, empowering users to identify and address anomalies effectively\n\n**Q. How does the product handle edge cases where a user re-registers with minimal information changes?**\nUser information is stored with reference to their email address. One cannot sign-up to the product with the same email address which they have used previously. Incase user is facing any trouble to sign in they will have to contact the support team at support@tresata.com\n\n**Q. How does the product handle different file extensions, like UTF-16, UTF-18, or binary code?**\nCurrently our product does not offer options for the conversion of encoded data. Users are required to have their data pre-encoded and prepared for processing as a prerequisite for using the system\n\n**Q. How tolerant is the product during data breakage incidents?**\nOur product features an autosave option for each step, ensuring that when a user initiates a process, it continues to run and complete in the background, even if they experience a loss of connection or close the window. Additionally, users can manually save their configurations at any step, and these saved configurations will be available to them when they log in again, providing a seamless and convenient experience\n\n **Q. How are manually added notes helping users?**\nWhen multiple users are working on the same product, they have the ability to log their findings and notes on each page, which are then visible to other users. This feature enhances the collaborative work experience, fostering better communication and shared insights among team members.\n\n**Q. Are there any cleaners to help standardize time zones or others that might suit specific use cases?**\nCurrently the majority of our pre-built data cleaners are designed to US data formats. However we have plans in the pipeline to incorporate region-specific standards, ensuring that our product becomes even more versatile and accommodating for users around the world\n\n**Q. Are the note features manually written? How does this facilitate team communication?**\nAny user who has access to the product can add notes for each step, these will be visible for other users working within the same product.\n\n**Q. Do we have any plans for introducing cleaners for different standards, like Indian standards?**\nCurrently the majority of our pre-built data cleaners are designed to US data formats. However we have plans in the pipeline to incorporate region-specific standards, ensuring that our product becomes even more versatile and accommodating for users around the world\n\n**Q. Is the output of this file one integrated data output?**\nAfter the successful completion of a pipeline, our system generates a single consolidated output. To enhance user flexibility, we offer the option for users to select specific sources and their respective fields from each step that they want to include in the final output, allowing for a customizable data output experience\n\n**Q. How can we tune the logic to identify cases where the same person books twice with slight input variations?**\nOur 'Connect' steps offer users the flexibility to create their own custom connection logic. In cases where users suspect that one of their fields might lead to duplicates in the table, they have the option to configure the connection using another unique field or a combination of fields, effectively addressing and eliminating these edge cases to ensure data accuracy and reliability."
"**Thank You to all attendees:**  for the active participation and continued engagement of the community. \n\nLooking to hear from our active community members @escchandru @drraghavendra99 @user7 (Fazil) and many more who interacted with @Iniyan @Shantanubose @Jubin @Mukul @archana @user1 @Shonan.bhalla & the rest of our extended team."
"Happy Friday Community Members,\nI trust this message finds you well. Following our recent event on 29th November 2023, \"DATA FOR AI (FOR REAL),\" we wanted to further our discussions of the AI-focused product demonstration in this space or discussion board. \nWe hope you enjoyed the enriching experience of our recent event.  \n\n**Reflections and Inquiries:**\n\nThis post serves as an official discussion hub for all insights, questions, and ongoing dialogues related to the showcased product capabilities. Your reflections on key takeaways and inquiries are highly encouraged.\n\n**Key Points for Discussion:**\n\n* Feel free to pose any questions or seek clarification on the product functionalities from the event. \n\n* Engage with fellow community members to foster meaningful discussions.\n\n* Contributing to the Community through active participation, by assisting fellow members or by sharing your noteworthy observations contributes significantly to our collective knowledge.\n\n**📣 Ongoing Updates:**\n\n\n**👀 What caught your eye?** \n\nBelow are the questions you asked us. We understand that discussions continue beyond the event. We'll be actively monitoring this post, providing updates, answering your questions, and sharing any additional information that may arise.\n\n**DEVOPS QUESTIONS**\n\n1. **On-Premise Hosting Support:** Does the product offer on-premise hosting support, and how are privacy concerns addressed?\n\n2. **Cloud Resource Deployment:** How does the product function on my cloud without data transit and extraction? What resources are deployed onto my cloud host?\n\n3. **Handling Ingress Traffic for Private Data:** How is ingress traffic managed if the data is kept private and off the internet?\n\n4. **Leveraging Other Resources from the Environment:** Can other resources from my environment be leveraged, apart from storage?\n\n5. **Live Data Stream Handling:** How is live stream data handled by the product?\n\n**DATA ENGINEERING QUESTIONS**\n\n1. **Anomaly Detection in Data:** How does the product detect anomalies in data?\n\n2. **Handling Edge Cases in User Re-Registration:** How does the product handle edge cases where a user re-registers with minimal information changes?\n\n3. **File Extension Handling:** How does the product handle different file extensions, like UTF-16, UTF-18, or binary code?\n\n4. **Data Breakage Tolerance:** How tolerant is the product during data breakage incidents?\n\n5. **Processing PI Data Securely:** What measures are in place for the secure transmission of personally identifiable data?\n\n**PRODUCT-RELATED GENERIC QUESTIONS**\n\n1. **Utility of Manually Added Notes:** How are manually added notes helping users?\n\n2. **Customizable Cleaners:** Are there any cleaners to help standardize time zones or others that might suit specific use cases?\n\n3. **Note Features - Manual Input:** Are the note features manually written? How does this facilitate team communication?\n\n4. **Cleaners Time Frame and Standards:** Do we have any timeframe for introducing cleaners for different standards, like Indian standards?\n\n5. **Integrated Data Output:** Is the output of this file one integrated data output?\n\n6. **Customizing Logic for Duplicate Bookings:** How can we tune the logic to identify cases where the same person books twice with slight input variations?\n\n\n**Call to Action:**\n\n* **Ask Questions:** If you have any more questions, kindly share them so we can address them accordingly. \n\nI appreciate your commitment to our community. We aim that knowledge is exchanged, questions find answers & connections are strengthened.\n\nThank you for your continued support."
"\n## INTRODUCTION:\n\nIn the modern cloud computing landscape, the ability to securely access and process data across different Azure accounts is essential for building robust and collaborative applications. This write-up outlines a method to achieve cross-account Azure Storage access using Managed Identities, maintaining security and simplicity while accessing a client's storage account from a different Azure account.\n\nIn a world driven by cloud computing and interconnected ecosystems, security and seamless access across multiple Azure accounts (tenants) have become paramount challenges. The traditional methods of accessing the client's storage account using access keys and passwords are not recommended due to the associated security risks.\n\n## SCENARIO:\n\nThe outlined solution works for all scenarios where the web application resides in one Azure account (Tenant A), and it needs to access a storage account, which is hosted in a different Azure account (Tenant B).\n\n## SOLUTION:\n\nTraditionally, managing access through access keys or passwords can introduce security risks and complexity. However, this can be accomplished more securely using Managed Identities and Federated Credentials.\n\n## STEPS TO ACHIEVE CROSS-ACCOUNT STORAGE ACCESS:\n\n1. Create Storage Account in the Client's Account:\n\nBegin by setting up a storage account within a different Azure account (Tenant B). This account will be the target for data access and processing. Enable Hierarchical namespace enabled.\n\n![|624x328](upload://zHVPf0OL2sZFC7EFrDsxmC8KUfu.jpeg)\n\n2. Configure Firewall Settings:\n\nTo enable cross-account access, configure the firewall settings of the storage account in Tenant B to include the necessary IP address ranges. For the easy and fast test, you can add the address range 0.0.0.0/0 to allow access from any IP address.\n\n![|624x328](upload://kw9pJmDh5R7xHvYnbELwC9kjIyh.jpeg)\n\nNote: This is a test case scenario, It may not be recommended to configure for all the public IP( 0.0.0.0/0 ) addresses for the Production environment.\n\n3. Create Managed Identity:\n\nIn the same client's Azure account (Tenant B), create a Managed Identity. Managed Identities provide a secure way to authenticate resources and services within Azure without the need to expose sensitive credentials.\n\n![|624x203](upload://1SRZnVX5Fg2aPpbYBDakRi2qjM4.png)\n\nFor more information on how to create managed identity: [click here](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm)\n\n4. Attach Federated Credentials:\n\nTo establish trust between your Web Application’ / product's Azure account (Tenant A) and the other Azure account (Tenant B), attach Federated Credentials to the Managed Identity in Tenant B. When configuring the Federated Credentials, provide:\n\n* A name for the federated identity\n* The Issuer URL, which is the URL of the AKS cluster in your organization's Azure account (Tenant A)\n* The Subject Identifier, composed of the namespace and service account name in the AKS cluster. For example: system:serviceaccount:<namespace>:default. This identifies the trusted entity in Tenant B.\n\n![|624x203](upload://e8BppBwa5cNxybgXYmhwn3AJmFZ.png)\n\n5. Role Assignment:\n\nAssign a suitable role to the Managed Identity within the storage account in Tenant B. For instance, grant the role of \"Storage Blob Data Owner.\" This role ensures that the Managed Identity has the necessary permissions to access and manage data within the storage account.\n\n![|624x328](upload://6JLVCpXey7MSZLrOua6y1FBr6gP.png)\n\n6. Retrieve Tenant B's Credentials:\n\nIn your Azure account (Tenant A), obtain the Client ID of the Managed Identity created in Tenant B and the Tenant ID of the other account’s subscription. These credentials will be used to authenticate the access from Tenant A to Tenant B.\n\nE.g. In our spark OPTS we would set these credentials to access the azure storage account:\nclientID: xxxxxxxxxxxxxxxxxxxxx\ntenatID: xxxxxxxxxxxxxxxxxxxxx\n\n## CONCLUSION:\n\nBy following these steps, you've established a secure and controlled method to access another Azure Storage account from your web application / product, even if they reside in different Azure accounts (Tenants). The combination of Managed Identities, Federated Credentials, and appropriate role assignments ensures a seamless, secure, and hassle-free way to process and manage data across accounts. This approach enhances the security posture of your application while promoting collaboration between different Azure tenants.\n\n#devops #managed-identities #learn #howto #tutorial"
"Greetings Feedback Fiest'ai Community,\n\nWe're still reeling in all the excitement from our incredible Feedback Fiest'ai  in-person event! Your active participation and valuable insights made it a huge success. It was also wonderful to see some familiar faces from our earlier event @Hackathon return to connect with us. \n\n**Event Highlights:**\n- Engaging discussions with industry experts.\n- Thought-provoking feedback and discussions.\n- Memorable moments and connections made.\n\n📷 Sharing a few snapshots from the event:\n\n![#CommunityEngagement\"|690x388](upload://A0cB1DRYD4Fr6tjbtwU92ZGWeZz.jpeg)\n\n When Vision Meets Community: **Dialogues with Our Founder** 🤝 #CommunityEngagement\n\nSpeakers at the Event\n![Speakers at the event|666x500](upload://rBnmCIFBneyp6Ql0bPEACcyZcPN.jpeg)\n\n\n\"**The Triple Impact**: Speakers Who Resonate with Our Community 🎯 #CommunityConnect\"\n\n![Main Speaker sharing the Product Demo|690x388](upload://mFe01n9hwFZkZ1Demu1GSdTagcO.jpeg)\n\n**Setting the Stage Ablaze**: Our very own Keynote Speaker's Insightful Presentation 🔥 #KeynoteSpeaker\n\n![Feedback Time|666x500](upload://b9mc6LpUz5dpQE0VnMGeSd6qGpa.jpeg)\n\n**Post-Presentation Cheers**: Cheers to Feedback, Swag, and Community! 🙌 #FeedbackMatters\n\n\nAt the 'Feedback Fiest'ai' event, your contributions were invaluable. We've listened, learned, and here are some of the key takeaways:\n\n- Insight 1: Talk around ETL which we acknowledged and shared a detailed post \n- Insight 2: Positive feedback from all the data enthusiasts present to join our future focussed groups\n\nWe're truly grateful for your contribution to our community. If you have more insights to share or suggestions for future events, please don't hesitate to reach out. Let's keep the momentum going, looking forward to see you in the next discussion!\n\n👥 Tagging our dedicated group, @Feedback.Fiestai to ensure everyone sees this post.\n\n\nWarm regards,\nCommunity Team"
"Thank you for your insightful post! @Shantanubose 🙌 . It would be great if we can turn this into a weekly/fortnightly series to share more knowledge across the community. \n\nETL is indeed a crucial part of our data journey, and this explanation makes it so much more relatable in simplifying complex concepts.\n\nThis adds up as continuation to the presentation made by @DimitriosTheGreek at our Feedback Fiest'ai' event. \n\nWe would love to hear more perspectives on this topic. Whether you're a seasoned expert or just starting on this journey, your input matters. Have you had any interesting experiences related to ETL and its role in handling Big Data with your work? Any questions or additional insights to add?\n \nLet's make this discussion even more enriching together! 🚀 Looking forward to hearing from everyone! @DataEngineering \n\n\n #CommunityEngagement #ETL #BigData #JoinTheDiscussion"
"In a world driven by data, there exists a crucial yet often overlooked figure, ETL, which stands for Extract, Transform, Load. ETL is like a skilled artisan, gathering raw information from various sources, cautiously reshaping it into a comprehensible and practical form for us.\n\nETL serves as the vital link between chaos and clarity. It takes the tangled data from its origins, refines it with care, and deposits it in a format that we can readily grasp and employ. Think of it as the behind-the-scenes facilitator that empowers businesses, analysts, and decision-makers by extracting invaluable insights. These insights, in turn, enable well-informed choices amidst the complexities of our digital world.\n\nSo, when we encounter ETL in our data endeavours, acknowledge it as the unsung hero that transforms chaos into comprehension. \n\n![Simplify, Streamline, Succeed: ETL for Big Data|500x500](upload://2W9o9xI0DjQCxKa7FYbRcJTIxak.jpeg)\n\n\n**What exactly is ETL?**\n\n\nThink of ETL as a superhero team, with three main members:\n\n1. Extract: This member is responsible for gathering data from different places, like databases, spreadsheets, or even social media. It's like a data detective, collecting clues from all over.\n\n2. Transform: Once the data is gathered, it needs a makeover. The Transform member cleans it up, organises it, and makes sure it's all in the same expression. Imagine taking messy puzzle pieces and putting them together to form a clear picture.\n\n3. Load: Now, the Load member carries the transformed data to its new home, a database or a data warehouse. It's like moving all your neatly organised files into a safe storage space.\n\n\n**Why ETL Matters for Big Data**\n\nBig Data is like a giant treasure chest filled with information. It's vast, it's fast, and it's diverse. Without ETL, this treasure chest would remain locked, and we'd never uncover its secrets. Here's why ETL is so important for Big Data:\n\n1. Handling the Data Beast: Big Data is enormous and comes in many different shapes and sizes. ETL is like the magical key that helps us understand and use this data effectively.\n\n2. Quality Control: It takes care that data is free from errors, duplicates, and inconsistencies, making it easier to work with and analyse.\n\n3. Speed and Efficiency: ETL makes sure the data journey from source to destination is swift and smooth. It's like having a super-fast delivery service for your data.\n\n4. Handling Changes: Just like the weather can change, data can change too. ETL is adaptable, so it can adjust to evolving data sources, formats, and requirements, ensuring that your data processes remain reliable and up-to-date..\n\n\n**Mapping the Data Maze: Why It's a Big Challenge**\n\nIn the world of data, there's another puzzle to solve: data mapping. Data mapping is like creating a map that shows how data from one place relates to data in another place. It might sound simple, but when you're dealing with Big Data, which is like millions and trillions of records, it becomes a really tough task:\n\n1. Complexity: Big Data isn't just big; it's also complicated. Data can come in different forms. Mapping all of these together can be like solving a giant jigsaw puzzle.\n\n2. Human Error: When humans create these maps manually, mistakes can happen. It's like trying to draw a perfect circle freehand; it's tricky.\n\n3. Time-Consuming: Imagine mapping a city with millions of streets and buildings by hand. It would take forever. Mapping Big Data manually is similarly time-consuming.\n\n\n**AI and ML in Data Mapping:**\n\nImagine having Artificial Intelligence and Machine Learning to assist us with ETL and data mapping. They're like the magical helpers that make our digital world even more amazing.\n\n1. Automatic Matching: AI can automatically find similarities between data. It's like having a digital detective that spots patterns and connections between different pieces of information.\n\n2. Understanding Meanings: These smart technologies can understand the meaning of data, even when it's not obvious. It's like having a language expert who knows what words mean, even if they're in a foreign language.\n\n3. Smart Suggestions: AI can suggest how to map data and even recommend transformations. It's like having a wise advisor who tells you the best way to arrange your puzzle pieces.\n\n4. Handling Changes: Just like AI can recognise when someone changes their appearance, it can also notice changes in data and adapt the mapping accordingly.\n\n\n**Neural Networks Between Data Sets:**\n\nImagine that data sets are like pieces of a gigantic puzzle, and we want to see how they all fit together. AI and ML can do this too:\n\n1. Spotting Relationships: These smart technologies can uncover hidden connections between different data sets. It's like discovering that your favourite book and your favourite movie have the same author.\n\n2. Grouping Similar Things: AI and ML can group similar data together. It's like having a magical organiser who puts all your (similar - word)  toys in one box, all your books on one shelf, and all your clothes in one drawer.\n\n3. Predicting What's Ahead: AI and ML allow us to foresee future events, just like an expert predicting how prices might change in the near future, similar to assessing inflation trends. Using AI and ML analysis, we can make smart guesses about what's likely to happen.\n\nIn conclusion, while ETL processes are strong on their own, the integration of AI and ML elevates them to an entirely new level of power and precision. AI and ML go beyond being mere technologies; they act as valuable allies that help us understand complex information, solve intricate problems, and make educated predictions. With these digital companions available, we can tap into the immense possibilities of our data-driven world, taking our ETL processes to unprecedented heights of efficiency and insight."
"Hello there, Community!\n\nWe're thrilled to have you on board as we begin an exciting journey together. This space is all about support and collaboration, and we want to ensure you have the resources you need to make the most of your experience here. I'm here to guide you through some quick links that will help you navigate our community seamlessly.\n\n1. **Code of Conduct:** Our community thrives on mutual respect and collaboration. Please take a moment to familiarize yourself with our guidelines to ensure we all have a positive experience. :arrow_heading_down:\n\n[quote]\nhttps://community.tresata.com/t/creating-a-harmonious-community-our-guidelines-code-of-conduct-for-an-incredible-journey/479\n[/quote]\n\n2. **Clarity on username:**  We've noticed that a few of our community members are using generic usernames like \"user1\" or \"user2.\" To ensure your presence should reflect your name & brand we believe the username can help express yourself & stand out. Learn more about the importance of a distinct username.:arrow_heading_down:\n\n[quote]\nhttps://community.tresata.com/t/unleashing-the-power-of-your-unique-identity/478\n[/quote]\n\n3. **Welcome to the Community**: Congratulations on joining the Tresata community! Here, you'll discover a wealth of information about Tresata products, industry best practices, a dynamic space for sharing ideas & knowledge. This interactive platform offers you the chance to explore informative topics, join discussions, and connect with Tresata experts. Explore our community features. :arrow_heading_down:\n\n[quote]\nhttps://community.tresata.com/t/welcome-to-tresata-community/210\n[/quote]\n\n4. **Community Experience & Engagement:** As your community manager, I extend a warm welcome. With our shared passion for data science & engineering, I'm confident that this space will become a hub for knowledge sharing, collaboration, and building meaningful professional connections. Learn more about your community experience. :arrow_heading_down:\n\n[quote]\nhttps://community.tresata.com/t/welcome-to-tresatas-community-lets-make-the-most-of-our-community-experience/384\n[/quote]\n\n5. **Community Engagement Tips:** We're here to help you get the most from your community participation. To make your posts engaging and informative:\n\n* Start with a clear and concise title.\n* Craft an engaging introduction.\n* Use clear and straightforward language.\n* Include relevant links, images, or other media.\n* Use formatting to make your posts reader-friendly. :arrow_heading_down:\n\n[quote]\nhttps://community.tresata.com/t/12-tips-on-how-to-make-an-engaging-post/425\n[/quote]\n\nRemember, this community is all about you, and your active participation is what will make it truly shine. Don’t hesitate to dive into discussions, ask questions, and share your valuable insights. Together, we’re creating a hub of knowledge and innovation…\n\nSee you around the community!\n\nCheers,\nVarun - Your Community Manager :rocket:"
"Get ready for an exciting evening of innovation and collaboration as Tresata presents its latest SaaS product designed to revolutionize the way we approach data and AI.\n\n**Key Highlights:**\n\n1. **Product Unveiling:** Witness the unveiling of our groundbreaking AI-powered SaaS product, the first of its kind.\n2. **Thought Leadership:** Gain insights into the driving forces behind this innovation, led by industry experts and tech leaders.\n3. **Networking:** Connect with like-minded peers in the Bangalore tech community.\n4. **Prizes & Surprises:** Participate in the product discussion and enter the raffle to win exciting prizes.\n\n**Event Details:**\n\n* **Date:** Tuesday, October 17\n* **Time:** 6:00 PM - 8:00 PM\n* **Location:** WeWork Embassy TechVillage\n\nLink to the eventbrite posting -> [https://dataforai.eventbrite.com](https://dataforai.eventbrite.com/)\n\n #dataforai #dataengineering #datAI"
"Get ready for an exciting evening of innovation and collaboration as Tresata presents its latest SaaS product designed to revolutionize the way we approach data and AI.\n\n**Key Highlights:**\n\n1. **Product Unveiling:** Witness the unveiling of our groundbreaking AI-powered SaaS product, the first of its kind.\n2. **Thought Leadership:** Gain insights into the driving forces behind this innovation, led by industry experts and tech leaders.\n3. **Networking:** Connect with like-minded peers in the Bangalore tech community.\n4. **Prizes & Surprises:** Participate in the product discussion and enter the raffle to win exciting prizes.\n\n**Event Details:**\n\n* **Date:** Tuesday, October 17\n* **Time:** 6:00 PM - 8:00 PM\n* **Location:** WeWork Embassy TechVillage\n\nLink to the eventbrite posting -> [https://dataforai.eventbrite.com](https://dataforai.eventbrite.com/)\n\n #dataforai #dataengineering #datAI"
"\n🚀 **Join Tresata for a Spectacular In-Person Event!** 🚀\n\nGreetings, Data Enthusiasts!\n\nIntroducing the World's 1st **'Data for AI'** product. Tresata is thrilled to invite you to our upcoming tech community event, where we're bringing together brilliant minds to explore the fascinating world of data and AI.\n\nHere are five compelling reasons why you shouldn't miss it:\n\n**1. Be First to Witness Cutting-Edge Innovation:** 🔮 Get an exclusive preview of our revolutionary AI-powered SaaS product, designed to simplify the creation of trusted data products. This isn't just a launch; it's a technological leap forward.\n\n**2. Uncover the Vision with Industry Thought Leaders:** 🤓 Discover the \"why\" behind our new product as we delve into the innovative thinking of data engineers. Join the discussion on how this product is shaping the future for data experts, by data experts.\n\n**3. Connect and Expand Your Network:** 👯 You'll have the chance to meet like-minded data engineers and tech aficionados. This isn't just a one-time event; it's the entry point to our thriving virtual community where you can connect beyond the event.\n\n**4. Win Exciting Prizes:** 🏆 Participate in the product feedback session and automatically enter our prize raffle. Trust us; these are not your everyday prizes, don't wish to giveaway more information :mask:. Get ready for a delightful surprise!\n\n**5. Enjoy the Party Atmosphere with Swag & Snacks:** 🎉 Tech innovation should be celebrated! Expect a fun and festive environment, complete with awesome swag and mouthwatering bites.\n\n**Event Details:**\n\n* **Date**: Tuesday, October 17th\n* **Time**: 6 PM to 8 PM IST\n* **Location**: WeWork Embassy TechVillage\n\nSave the date and stay tuned for more information as the event approaches. We can't wait to welcome you to a memorable evening of tech, innovation, and community.\n\nLink to the eventbrite posting -> [https://dataforai.eventbrite.com](https://dataforai.eventbrite.com/)\n\nCheers to the future of data and AI! 🥂\n\nYour friends at Tresata\n\n #dataforai #dataengineering #datAI"
"Welcome to Tresata's Feedback Fiest'ai, the space for your valuable input and collaboration.\n\nWhat to Expect\n\n1. Engage in discussions about our products, events, and community\n2. Interact with team Tresata, industry peers &  fellow users\n3. Participate in feedback sessions and innovation dialogues\n4. Follow Community Guidelines\n\n\nHave questions or need assistance? Contact our community team.\n\nThank you for being part of Tresata's community.  Join us in building a stronger data engineering community."
"**Streamline Your Workflow with Orchestrate Feature**\n\n\n***Table of Contents:***\n\n\n**1. Introduction**\n**2. How can I automate workflows in Tresata for efficient time management?**\n**3. What are the differences between Manual and Automatic Orchestration in Tresata?**\n**4. How can I monitor the progress of an orchestrated workflow in Tresata?**\n**5. What should I remember when setting up workflow orchestration in Tresata?**\n**6. What is the significance of automating and monitoring workflows in Tresata?**\n**7. Conclusion**\n\n\n---\n\n\n**1. Introduction**\n\n\nIn this guide, we'll explore how you can automate and monitor your workflows efficiently, saving time and ensuring smooth data processing.\n\n\n---\n\n\n**2. How can I automate workflows in Tresata for efficient time management?**\n\n\n**A:** To automate workflows in Tresata, follow these steps:\n\n\n* Orchestrate Workflow: After creating your workflow, click on the \"Automate Workflow\" button.\n  * Manual Orchestration:\n    * Select the workflow you want to run using the search bar.\n    * Choose \"Manual\" as the schedule type.\n    * Click \"Confirm & Orchestrate\" to trigger the workflow when you desire.\n  * Automatic Orchestration:\n    * Choose \"Automatic\" and set the time interval (e.g., daily at 1 pm).\n    * Click \"Confirm & Orchestrate\" to run the workflow automatically at the specified intervals.\n\n\n---\n\n\n**3. What are the differences between Manual and Automatic Orchestration in Tresata?**\n\n\n**A:** In Tresata, there are two types of Orchestration:\n\n\n* Manual Orchestration: You select when to run the workflow manually. It only executes when you trigger it.\n* Automatic Orchestration: The workflow runs automatically at set time intervals (e.g., daily at 1 pm) without manual intervention.\n\n\n---\n\n\n**4. How can I monitor the progress of an orchestrated workflow in Tresata?**\n\n\n**A:** You can monitor an orchestrated workflow's progress in Tresata using these methods:\n\n\n* Progress Bar: You can check the progress of your running workflow, from a single stage to all stages, using the progress bar.\n* Restart Workflow: If changes are needed or you want to rerun the workflow, use the restart option. Confirm the restart when prompted.\n* Cancel Workflow: If a situation demands changes in the scheduled run, you can cancel the running workflow.\n* Start, Edit, and Delete: You can manually start a run, edit timelines, or delete a scheduled workflow.\n\n\n---\n\n\n**5. What should I remember when setting up workflow orchestration in Tresata?**\n\n\n**A:** Here are important points to remember when setting up workflow orchestration in Tresata:\n\n\n* The name of the workflow cannot be changed once chosen.\n* Workflow names cannot contain special characters or be duplicated.\n* Scheduled workflows can be changed or deleted at any time.\n* This completes the Tresata workflow stages. If you have any further questions or issues, feel free to ask in the community for assistance.\n\n\n---\n\n\n**6. What is the significance of automating and monitoring workflows in Tresata?**\n\n\n**A:** Automating workflows in Tresata is essential for efficient time management in a fast-paced world. It allows you to schedule and run workflows automatically, saving time and ensuring data processing occurs at the right moments. Monitoring workflows is equally crucial as it helps track progress, detect errors, and make necessary adjustments. Whether you need to check progress, restart, cancel, start, edit, or delete a workflow, these actions ensure the smooth execution of your data processing tasks. Automated workflows and monitoring capabilities streamline data management and improve overall efficiency.\n\n\n---\n\n**7. Conclusion**\n\nIn conclusion, you have the tools to make your workflow management effortless and efficient. Whether you choose manual or automatic orchestration, monitoring your workflows ensures they run smoothly and produce the desired results.\n\n#WorkflowAutomation, #Orchestration, #TimeManagement, #tutorial #Efficiency"
"**Optimizing Data with Tresata's Enrich Feature**\n\n\n***Table of Contents:***\n\n\n**1. Introduction**\n**2. What is Tresata's Enrich feature, and why is it important?**\n**3. How can I choose the best fields for my Golden Record using Tresata's Enrich feature?**\n**4. How can I make sure I've applied the right logic to select my Golden Record using Tresata's Enrich feature?**\n**5. How do I access the final output generated by Tresata after using the Enrich feature?**\n**6. What is the significance of the final output generated by Tresata, and why is it important?**\n**7. Conclusion**\n\n\n---\n\n\n**1. Introduction**\n\n\nIn this guide, we'll delve into the importance of creating a \"Golden Record\" and how Enrich feature can help you consolidate and enhance your data, turning it into a valuable asset.\n\n\n---\n\n\n**2. What is Tresata's Enrich feature, and why is it important?**\n\n\n**A:** Tresata's Enrich is a crucial capability that helps you create a \"Golden Record\" for each of your TresataIDs, consolidating the best and most accurate information from various records. This is important because it enables you to have a single, comprehensive record for each entity, improving data quality and usability.\n\n\n---\n\n\n**3. How can I choose the best fields for my Golden Record using Tresata's Enrich feature?**\n\n\n**A:** Selecting fields for your Golden Record involves two main categories of enrichment logic in Tresata:\n\n\n* Field Preferences:\n  * Choose a Canonical Field.\n  * Use custom logic (Expression) to pick a value from a record that meets specific ranking criteria.\n  * Select a field based on the most frequent value (Count).\n* Table Preferences:\n  * Choose a complete dataset as the best information for the Golden Record.\n  * Use ranks to prioritize fields within the dataset.\n\n\nYou can configure these preferences using drag and drop operations in Tresata.\n\n\n---\n\n\n**4. How can I make sure I've applied the right logic to select my Golden Record using Tresata's Enrich feature?**\n\n\n**A:** To ensure you've applied the right logic for your Golden Record in Tresata, follow these steps:\n\n\n* Use Field Preferences: Select the best value for a field using Canonical Fields, custom logic (Expression), or the most frequent value (Count).\n* Use Table Preferences: Choose a complete dataset as the best information for the Golden Record and prioritize fields using ranks.\n* Profiled Heat-map: Utilize the profiled Heat-map to understand data metrics and select accurate canonicals for enrichment.\n* Initiate Enrich: When you've finalized your enrichment logic, click \"Initiate Enrich\" to apply all selected rules. You can add, edit, or delete rules as needed.\n\n\nThese steps will help you create an optimal Golden Record with the best attributes.\n\n\n---\n\n\n**5. How do I access the final output generated by Tresata after using the Enrich feature?**\n\n\n**A:** Accessing the final output in Tresata after using the Enrich feature is straightforward:\n\n\n* Universal (Per Data Source):\n  * From the list of Output sources, select \"Universal (Per Data Source)\" as shown in Screen 2.0.\n  * Choose the output schema for every data source (Raw Data, Cleaned Data, Enriched Data) by clicking the checkboxes next to each schema.\n* Enriched:\n  * Similar to Universal Output, select \"Enriched\" from the Output options.\n  * Here, one output schema will be generated for all the data sources, providing standardized output globally. Raw Data fields are not considered for Enriched Output.\n* Initiate Output: Once you've selected the desired schema(s), click the \"Initiate Output\" button to obtain your final output.\n\n\nThis final output contains the data with the best attributes that you've configured during the Enrichment process.\n\n\n---\n\n\n**6. What is the significance of the final output generated by Tresata, and why is it important?**\n\n\n**A:** The final output generated by Tresata is the culmination of your data transformation journey. It's significant because it represents the refined, high-quality data that's ready for use. This output contains Golden Records with the best attributes, making it highly valuable for downstream usage, such as analytics, reporting, and decision-making. In essence, it's the endpoint of the data processing journey, where messy data has been transformed into a clean, usable format, making it a critical step in the data pipeline.\n\n---\n\n**7. Conclusion**\n\nIn conclusion you have the power to create data assets of exceptional quality. By crafting Golden Records with the most accurate information, you enable better decision-making and enhance the usability of your data.\n\n#DataOptimization, #GoldenRecord, #DataEnrichment, #tutorial #DataQuality"
"**Mastering Data Accuracy with Tresata's Validate Step**\n\n\n***Table of Contents:***\n\n\n**1. Introduction**\n**2. What is the Validate step in Tresata, and why is it important for data accuracy?**\n**3. What are the key features of the Validate step, and why are they important?**\n**4. How can I evaluate the overall statistics provided by the Validate step?**\n**5. How can I evaluate the data integrity within a specific source using the Validate step?**\n**6. How can I use source overlaps to evaluate the quality of my data connections?**\n**7. Conclusion**\n\n\n---\n\n\n**1. Introduction**\n\n\nIn this comprehensive guide, we'll explore how this critical phase can help you assess and improve the accuracy of your data connections, ensuring that your insights and decisions are based on reliable information.\n\n\n---\n\n\n**2. What is the Validate step in Tresata, and why is it important for data accuracy?**\n\n\n**A:** The Validate step in Tresata is a crucial part of ensuring the accuracy of your data product. It comes right after the Connect step and provides valuable metrics to evaluate the performance of your data connections. This step is all about investigating the quality of your resolution and finding areas for improvement. It's like a health check for your data connections.\n\n\n---\n\n\n**3. What are the key features of the Validate step, and why are they important?**\n\n\n**A:** The Validate step in Tresata offers several important features:\n\n\n* Overcompression: This occurs when too many records are forced to link together, often resulting in incorrect connections. Loose Connect logic can lead to overcompression.\n* Undercompression: This happens when connections between records are missed, leading to incomplete data connections. This is often caused by very restrictive Connect logic.\n* Tresata: This unique identifier is used to identify entities across records and sources. It's crucial for understanding data connections.\n* Singletons: TresataIDs assigned to only one record, indicating records that haven't connected with any others.\n* Trapped: TresataIDs that connect records from one source only, showing records that have connected only within the same source.\n* Spanning: TresataIDs that connect records across different sources, revealing records that have connected with records from different sources.\n\n\nThese features help you identify issues with your data connections and understand the compression levels within your data.\n\n\n---\n\n\n**4. How can I evaluate the overall statistics provided by the Validate step?**\n\n\n**A:** The overall statistics provided by the Validate step give you a snapshot of your data's health. Here's how to evaluate them:\n\n\n* The # of Tresata IDs (TIDs): This shows how many entities have been identified within your data. Low TIDs compared to a high number of records might indicate overcompression, while too many TIDs could signal undercompression. It depends on your specific use case.\n\n\n* Avg. # of Records per TID: This statistic helps you understand compression. A higher ratio suggests more compression, while a lower ratio indicates less compression.\n* Singletons: The closer the ratio between records and TIDs is to 1:1, the higher the number of the singletons. This metric indicates how many records haven't connected with others.\n* Trapped and Spanning: These metrics reveal how much compression you're seeing. More compression leads to higher numbers of Trapped and Spanning.\n\n\nUnderstanding these statistics in the context of your specific use case is key to evaluation.\n\n\n---\n\n\n**5. How can I evaluate the data integrity within a specific source using the Validate step?**\n\n\n**A:** To evaluate data integrity within a specific source using the Validate step, you can look at the overall statistics on a source level. This allows you to pinpoint errors more accurately within that source. For example, if you notice unexpectedly high average records per TID within a source, it can indicate an issue with that source's data or your Connect logic. Granular source-level statistics help you investigate and improve the accuracy of your data connections.\n\n\n---\n\n\n**6. How can I use source overlaps to evaluate the quality of my data connections?**\n\n\n**A:** Source overlaps in the Validate step are valuable for evaluating data connections. Here's how you can use them:\n\n\n* Source overlaps show how many records have connected across different data sources.\n* By comparing source overlaps with your business logic and expectations, you can assess the quality of your data connections.\n\n\nFor example, if you're trying to identify customers who book tickets but don't check in for flights, source overlaps can help. A high percentage of records overlapping between booking and ticketing sources would indicate that most customers who book tickets also check in, which aligns with expectations.\n\n\nSource overlaps can validate your initial assumptions or point you to areas where your data connections may need improvement. Remember, the quality of your data connections is crucial for accurate insights and decision-making.\n\n\n---\n\n**7. Conclusion**\n\nYou can gain the tools and insights needed to ensure your data is accurate and trustworthy. By assessing compression, Tresata IDs, and source overlaps, you can fine-tune your data connections and make confident, data-driven decisions.\n\n#DataAccuracy, #DataValidation, #ValidateStep, #DataQuality #tutorial"
"**Unlocking Data Integration with Tresata's Connect Step**\n\n\n***Table of Contents:***\n\n\n**1. Introduction**\n**2. What is the purpose of the Connect step in Tresata?**\n**3. Why is connecting records important in data management?**\n**4. What are the main configuration options in the Connect step?**\n**5. What are some types of connectors available in Tresata's Connect step, and what are their functions?**\n**6. How do I execute the Connect step in Tresata?**\n**7. Conclusion**\n\n\n---\n\n\n**1. Introduction**\n\n\nIn this guide, we'll explore how this powerful feature can help you link and integrate records from different data sources, providing you with a comprehensive view of your data landscape.\n\n\n---\n\n\n**2. What is the purpose of the Connect step in Tresata?**\n\n\n**A:** The Connect step in Tresata aims to link and integrate records across different data sources and systems. Its primary purpose is to provide a comprehensive and accurate view of unique entities, even when data about these entities is scattered across multiple datasets.\n\n\n---\n\n\n**3. Why is connecting records important in data management?**\n\n\n**A:** Connecting records is crucial in data management because it allows organizations to gain a holistic understanding of entities, such as customers or products, by aggregating data from various sources. This comprehensive view enables better decision-making, personalized services, and improved customer loyalty.\n\n\n---\n\n\n**4. What are the main configuration options in the Connect step?**\n\n\n**A:** The main configuration options in the Connect step include:\n\n\n* Resolver Type: You can choose between \"Resolve\" and \"Against\" resolution types. Resolve connects all records within selected sources, while Against is a directional linkage connecting records from one set of sources to another.\n* Add Data Sources: You select the datasets you want to connect in this step.\n* Canonicals: These are the fields that can be used to build the logic for record connections.\n* Add Resolution Logic: You specify the fields and their combinations that, if matching, create a connection between records.\n* Add Link Shields: Shields are fields that, if inconsistent across records, prevent connections. They ensure data integrity.\n\n\n---\n\n\n**5. What are some types of connectors available in Tresata's Connect step, and what are their functions?**\n\n\n**A:** Tresata offers various connector types for linking records. Here are some examples:\n\n\n* Same: This connector type performs an exact comparison of strings in lowercase and does not consider null values.\n* Name: The name matcher parses raw name strings into components like first name, last name, middle name, etc., and performs matching on these parsed values.\n* Address: Similar to the name matcher, the address matcher parses raw address strings into components like house number, road, city, etc., and then matches based on these parsed fields.\n* CompanyName: The company name matcher parses raw company name strings into components like company name, activity, region, and legal, and performs matching based on these parsed fields.\n\n\nYou can choose the connector type that best suits your data.\n\n\n---\n\n\n**6. How do I execute the Connect step in Tresata?**\n\n\n**A:** To execute the Connect step in Tresata:\n\n\n* Configure the connector type based on your data (e.g., Same, Name, Address, CompanyName).\n* Click the \"Initiate Connect\" button to start the execution.\n* The execution time can vary depending on the dataset size, ranging from 30 minutes to 1 hour for a small dataset.\n* You can make changes to your logic before initiating the connection.\n\n\n---\n\n**7. Conclusion**\n\nWith Connect, you have the power to unite and harmonize your data, unlocking valuable insights and opportunities. Seamlessly integrate records, gain a holistic view, and enhance your data management capabilities.\n\n#DataIntegration, #RecordLinkage, #ConnectStep, #DataManagement #tutorial"
"**Unlocking the Power of Tresata's Prepare Step**\n\n\n***Table of Contents:***\n\n\n**1. What is the significance of the Prepare step in Tresata, and why is it important?**\n**2. What are Canonical Fields in Tresata, and why are they important in data preparation?**\n**3. How are Canonical Fields created in Tresata, and what is their role in data preparation?**\n**4. What are the steps to clean a field in Tresata's Prepare step, and why is field cleaning important?**\n**5. What is the significance of Regular Expressions (Regex) in data cleaning, and how can they be applied in Tresata's Prepare step?**\n**6. Can you provide examples of commonly used Regex filter expressions and their purposes in data cleaning?**\n**7. How can users track the cleaners applied to fields during the data preparation process in Tresata?**\n\n\n---\n\n\n**1. What is the significance of the Prepare step in Tresata, and why is it important?**\n\n\n**A:** The Prepare step in Tresata is crucial because it involves cleaning and standardizing data, which ensures the accuracy and reliability of your data product. Uncleaned data can lead to inaccuracies, inconsistencies, and wrong decisions. It also consumes valuable time in manual error correction. By preparing data correctly, you enhance record linkage, promote data integrity, and create a solid foundation for future analysis.\n\n\n---\n\n\n**2. What are Canonical Fields in Tresata, and why are they important in data preparation?**\n\n\n**A:** Canonical fields are standardized, universally recognized fields used to represent specific data attributes. They serve as fundamental elements for analyzing data, creating pipelines, and deriving insights. Canonical fields establish a common language and structure for data across different sources, ensuring consistency and uniformity.\n\n\n---\n\n\n**3. How are Canonical Fields created in Tresata, and what is their role in data preparation?**\n\n\n**A:** Canonical Fields are created by tagging fields that represent the same attribute across different sources. For example, if two sources have fields for customer names (\"Full_Name\" and \"Cust_Name\"), tagging them as \"name\" results in a single canonical field called \"name.\" During data preparation, canonical fields serve as the basis for cleaning and standardizing data.\n\n\n---\n\n\n**4. What are the steps to clean a field in Tresata's Prepare step, and why is field cleaning important?**\n\n\n**A:** Field cleaning is essential to standardize data. In Tresata, the steps to clean a field include:\n\n\n* Choose the field to clean.\n* Select an appropriate cleaner based on the field's nature.\n* Click \"Add Cleaner To Field\" to apply the cleaner.\n* Use \"Edit\" to modify or delete cleaners if needed.\n* Click \"Mark Complete\" to confirm cleaning. Cleaning ensures data consistency and removes outliers or inconsistencies, improving data quality.\n\n\n---\n\n\n**5. What is the significance of Regular Expressions (Regex) in data cleaning, and how can they be applied in Tresata's Prepare step?**\n\n\n**A:** Regular Expressions (Regex) are powerful for data cleaning as they filter and substitute data patterns. In Tresata's Prepare step, users can apply Regex filters to clean specific data patterns. For example, you can use Regex to accept numbers, exclude special characters, validate email IDs, phone numbers, and more. Regex ensures data conforms to desired formats, enhancing data quality.\n\n\n---\n\n\n**6. Can you provide examples of commonly used Regex filter expressions and their purposes in data cleaning?**\n\n\n**A:** Certainly, here are some examples:\n\n\nAccept Numbers: ^[0-9]+ - Accepts only numeric characters.\n\n\nExclude Special Characters: ^[A-Za-z0-9]+$ - Matches alphanumeric characters.\n\n\nAccept Valid Email ID: ^[a-zA-Z0-9._%±]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$ - Validates email addresses.\n\n\nPhone Number Validator: ^[+]{1}(?:[0-9\\-\\(\\)\\/\\.]\\s?){6,15}[0-9]{1}$ - Validates phone numbers.\n\n\nZIP Code Validator: ^\\d{5}[-\\s]?(?:\\d{4})?$ - Validates ZIP codes.\n\n\nSSN Number Validator: ^\\d{3}-\\d{2}-\\d{4}$ - Validates Social Security Numbers.\n\n\nAccept Valid Date (MM/DD/YYYY): ^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/(19|20)\\d{2}$ - Validates dates in MM/DD/YYYY format.\n\n\nAccept Valid Date (DD/MM/YYYY): ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/(19|20)\\d{2}$ - Validates dates in DD/MM/YYYY format.\n\n\nAccept Valid Date (YYYY/MM/DD): ^(19|20)\\d{2}/(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])$ - Validates dates in YYYY/MM/DD format.\n... \n\n\n---\n\n\n**7. How can users track the cleaners applied to fields during the data preparation process in Tresata?**\n\n\n**A:** Users can track applied cleaners by using the \"View Applied Cleaners\" button at the bottom of the screen. It provides a quick overview of all cleaners applied. Once satisfied with the cleaning, users can click \"Mark Complete\" to proceed with data preparation.\n\n\n#DataCleaning, #Regex, #CanonicalFields,  #DataPreparation #tutorial"
"**Understanding Tresata's PROFILE Feature**\n\n\n***Table of Contents:***\n\n\n**1. What is the PROFILE step & why is it important?**\n**2. How do I use Tresata's PROFILE to get the most out of it?**\n**3. What are Overview Statistics in Tresata's PROFILE?**\n**4. How can I investigate data at the field level in Tresata's PROFILE?**\n**5. How do I add tags to fields in Tresata's PROFILE?**\n**6. What is the PROFILE Heatmap in Tresata?**\n**7. How do I review and interact with the PROFILE Heatmap?**\n\n\n---\n\n\n**1. What is the PROFILE step & why is it important?**\n\n\n**Profiling in Tresata** means examining and understanding your data sources to extract valuable insights. It involves looking at the structure and content of your data, including the number of records and fields, field names, types, and actual values.\n\n\n**Why is it important?** Profiling is crucial because it helps you identify problems in your data, such as test values or duplicate entries, which can affect downstream operations like data cleaning and analysis.\n\n\n---\n\n\n**2. How do I use Tresata's PROFILE to get the most out of it?**\n\n\n**To maximize the benefits of Tresata's PROFILE feature:**\n\n\n* Ensure you profile all your data sources, using the overview numbers of records and fields across all sources.\n* Look for unusual patterns, placeholder values, or irregularities in formats within your fields.\n* Check the suggested tags for each field and either approve them or create your own.\n* Investigate and understand the PROFILE Heatmap view, which is critical for decision-making in subsequent steps.\n\n\n---\n\n\n**3. What are Overview Statistics in Tresata's PROFILE?**\n\n\n**Overview Statistics** provide high-level information about your processed data, including the number of sources, records, and fields. It helps you quickly identify if any records or fields have been skipped or missed.\n\n\n---\n\n\n**4. How can I investigate data at the field level in Tresata's PROFILE?**\n\n\n**To dive deeper into your data at the field level:**\n\n\n* Click on the expand button next to a data source's name in the left-side navigation panel.\n* Examine field-level details, including tags, data completeness (% populated), and the number of unique values.\n* Look for highly populated and unique fields, which are often good candidates for further analysis.\n\n\n---\n\n\n**5. How do I add tags to fields in Tresata's PROFILE?**\n\n\n**To organize and categorize fields, you can add tags like name, email, phone, or ID.**\n\n\n* Simply click the \"Add\" button at the bottom of the screen to assign tags to critical fields.\n* You can also edit or delete tags as needed.\n* Best practices include using descriptive names and assigning only one tag per field.\n\n\n---\n\n\n**6. What is the PROFILE Heatmap in Tresata?**\n\n\n**The Heatmap** is the final part of the Profiling stage, providing a visual representation of data patterns and relationships between fields across multiple datasets.\n\n\n* It's created based on the tags you've assigned to fields and is essential for getting a holistic view of your data.\n\n\n---\n\n\n**7. How do I review and interact with the PROFILE Heatmap?**\n\n\n**To access the Heatmap, ensure you've tagged at least one field.**\n\n\n* Click on the \"Review Canonical Heatmap\" button.\n* The Heatmap displays data patterns, fields, the number of unique values, and the percentage of data population.\n* You can sort values, hide/unhide sources and fields, and lock data sources.\n* Hover over fields and tags to see details like top values, patterns, and population percentages.\n* You can export the Heatmap to a CSV for further reference.\n\n\n#Navigation #Search #Schema #View #tutorial"
"Introduction: Here, you'll find answers to common questions related to navigating and working with sources in Tresata.\n\n***Table of Contents:***\n\nIntroduction: Here, you’ll find answers to common questions related to navigating and working with sources in Tresata.\n\n**Table of Contents:**\n\n**1. How can I quickly navigate through my file system in Tresata?**\n**2. How do I find a specific file or directory in Tresata if I don’t know its exact name?**\n**3. How can I quickly return to a specific directory in Tresata?**\n**4. How do I copy the file path in Tresata for reference or sharing?**\n**5. Can I search for sources in Tresata using a file path directly?**\n**6. How can I check the schema of a source in Tresata?**\n**7. How can I view the first 10 records of a source in Tresata?**\n\n**1. How can I quickly navigate through my file system in Tresata?** \nTo navigate your file system in Tresata, use the left-side panel. It mirrors the object structure on the backend, allowing you to see your folders and files. Click on directories to reveal nested folders and tables.\n\n**2. How do I find a specific file or directory in Tresata if I don’t know its exact name?** \nYou can use the search box to search for a specific file or directory in real-time. If you’re not sure of the exact name, you can continue navigating through the available objects until you find what you’re looking for.\n\n**3. How can I quickly return to a specific directory in Tresata?** \nTo return to a specific directory in Tresata, click “Back to data” to go back to the parent directory. This makes it easy to navigate and find your desired files.\n\n**4. How do I copy the file path in Tresata for reference or sharing?** \nYou can copy the file path by clicking the button next to the path. This allows you to quickly reference or share the path.\n\n**5. Can I search for sources in Tresata using a file path directly?** \nYes, Tresata has a universal search box with a “Search using file path” feature. You can paste a file path here to perform a direct search, and the results will be reflected in the left-side Objects panel.\n\n**6. How can I check the schema of a source in Tresata?** \nTo check the schema of a source in Tresata, navigate to the file you want to investigate using the left-side navigation panel. You can then choose the format of your file and view a list of available fields.\n\n**7. How can I view the first 10 records of a source in Tresata?** \nTo view the first 10 records of a source in Tresata, select the fields you want to explore (up to 4 at a time) using the search box. After applying the selection, you can see the values. You can also expand the right-side panel for more detailed views.\n\nUse the search bar at the top of the page and enter keywords related to your query.\n\n\n#Schema #Source #Records #Search #FilePath #copy #Navigation #FileSystem #directories"
"**Introduction:** Welcome to the community! This post contains frequently asked questions (FAQs) related to signing up for Tresata and creating products for administrators. Whether you're new here or looking for quick guidance, you'll find answers to common questions below.\n\n\n**Table of Contents:** For quick navigation to the specific question you need answers to. \n\n**1.How do I sign up for Tresata?**\nTo sign up for Tresata, go to the Tresata website and click on \"Get Started.\" If you're already registered, click on \"Login.\"\n\n**2. How do I choose my environment, AWS, or Azure during sign-up?**\nDuring sign-up, you'll need to select either AWS or Azure based on your environment requirements. The sign-up process varies slightly depending on your choice.\n\n**3. What information do I need to provide during sign-up?**\nYou'll need to provide your name, email, password, and agree to the terms and conditions.\n\n**4. What happens after I sign up?**\nAfter signing up, you'll receive a verification email with a code for two-factor authentication. Once verified, you can set up your \"namespace.\"\n\n**5. How do I create a new product on Tresata?**\nAdmin users can create a new product by clicking on \"Create Product\" on the dashboard. Provide a name and description for your product.\n\n**6. How can I set up a Namespace on AWS using Role ARN?**\n* To set up a Namespace on AWS using Role ARN, follow these steps:\n* Create an Identity Provider with \"OpenID Connect\" as the provider type.\n* Define the Provider URL, Audience, and other settings.\n* Create an IAM role with a custom trust policy that allows access based on your Namespace UUID.\n* Attach a policy to the role that grants necessary S3 permissions.\n* Copy the RoleArn of this role and paste it in the \"Create a Namespace\" page.\n\n**7. What do I need to substitute in the JSON for Role ARN setup?**\n* You need to substitute the following values appropriately in the JSON:\n* YOUR_ACCOUNT_ID: Your AWS subscriptions Account ID.\n* NAMESPACE_UUID: The value you noted down in the first step.\n* YOUR_S3_BUCKET_NAME: The S3 bucket name of the bucket you are giving access to.\n\n**8. How do I set up a Namespace on AWS using Access Key and Access Secret?**\nTo set up a Namespace on AWS using Access Key and Access Secret, create an IAM user, attach S3 Bucket access to that user, and generate Access Keys. Then, paste appropriate values for AccessKey and SecretKey in the Create Namespace page.\n\n**9. What is the prerequisite for setting up a Namespace on AWS using Access Key/Access Secret?**\nYou need to have administrator-level privileges in your AWS account and an S3 Bucket where the data is stored.\n\n**10. Can I use both Role ARN and Access Key/Access Secret methods for the same Namespace?**\nNo, you need to choose either the Role ARN method or the Access Key/Access Secret method for setting up a Namespace on AWS. You cannot use both methods for the same Namespace.\n\n\n**Conclusion:** We hope these FAQs help you navigate the Tresata platform effectively. If you have more questions or need further assistance, please don't hesitate to ask in this thread.\n\n**Search Instructions:** To quickly find the answer you need, use the search function by entering relevant keywords (e.g., \"sign-up,\" \"Role ARN\") in the search bar at the top right of this page\n\n#FAQs, #Sign-Up, #Product Creation, #AWS, #Azure, #Namespace, #Role ARN, #Access Key, #Access Secret, #Administrator"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Tresata Product Support category:\n\nThis is the where one can make posts about service, technical, product & consumption based queries & receive end to end solutions."
"## Azure Namespace Setup \n\nConnecting your data to our AKS cluster\n\n![Set_Up_Namespace_On_Azure|690x359](upload://kB0YXVl4JSXjaLZV0zaLEw4v08h.png)\n\nScreen: 6.0\n\n**PREREQUISITES:**\n\n1. You (ADMIN) have administrator-level privileges in your Azure account. \n2. You have a storage account with Hierarchical Namespace enabled with a container inside it holding all the data you want to process. \n\n**STEPS:**\n\n1. Note down the Namespace UUID from the namespace creation panel\n2. Make sure that the storage account has \"Hierarchical namespace Enabled\". \n3. In the \"Networking\" Tab of your storage account, under \"Firewall\", add the address range: 0.0.0.0/0 and hit save. \n4. Create Custom Role:\n    a. In the Azure Subscription, click on \"Access Control (IAM)\" and click on \"Add\" and then \"Custom Role\"\n    b. Directly go to the JSON Tab and Copy Paste the following code block. Click on Review + Create\n\n```\n{\n    \"id\": <AUTOGENERATED_CUSTOM_ROLE_RESOURCE_ID>,\n    \"properties\": {\n        \"roleName\": \"synapse-custom-role\",\n        \"assignableScopes\": [\n            \"/subscriptions/<YOUR_SUBSCRIPTION_ID>\"\n        ],\n        \"permissions\": [\n            {\n                \"actions\": [\n                    \"Microsoft.ManagedIdentity/userAssignedIdentities/read\",\n                    \"Microsoft.Resources/subscriptions/resourceGroups/read\",\n                    \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n                    \"Microsoft.Synapse/workspaces/read\",\n                    \"Microsoft.Synapse/workspaces/write\",\n                    \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\",\n                    \"Microsoft.Synapse/workspaces/operationStatuses/read\",\n                    \"Microsoft.Synapse/workspaces/replaceAllIpFirewallRules/action\",\n                    \"Microsoft.Synapse/workspaces/operationResults/read\"\n                ],\n                \"notActions\": [],\n                \"dataActions\": [],\n                \"notDataActions\": []\n            }\n        ]\n    }\n}\n      \n```\n\n4. Create Managed Identity:\n    a. Go to Managed Identity in the Azure portal and click on Create. \n    b. Give relevant values for the resource group, region, name, and tags and click on Create. \n5. Attach federated Credentials:\n    a. Go to the newly created Managed Identity and select federated credentials\n    b. Click on Add Credentials\n    c. Select Scenario as \"Kubernetes accessing Azure Resources\"\n    d. Cluster Issuer URL = \"https://eastus.oic.prod-aks.azure.com/5689d11b-14df-4535-9a56-0f243ac35eca/6efda958-5385-4c8f-bc41-36ff4cac9519/\"\n    e. Copy the Namespace UUID noted down from the first step - \n         Namespace = \"NAMESPACE_UUID\"\n    f. Service Account = \"default\"\n    g. Subject Identifier will be auto-filled as \"system:serviceaccount::<namespaceUUID>:default\n    h. Give relevant names to the credentials\n    i. Audience = \"api://AzureADTokenExchange\"\n    j. Click on Add\n6. Add Role Assignments:\n    a. In that same managed identity go to the \"Azure Role Assignment\" and select add role assignment\n    b. Scope = Storage, Resource = <STORAGE_ACCOUNT_NAME>, Role = \"Storage Blob Data Owner\" and hit \"Save\"\n    c. In that same managed identity, again go to the \"Azure Role Assignment\" and select Add Role Assignment\n    f. Scope = Subscription, Subscription = <YOUR_SUBSCRIPTION_NAME>, Role =\"synapse-custom-role\" (This is the custom role you \n       created above. \n7. Copy the ClientID and Resource ID of the managed identity, TenantID, Storage Account Name, and Container Name to the Namespace Creation Page.\n8. After creating the namespace, the corresponding Synapse workspace will also be created with same Name as the NamesapceUUID.\n9. To give the permissions to access Synapse:\n   a. Go to the Synapse Workspace Created and click on Networking. Here add your ClientIp to the firewall and click on save.\n   b. Click on the Web URL to open the Synapse workspace and click on Manage --> Access Control --> Add Role Assignment --> Here add \n       your user as the \"Synapse Administrator\". (This will give you the access to add/delete role assignments.\n   c. Once you have access and you can see the other role assignments, Delete the Role Assignment for the Managed Identity as Synapse \n       Administrator.\n d. Now Add the same Role Assignment again i.e Synapse Administrator for the Managed Identity."
"## AWS Namespace Setup - Using Access Keys and Secret Keys\n\n![Set_Up_Namespace_On_AWS_Using_Access_key/Access_Secret_Key|690x359](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)\n\nScreen: 5.0\n\nConnecting your data to our EKS cluster\n\n**PREREQUISITES:**\n\n1. You (ADMIN) have administrator-level privileges in your AWS account. \n2. You have an S3 Bucket where the data is stored\n\n**STEPS:**\n\n1. Create an IAM user and attach S3 Bucket access to that user\n    a. Go to the IAM service\n    b. Navigate to the \"Users\" and click on \"Create user\"\n    c. Give the appropriate User Name and click on Next\n    d. From the \"Permissions options\" select Attach Policies Directly \n    e. Click on \"Create Policy\"\n    f. Select JSON and paste the following in the \"Policy Editor\"\n```\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::<S3_BUCKET_NAME>\"\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectTagging\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:AbortMultipartUpload\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::<s3_BUCKET_NAME>/*\"\n        },\n        {\n            \"Action\": [\n                \"athena:StartQueryExecution\",\n                \"athena:GetQueryExecution\",\n                \"athena:GetQueryResults\",\n                \"athena:StopQueryExecution\",\n                \"athena:ListQueryExecutions\",\n                \"athena:CreateWorkGroup\",\n                \"athena:GetWorkGroup\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"glue:GetTables\",\n                \"glue:GetTable\",\n                \"glue:CreateTable\",\n                \"glue:UpdateTable\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/*\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"glue:CreateDatabase\",\n                \"glue:GetDatabases\",\n                \"glue:GetDatabase\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n```\n*\n    g. Review and Create the Policy and attach it to the user created in the above step\n    h. Navigate to the newly created user and click on the \"Security Credentials\" tab. \n    i. Click on \"Create Access key\" select \"Command Line Access\" and check on the confirmation button\n    j. Download and open these keys and paste appropriate values for AccessKey and SecretKey in the Create namespace page.\n\n---\n**NOTE**: Substitute these values appropriately in the JSON\n* S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to.\n\n* NAMESPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page.\n\n* YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.\n---"
"# AWS ROLE ACCESS\n\n![Set_Up_Namespace_On_AWS_Using_Role_ARN|690x359](upload://xH69vv1YISaUPy9LlUQjo0WaISF.png)\n\nScreen: 4.0\n\nConnecting your data to our EKS cluster (Recommended method)\n\n**PREREQUISITES:**\n1. You (ADMIN) have administrator-level privileges in your AWS account. \n2. You have an S3 Bucket where the data is stored\n\n**STEPS:**\n1. Note down the Namespace UUID from the namespace creation panel\n2. In your AWS Account, create an Identity Provider:\n    a. Go to the IAM service\n    b. Navigate to the \"Identity Providers\" and choose \"Create Provider\"\n    c. Select \"OpenID Connect\" as the provider type.\n    d. Provider URL = \"https://oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550\"\n    e. Click on Get thumbprint and AWS will generate this value. \n    f. Audience = \"sts.amazonaws.com\"\n    g. Review the information and then click on \"Add Provider\"\n3. Create an IAM role and Trust relationship so that only the Namespace created by you, will have access to data\n    a. Go to the IAM service\n    b. Navigate to the \"Roles\" and select \"Create Role\"\n    c. Select Custom Trust Policy and add this JSON\n\n```    \n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n      \"Federated\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:sub\": \"system:serviceaccount:<NAMESPACE_UUID>:default\",\n                    \"oidc.eks.us-east-1.amazonaws.com/id/E1B2EB40F8031126E7E6AA1D69937550:aud\": \"sts.amazonaws.com\"\n                }\n            }\n        }\n    ]\n}\n```\n   - d. To attach the policy to the role, select \"Create Policy\" and add this JSON:\n\n```\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::<S3_BUCKET_NAME>\"\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectTagging\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:AbortMultipartUpload\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::<s3_BUCKET_NAME>/*\"\n        },\n        {\n            \"Action\": [\n                \"athena:StartQueryExecution\",\n                \"athena:GetQueryExecution\",\n                \"athena:GetQueryResults\",\n                \"athena:StopQueryExecution\",\n                \"athena:ListQueryExecutions\",\n                \"athena:CreateWorkGroup\",\n                \"athena:GetWorkGroup\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:athena:us-east-1:<YOUR_ACCOUNT_ID>:workgroup/<NAMESPACE_UUID>\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"glue:GetTables\",\n                \"glue:GetTable\",\n                \"glue:CreateTable\",\n                \"glue:UpdateTable\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:table/<NAMESPACE_UUID>/*\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"glue:CreateDatabase\",\n                \"glue:GetDatabases\",\n                \"glue:GetDatabase\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:catalog\",\n                \"arn:aws:glue:us-east-1:<YOUR_ACCOUNT_ID>:database/<NAMESPACE_UUID>\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n```\n*\n    e. Give this policy an appropriate name and attach it to the role you are creating. \n    f. Give this role an appropriate name and click on \"Create Role\"\n    g. Copy the RoleArn of this role and paste it under the roleArn box in the \"Create a Namespace\" page.\n\n---\n**NOTE**: You need to substitute these values appropriately in the JSON\n\n* YOUR_ACCOUNT_ID - This is your AWS subscriptions Account ID. You will get this information if you click on your user in the upper right-hand corner of your AWS Console.\n\n* NAMESCPACE_UUID - This is the value you noted down in the very first step. Copy and paste the value from the Namespace Creation Page. \n* YOUR_S3_BUCKET_NAME - Paste the S3 bucket name of the bucket you are giving us access to. \n---"
"\nNow that we have successfully connected to our namespace, now its time to start the first product on TRESATA. In this post let's see how to create a new product from your dashboard, how to edit them if required and many more. So with no further ado, let's begin.\n\n**STEPS TO ADD NEW PRODUCT**\n\n![Dashboard_Create_New_Product|690x359](upload://1ZUIc2JBUcjqykts1QvWWTLFqkE.jpeg)\n*Screen:3.0*\n\n![image|690x347](upload://pbDMP5koh3fLMbfmKWQbYsU0PrY.png)\n*Screen 3.1*\n\n![Dashboard_Add_Name_Description|690x359](upload://6W3sc22HR6e09e4Zf8djFBlo1Kl.jpeg)\n*Screen: 3.2*\n\n* For admin users, they can easily get started off with creating their product simply just clicking on the *Create Product* button as shown in the screen 3.0\n* Next choose a category from the list above to select product type. Users can choose NOTA(None of the above) when they want to give custom category. These categories becomes prominent as TRESATA will suggest customised cleaners and resolution logic for your product.\n* After which a pop-up comes to allow users to provide a name for their product and a small description about it as shown in screen 3.1. After which it takes the user to Sourcing Step.\n* If the user is a non-admin, and just received link to sign up to the product, then they will have to wait for their admin to approve your namespace. Only approved users will be allowed to create their products. So be sure to ping your admin if not approved yet.\n\n---\n**EDIT PRODUCT NAME**\n\n![Dashboard_Edit_Product|690x359](upload://qTkS0gJzWVg5lRFxLohRk5Y7fKy.jpeg)\n*Screen: 3.2*\n\nIn the Screen 3.2, as you can see the list of products that has been created by the user. If they want to edit the name or the description of the product, user has to simply click on the *edit icon* as seen above.\n\n![Dashboard_Naming_Best_Practice|690x359](upload://1XSsuWCUOCCP54Tp0SBDFaeWrKL.jpeg)\n*Screen: 3.3*\n\nThe name of the product can not have any special symbols or product names can not be repeated in the given namespace.\n\n---\n**Go to Step in Product Listing**\n \n![Dashboard_Product_Stage_Listing|690x359](upload://4OIRI6jcrQSbFiRkHwtQVRqiW8v.jpeg)\n*Screen: 3.4.1*\n\n![Dashboard_Product_Stage_Listing_updated](upload://dfwyzahbmoOfPIcbpNvKKlUzKbN.jpeg)\n*Screen: 3.4.2*\n\nIf the product has been created, and some steps of TRESATA has been finished. In that case, users can directly access that step with the help of a drop-down to view or continue working from that step directly As seen above in Screen 3.4.1, choose any of the steps that has been listed, and clicking on the button *GO* will take us to corresponding home-screen immediately \n\n----\n\n**DELETE PRODUCT**\n\nProducts can be deleted easily from the Dashboard with the delete option that is present next to edit button. If the user is an Admin, then they will get couple of extra features to handle the deleted products. \n\nMore information on Admin rights of Deleted product was discussed in the previous post  [here](https://community.tresata.com/t/2-admin-panel-actions/566). \nFeel free to check them out. Let's begin our first step of your Data transformation which we call as Sourcing in the next post.. See you soon.!!"
"As we have started off with the first step to use product TRESATA by initial sign up process and login. It is important to note that there are few privileges that is availed only to an Admin user. This post is aimed to list down them together so that your product can be managed effectively.\n\n**MANAGE NAMESPACE**\n \nNamespace plays a very important role for our product, without which we wont be able to connect to our data. Hence handling it better becomes crucial.  When an admin user subscribes product TRESATA, soon after the payment completion they will be taken to the TRESATA Admin Panel. So the screen 2.0 shows how  *Admin Panel* looks like. \n\n**Namespaces:** \n\n![Admin_Panel_Screen|690x359](upload://xcCloyhnnEkdiKo5T2DiAA0SOu3.jpeg)\n*Screen: 2.0*\n\nThis tab shows the details of your namespace. Name of the namespace that was created during the sign up process will be shown here as seen in Screen 2.0\n\n----\n\n**Products:** \n\n![Admin Panel-Add_Product|690x359](upload://mzNDtWVUhiiRGZ4pIYvvrpJQsLP.jpeg)\n*Screen: 2.1.1*\n\n![Admin Panel_Add_Product|690x358](upload://kfVDTqTcYgUcGxmYNp3EgOovUXW.jpeg)\n*Screen: 2.1.2*\n\nOnly admin user can create products from the Admin Panel screen, and assign users to it. Admin will have to click on the Products Tab to add a new product as shown in Screen 2.1.1.  Admin has to enter Product Name, with a small Description and Users to that Product. As seen in the Screen 2.1.2, Admin can simply choose their users from the drop down list.\n\n**NOTE**: \nOnly one user can be assigned to every product. Admin can create multiple products but each product shall have just one user handling it. \n\n**Product Actions**\n* **Edit:**\n\n![Admin Panel_Edit_Product|690x359](upload://rsuG8LHePFFmbQfuRf8JB5VGhp7.jpeg)\n*Screen: 2.2*\n\nAdmin can change the Name and the Description of the product anytime according to their requirement. Once a User has been assigned to a product, that can not be changed under any circumstances. So it's important to the Admin to be cautious while assigning Users to their product. \n\nThere will be an upgrade very shortly allowing users to  collaborate with their co-data engineers, so stay tuned..!!!\n\n* **Delete:**\n\n![Admin Panel_Delete_Product|690x359](upload://qtIhNy62C5dwIr1ZF1f1csU9y4f.jpeg)\n*Screen: 2.3*\n\nAdmin can also delete their products easily by clicking on the delete button as shown in the Screen 2.3\n\n![Admin Panel_Select_Multiple_product|690x359](upload://osyveTf1jMPI16wASQ3hMSDFtR2.png)\n*Screen: 2.4.1*\n\n![Admin Panel_Delete _All|690x359](upload://baoqkaJfDxnFsBl5CTNWRGnlHWn.png)\n*Screen: 2.4.2*\n\nAdmin can also delete multiple products together by selecting them with the checkbox shown in the Screen 2.4.1 and then continuing to delete all as seen in 2.4.2\n\n----\n**Users:** \n\n![Admin Panel_Add_Users|690x359](upload://7WBpzAmQhxg84581LrV3dIGGWir.png)\n*Screen: 2.5*\n\nOnly Admin user will be able to add new users to the namespace that they just created. In order to do so, click on the tab of Users, hit on the button *Add Users* as seen in Screen 2.5\n\n![Admin Panel_Add_Recipients_email|690x359](upload://kACKMdJp88uhqxp6nulmThhPLcu.jpeg)\n*Screen:2.6*\n\nAdmin then has to add the email of the list of user they wish to add in the namespace\n\n![Admin Panel_Users_List|690x359](upload://byZ2E8lXs6ns2s03uzauXnLfzGs.jpeg)\n*Screen: 2.7.1*\n\n![Admin Panel_Invites_Sent|690x359](upload://q2EQrSYkfAQzMtOvBlnrUVi6dZt.png)\n*Screen: 2.7.2*\n\nAs shown in the Screen 2.7.1, even multiple users can be added by Admin to send invites to join the product. Soon after clicking on the button Send Invites, a email will be sent to every user to join TRESATA.\n\n**IMPORTANT NOTE**\n\n1. When user accepts the invite, the status of the User turns from red tag *Pending* to  green tag *Accepted*\n2. When users join through the invite sent by Admin, after user login, next Admin has to also approve them to join the Namespace too. If not, then users can not create any products on TRESATA\n\n----\n\n**CONSUMPTION**\n\n![image|690x349](upload://jo0QKpVYSD0WvsHP9XyWYm5zsBI.png)\n*Screen: 2.8*\n\nAdmins can keep track of the CPU consumption and  Billing cost for the selected period of time as seen in above. User can also view details of avg and max consumption of CPU units and Billing cost when they hover over the graph.\n\n----\n**DELETED PRODUCTS**\n\n![Admin Panel_Deleted_Products_list|690x359](upload://wESaTWb2yTFHKIXSvnmwqNTi5vB.jpeg)\n*Screen: 2.9*\n\nThis feature is available only to an Admin User. So under the Tab of Deleted Products, users can find a list of all products that was deleted by them and deleted by the users that they assigned.\n\n![Admin Panel_Restore_Option|690x359](upload://85JfHdtCNQkWQMt2jAKUkoAYgRM.jpeg)\n*Screen: 2.10.1*\n\n![Admin Panel_Delete_Forever_Option|690x359](upload://idZjx9E0QTbVYB2DC8dWcbejWz.png)\n*Screen: 2.10.2*\n\nThe users can either restore the deleted product, which will again reflect back in the deleted users environment, or to delete it permanently, click on the button called *Delete Forever* to get them out of your namespace completely.\n\n![Admin Panel_Mass_Action|690x359](upload://6iYKh27n1pTRf4jneEAX1Q8s0qN.png)\n*Screen: 2.11*\n\nThe above screen the representation of how we can restore or delete multiple deleted products.\n\nThis is the overall walk through on Admin Panel, more details to get started with your first product is coming soon in the next post. \n\nSo stay Tuned...!!"
"**Introduction**\n\nWelcome..!! Excited, as we take the first step towards an incredible journey of discovery, transformation and enrichment. This is the place for all users to unlock the power of data to enrich life.\n\nBut before we dive in, let's start with the first step which is signing up to our product TRESATA. This post is aimed to help user to get started with the seamless sign up process.\n\n**Steps To Sign Up:**\n\nAll users will be able to join TRESATA product from the Tresata website, with the three main options to get started \n**Try for Free**, **Upload** and **Access**\n\n![image|690x349](upload://uXD7N0toDUZdkBXdc8swg6NdSZ8.png)\n *Screen: 1.0*\n\nClick on Try For Free, if you want to playaround the product using pre uploaded tresata files. \nClick on Upload, if you want to upload your files in tresata buckets in order to create the data product.\nClick on Access, where users will be directed a new page to choose their S3 bucket.\n\n**Environment Selection:**\n\n![image|690x346](upload://1LFAQYo45TqcColku3E6j5ch7uY.jpeg)\n*Screen: 1.1*\n\nUsers are given with two options as you can see on the Screen 1.1. According to the requirement of the environment, users can choose either AWS or Azure. The sign up process vary a little based on the kind of choice is made\n\n**User Information:**\n\nThe next step is to complete sign up process by providing essential information:\n\n\n![Sign_Up|690x391](upload://r7vDq8kCO6kCeehRb4Cn67iEpFL.jpeg)\n*Screen: 1.2*\n\n* Name\n* Email\n* Password (with specific criteria)\n* Confirm Password\n* Checkbox to agree to our terms and conditions.\n* You can review our terms of service by clicking on the provided link.\n\n**Verification Email:**\n\n![Code_Verification |690x359](upload://kUMoUVF1Jk6IHos9P6anipgjpqs.jpeg)\n*Screen: 1.3*\n\nOnce you hit the sign-up button, user will receive an email with  verification code along with a login link. This is for two-factor authentication to confirm your account.\n\n**Setting Up Your Namespace:**\n\nAfter confirming your account, you'll need to set up a \"namespace.\" The process differs depending on your chosen environment:\n\n**AWS**: \n\n![AWS_Namespace|690x359](upload://aT29DpSvk5M9tehF0WThujPxgqZ.png)\n*Screen: 1.4*\n\nYou'll be prompted to provide:\n\n* Namespace alias name: Need to be unique and also an error message will guide you if it already exists\n\n* Role ARN: If you choose to set up your AWS account using a Role ARN (Amazon Resource Name), you need to provide the Amazon Resource Name of the IAM role that TRESATA will assume. This role should have sufficient permissions to access the necessary AWS resources.\n\n* Access Key and Secret Key: If you opt for the Access Key and Secret Key method, you'll need to provide:\n\n* Access Key: This is a unique identifier used to authenticate with AWS services.\n* Secret Key: This key is used as part of the authentication process along with the access key.\n\n* Configure AWS S3 Bucket: You need to specify the details of the AWS S3 bucket where your data sources are stored. This typically involves providing the:\n\n* Bucket Name: The name of the AWS S3 bucket.\n\n* For all the first time users, we have a detailed document on steps to create your namespace on AWS   [click here](https://docs.google.com/document/d/1zR67Hjo2jbH_kii2kZc-C5m8sq12T0ogn_5nt4P0HF4/edit)\n\n**Azure**: \n\n![Azure_Namespace_Setup|690x359](upload://fi1HNPBKI4LGoSKhUVDsbTMfWSd.png)\n*Screen: 1.5*\n\nYou'll be prompted to provide:\n\n* Namespace Alias Name: Provide a unique name that will serve as an alias for your namespace. This is what you'll use to reference your namespace within TRESATA. If the name is already in use, you'll receive an error message and will need to choose a different name.\n\n* Namespace Key: An auto-generated alphanumeric key will be provided to uniquely identify your namespace. This key is crucial for secure access to your namespace and TRESATA resources.\n\n* Azure Account Details: To connect TRESATA with your Azure account, you'll need the following information:\n\n* Client ID: This is the unique identifier for the application or service principal in Azure Active Directory that TRESATA will use to access Azure resources.\n\n* Tenant ID: This is the identifier for your Azure Active Directory tenant.\n\n* Storage Account Name: The name of the Azure Storage Account where your data sources are stored.\n\n* Container Name: The name of the container within the Storage Account where your data resides.\n\n*  For all the first time users, we have a detailed document on steps to create your namespace on Azure, [click here](https://docs.google.com/document/d/1ytCZ33m-jErwmrU5JL4sj2C997350pjfkrphkib-7d8/edit?usp=sharing)\n\n**Logging In:**\n\n![User_Log_in|690x391](upload://4JIZac164jREDp2hLR4f2HP3mkJ.jpeg)\n*Screen: 1.6*\n\nOnce user has  successfully signed up and confirmed your account, you'll land on the welcome login page. Here's what you need to know about the login process:\n\n* Provide users registered email and password to log in.\n* If you've forgotten your password, we offer a password recovery option.\n* New users who haven't signed up can use the provided link to join the TRESATA platform.\n* Once you log in, you can start exploring and utilising the TRESATA product.\n\n**Forgot Password**\n\n![Login_Forgot_Password](upload://uybDyuyLaFecitCb98SwdxPVohd.jpeg)\n*Screen: 1.7.1*\n\n![Login_Reset_Forgot_Password](upload://nWtttmT9SOiNQp2AWKajPCJJs69.jpeg)\n*Screen: 1.7.2*\n\nForgot your password ? Never-mind, we have got you covered. Just click on the forgot password option, and provide the username as shown in the Screen 1.7.1, and then users will be getting an automated reset password link on their mail list. Simply click on it and a Screen 1.7.2 will appear to reset and confirm your new password. And so, you're all set to access TRESATA..!!!"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"And just like that, we have made it to the grand climax of getting your data usable in Tresata. \nStarting from the raw data, then understanding them using different metric, continuing to clean messy data, moving on with entity resolutions to link records together, and in the last step we chose data with best attributes for our Golden Record. The journey went by in a flash. Now its time to view the most anticipated Output that is generated by Tresata\n\n\n**Getting Started with Output**\n\n![Enrich_Output_Home_Screen|690x359](upload://2tRSwTT7MfLw6b7M64EOeILmLRa.png)\n*Screen: 2.0*\n\nAs shown in Screen 2.0, from the list of Output sources, users are given with two choices to select their Output data\n\n1. **Universal (Per Data Source)**:\n\n![Enrich_Universal_Output|690x359](upload://7UXxIpD3l3SEMtl5afk2adYXm0k.png)\n*Screen: 2.1*\n\nThe Screen 2.1 shows the usage of Universal Output. In this method, users are given option to customise the output schema for every data source. \n\n* **Raw Data** is your initial sourced data\n* **Cleaned Data** is the data after we finished data cleaning in Prepare Step\n* **Enriched Data** is the data after applying optimal rules to get Golden Record\n\nWith the above 3 options, users can select the required schema from their type simply by clicking on the checkbox next to it as seen in Screen 2.1. These selected schema is source dependent and different schema can be selected for every data source\n\n2. **Enriched**:\n\n![Enrich_Enriched_Output|690x359](upload://d4EQS0XzASYtJUbpiMjGm1FTcym.png)\n*Screen: 2.2*\n\nAs seen earlier in the Universal Output step, the same process is followed for Enriched Output too. The main differentiator between the two is that, One Output Schema will be generated for all the Data Sources forming it a standardised output globally. And Raw Data fields can not be considered for Enriched Output. \n\nRest assured, users can select any schema that they wish to view output for and click on the button *Initiate Output* to get your Final Output."
"One of Tresata's core capabilities is Enrich, a core step allowing you to choose the best, most accurate values to create a *golden record* for each of your TresataIDs as well as tailor the output files to best feed downstream usage.\n\nThis post will provide high level information on why Enrich is important and it will briefly discuss its capabilities.\n\n**GOLDEN RECORD**\n\nHaving assigned TresataIds (TIDs) to records, you can now filter for a TID to view all the records describing one entity. However, what if you wanted to create one record, including the best, most accurate information for all of the available fields? That can be done using Enrich feature.\n\n**STEPS TO SELECT YOUR ENRICH FIELDS**\n\n![CLick_On_Initiate_Enrich|690x359](upload://pGK0Cwne78QDwgoNmGPi1k99Ui2.png)\n*Screen: 1.0*\n\nBefore we start to choose fields for our Golden Record, it's important to shine light on a few set of rules upon which the best fields can be selected by the user. In Enrich stage, there are two main categories of enrichment logic available (Table and Field Preferences) and can all be configured via drag and drop operations.\n\n\n1.**Field Preferences**:\n\n![Enrich_Select_Canonical_Field|690x359](upload://eT5VFmsUAOFRbze67Q0QWS5oy8s.png)\n*Screen: 1.1*\n\nThis category will allow you to select best value for a field to populate your Golden Record.  To use this rule, the users has to first select a Canonical Field, on the left side bar as seen in Screen 1.1. \n\n![Enrich_Rank_Records|690x359](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)\n*Screen: 1.2*\n\n* **Expression:** As seen in above in the Screen 1.2, by selecting this option, users can specify custom logic for how to identify the best information for this field. Expression, allows you to pick a value coming from a record that meets specific ranking criteria on a field of your interest. Notice the following example:\n\n*While gathering information for all the bookings by *\n\n\n For example, taking the example of *FunAirways* from before,  Prefer First name on the expression of Create Date. So the system will set Best First Names on the basis of the Create Date values\n\n![Enrich_Count_Preference|690x359](upload://8YEE2O4FHyZEvXJI0UBXOEbY3xR.png)\n*Screen: 1.3*\n\n* Count: This option allows user to select a particular field on the basis of most frequent value. A counting algorithm is running on the background on the selected canonical field values. Continuing from the example above, Prefer First Name on the count of Tickets booked. \n---\n2.**Table Preferences**: \n\n As the name suggests this category is used by users to choose a complete dataset as their best information for the Golden Record. We can further see 2 ways to choose best dataset.\n\n![Enrich_Prefer_Data_Set|690x359](upload://pvOxebvRcW7PVOCyODN19re1s4P.png)\n*Screen: 1.4*\n\n* **Prefer:** Selecting Prefer Rule is simply telling Tresata, that the selected data source is better than the rest that was participated in the resolution. Now our algorithm in the background  will assign high scores on the preferred sources and chooses its field values for all the canonicals from that preference.  \n\n![Enrich_Rank_Records|690x359](upload://8ZqZSNBsm7pYzc0WzJp3jsGBrIh.png)\n*Screen: 1.5*\n\n*  **Rank:** Just giving a preferred data source could be not just enough when users are dealing with complex data. So to facilitate that, there is another option called RANK. Here users can choose, Rank as a criteria to sort their best data. Assigning ranks to the canonical fields, so that those fields gets prioritised. In the Screen 1.5, as you can see users can select any canonical from the available list. Users can give their preferences by allocating a specific rank to it, it starts to execute from top ranked data source. \n\n---\n![Enriched_Profile_Heat_Map|690x359](upload://bBtvdWZ4WaClfitzHqnva5vOMXF.png)\n*Screen: 1.6*\n\nTo make sure you are applying right logic to select your Golden Record, users can also make use of the profiled Heat-map to understand the metrics of the data and get accurate canonicals for enrichment.\n\n![Enrich_Initiate|690x359](upload://l6T637D8IlyyiQBodf80uWebED0.png)\n*Screen: 1.7*\n\nWhen user finalises their enrichment logic, it is good to initiate the enrich step to apply all the selected rules. Users will be allowed to add any number of rules, that suites the best to their used case. Users can also edit and delete rules before clicking on the button *\"Initiate Enrich\"*\n\nThis will now create Optimal Output values that contains Golden Records with best attributes..!!!\nIn the next post, let's unfold the final transformed data... It's exciting, don't miss it..!!"
"**** \n\nIn the previous post, the importance of Validate as a step was discussed, and the main terminology and features explained. However, there is more to investigating Connect results and identifying areas for improvement: it takes investigation and evaluation of findings to pinpoint what is wrong when it comes to your Connect configurations.\n\nThat's why this post will walk you through how to evaluate and understand your Validate metrics and where to look to find some of the most common resolution gaps.\n\n**OVERALL STATISTICS**\n\nAs discussed on the earlier post, one of the main metrics provided throughout the Validate steps are the overall statistics on the top of the *Data Products Statistics* panel (*screen 1.0 below*).\n\n![Screen Shot 2023-08-20 at 3.34.18 PM|690x359](upload://6AHtYe11wsAZK5hDyMhCF74Y4Pf.png)\n*Screen 1.0*\n\nAs shown above, those statistics include:\n\n* \\# of Tresata IDs (TIDs)\n* \\# of Records\n* \\# of Data Sources\n* Avg. # of Sources per TID\n* Avg. # of Records per TID\n* Singletons\n* Spanning\n* Trapped\n\nAll the above are aggregated across all your sources, giving you a quick overview of everything within your data. But how should they be evaluated?\n\nTo begin with, the overall ***# of TresataIDs*** can show you how many entities have been identified within your data (i.e. how much your records have compressed). A very low number of TIDs compared a very high number of records can indicate over compressing (\"loose\" restrictions on what should bring records together). On the other hand, having too many TresataIDs can be a sign of undercompression (very restrictive connect logic) where records that should have connected don't. Of course, how much compression should be expected is relative to your use case. Let's look at the two following FunAirways use cases:\n\n* *As a data scientist at FunAirways, I am looking on all of the bookings with us for flights from the Charlotte airport by our loyalty clients (the ones signed up for our loyalty program, flying frequently with FunAirways). As I am trying to identify **how many clients** have booked with us, I am not interested on the number of bookings but rather, the unique number of clients that made them. Thus, as one client makes multiple bookings throughout a year, I should expect a low number of TIDs in relation to the number of records (number of bookings).*\n\n* *As a data scientist on FunAirways marketing team, I am trying to identify how many clients have signed up on your loyalty promotional program. For that, I am looking through the loyal customers dataset, which includes the names of the loyal customers and the unique key assigned to them by us upon sign up. On such a use case, compression is expected to be low as no customer is allowed to sign up for the promotional program twice and thus, duplication is unexpected. So, I would expect a high number of TIDs, relative close to the number of records...*\n\nSimilar way of thinking should be applied to statistics like the **Avg # of Records mer TIDS**. However, statistics like **Singletons, Trapped** and **Spanning** have some more interesting insights. As explained on the last post\n\n* **Singletons**: are tresataIds assigned to one record only. In other words, that record hasn’t connected with any other record.\n* **Trapped**: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source.\n* **Spanning**: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s).\n\nAs it can be derived, the closer to 1:1 the ratio is between records and TIDs, then higher the number of Singletons will be. Similarly, the more compression we are seeing, the highest the number of Trapped and Spanning will be. But, there is more to be derived from such statistics:\n\n* *As a data scientist in FunAirways, I am trying to identify how often customers book tickets but don't actually check in for their flights. For that use case, I'll use my *booking* and *ticketing* data sources. In theory, each record in booking should connect against a record in ticketing as usually, people that book a flight eventually check in for it too. In such a use case, I should expect a high number of Spanning TIDs (TIDs with records across different sources) as it would be an unusual case to have a customer who booked a ticket but didn't check in (and eventually missed the flight).* \n\n**DATA INTEGRITY CHECK**\n\nAll the above findings, while applicable on an overview level, they are also important when looking at each source separately as, with a more granular view, you are able to pinpoint the error more accurately.\n\nNow, rather than looking at those across sources, the user can identify if there is over compression or under compression within a source, which can help when investigating what part of your Connect logic has caused the incorrect resolution. Imagine a scenario where one source has unexpected high average records per TID. While that can be tricky to identify if you have many more sources on your Data Product, especially as they interact with each other, it becomes much clearer when separating those statistics on a source level.\n\n**CROSS-DATA SOURCE OVERLAPS**\n\nAnother interesting metric, besides how many records have connected, is how many of those have connected across what data sources. As showcased above, there is different expectation on how much records should connect within or across sources, based on the information they have. Knowing what's expected it (based on your business logic) can be very valuable when evaluating overlap statistics. \n\nSource overlaps are very useful to show how your records have connected, what sources have the highest overlaps and more. Notice on screen 1.1:\n\n![Screen Shot 2023-08-20 at 4.31.41 PM|690x359](upload://xztvJCYMHw8RjV93RhpDsmNCdV3.png)\n*Screen 1.1*\n\nOn the table s on the screen, you can find the exact resolutions metrics for each data source permutation set that records have connected. Based on the table, 56% of all the records belong to a TID that only spans two sources, ***finance*** and ***esg***. Based on your domain knowledge, those statistics can either validate what you initially expected or point you to the source of the problem."
"At this point, records from each siloed source are connected with each other, using a tresataId to identify entities rather than records. Having connected your data, the next step is to investigate the accuracy of your resolution and identify areas of improvement. This post will:\n\n* Explain what is the Validate step and why it is important for an accurate Data Product\n* List and briefly describe its most important features\n\nLet's dive right in...\n\n**WHAT IS VALIDATE**\n\nValidate is the next step right after configuring Connect. In terms of user actions, Validate is automatically kicked off after successful completion of the Connect job, and upon completion if provides you with important metrics to evaluate the performance of your Connect logic. All in all, it is a hub of statistics and metrics, available in tabular format as well as a *csv* export report. \n\nAll the above are interesting, but...\n\n**WHY IS VALIDATE IMPORTANT**\n\nAccurate resolution is an outcome of an iterative process, where you configure, validate the output and adjust your logic until you get optimal results. On that cycle, Validate holds a crucial role as it provides visibility on how good your Connect output is. Without the Validate statistics, identifying resolution gaps and tuning your logic would be impossible and thus, accurate Data Products would be much more difficult to achieve. \n\n **MOST IMPORTANT FEATURES**\n\nTresata's Validate statistics suite comprises of various level of metrics, from overview statistics all the way to source specific ones. However, before focusing on what those are, it's important to understand some key terminology that will be referenced in the Validate section:\n\n* **Overcompression**: is the concept of forcing too many records to link with each other, often causing incorrect connections, bringing together records that should be separate. It is often caused by \"loose\" Connect logic, where the user isn't strict enough on what should link two records together. \n* **Undercompression**: is the concept of missing out on connections between records. It leads to incomplete or fragmented data connections, leaving records separated when they should have connected. This can result in lost insights and a less comprehensive view of the data landscape and it can be caused by very restrictive Connect logic. \n* **TresataID**: a unique identifier used to identify entities across many records & sources. In other words, when records connect with each other during Connect, they will receive the same tresataId, which then can be used to identify all the records related to the same entity. \n* **Singletons**: are tresataIds assigned to one record only. In other words, that record hasn't connected with any other record. \n* **Trapped**: are tresataIds that connect records from one source only. In other words, the records with that tresataId have connected only with records within the same source. \n* **Spanning**: are tresataIds that connect records across many different sources. In other words, the records from that tresataId have connected with records from different source(s). \n\nHaving explained those important concepts, let now take a quick look on Validate's metrics that will enable you on your investigations:\n\n* **Overview stats**: will include statistics such as *# of tresataIds*, *# of Records*, *# of Data Sources*, *Avg. # of Sources per TID*, *Avg. # of Records per TID*, *Singletons*, *Spanning*, *Trapped*. \n* **Counts per Data Source**: will include most of the metrics above for each data source, to provide an extra layer of granualarity.\n* **Cross-Data Source Overlaps**: will include *Data Source Combinations*, *# of Records* and *% of Records Overlapped*. This metric will show how many records overlapped (connected) between sources and what is that percentage compared to all of the connections. \n\nThe above metrics are a starter pack on identifying areas of improvements on your connection logic. Checkout the next post for a deep dive on how to utilize them to improve the efficiency of your Connect step!"
"Hello members of Faramir Community!\n\nThis is your Destination for All Things Support 🌟, tailored to cater to your needs and enhance your experience with our solutions.\n\n**Support Structure:** \n\n🔹 **Swift Problem Solving**: Whether it's about services, technical aspects, product or usage queries, this is your direct line to assistance. Your posts, comments, and replies are the heart of our support structure, shaping the way we meet your requirements.\n\n🔹 **Clear and Connected Updates**: Forget scattered messages. Our support team will engage with you right within your post, keeping you in the loop at all times. Each response and update will be neatly organized in one place.\n\n🔹 **Specify Your Query Type**: When you create a new post, choose the relevant request category: Service, Technical, Product or Consumption. This ensures your query lands in the hands of the most fitting experts and will tackle your specific query within the same thread be it any updates, additional queries, or next steps. \n\n🔹 **Your Insights Matter**: Once a solution is in sight, share your feedback or tips. Your experience can light the path for others facing similar challenges.\n\nCreating Effective Support Interactions:\n\n🔸 **Detail Matters**: Share all the relevant info to help us understand your situation thoroughly.\n\n🔸 **Stay Engaged**: Respond promptly to follow-up questions for a smoother resolution journey.\n\n🔸 **Share the Wisdom**: If you uncover solutions, spread the knowledge! Your expertise can be the guiding light.\n\n🔸 **Support Team Role** : Handle different types of queries/challenges efficiently and responds according to the urgency and impact of each situation with timely responses and resolutions.\n\nTo begin, initiate a new post in the fitting category, and we'll be there to assist.\n\nHere's to your success and our joint progress!\n\nWarm regards,\nVarun\nYour Community Support Manager"
"**SUMMARY**\n\nRegular Expressions or RegEx, are a powerful feature when it comes to cleaning data, aiding in the filtration and substitution of different data patterns. In addition to all the cleaners that Tresata offers, this post will help users fully harness the capabilities of Regex-Filter functionality\n\n\n**LIST OF COMMONLY USED REGEX FILTER  EXPRESSIONS**\n|Name|Regex Expression|Description|Example|Output\n|---|---|---|--|--\n|Accept Numbers | ^[0-9\\]+|Accepts only characters from 0 to 9 and '+' sign indicates to accepts all digits entered|12345|12345\n||||ab123| None\nExclude Special Characters|^[A-Za-z0-9\\]+$|Matches with characters A to Z, a to z and 0-9|hello123| hello123\n||||hello@me| None\nAccept Valid Email ID|^[a-zA-Z0-9._%+-\\]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$| [A-Za-z0-9\\_] represents characters from a to z with 0 to 9 digits. '@' sign is a must to validate the regex followed by any characters from a to z in lowercase then comes '.' a period with a to z characters again|user@gmail.com | user@gmail.com\n||||user@gamil|None\nPhone Number Validator|^[+\\]{1}(?:[0-9\\\\-\\\\(\\\\)\\\\/\\\\.]\\\\s?){6,15}[0-9]{1}$|Accepts any country code with the condition of starting with a plus sign (+) and then checks for 10 digits in the phone number|+1 7878 123456 | +1 7878 123456\n||||+1 7875 1234 | None\nCredit Card Validation|^(\\\\d{13,16})$|Accepts 13-16 digits of numbers shows none, if this condition is not satisfied| 1234567890123345 | 1234567890123345\n||||12345  | None\nAlpha Numerals|*^[a-zA-Z0-9\\]+$|Accepts both alpha characters and numbers from 0-9|johnjacob123 | johnjacob123\n||||johnjacob*&1  | None\nAccept Alpha Numerals with Special Characters|^[a-zA-Z0-9_-*/%&\\]+$|Accepts both alpha characters and numbers from 0-9 including special characters|johnwick@123 | johnwick@123\n Scrub first 4 characters in a string|^[\\s\\S\\]{0,4}|Selects only  first 4 characters of any string| DUNE_world |DUNE\n||||123456 | 1234\nScrub Last 3 characters in a string|.{3}$|The dot operator represents characters from the end, so number 3 denotes to scrub last 3 characters| DUNE_Rocks | cks\n||||123456 | 456\nScrub only first name from Full Name|^[^,-. \\]*|This Regex returns the first name that is separated by hyphen(-) or comma(,) or space| Alex, Feliciano |Alex\nScrub only last name from Full Name|(?<=[,-. ]).*|This Regex will returns the last name that is separated by hyphen(-) or comma(,) or space|Alex- Feliciano| Feliciano\nZIP code Validator|^\\d{5}[-\\s]?(?:\\d{4})?$|Accepts 9 digit zip code and can be separated by a hyphen (-)|12345-6789 | 12345-6789\n||||1234| None\nSSN Number Validator|^\\d{3}-\\d{2}-\\d{4}$| Accepts 9 digits SSN code that is divided into first 3 digits next 2 and last 4 digits separated by hyphen.|12345-6789 | 12345-6789\n||||1234| None\nAccept Valid Date|MM/DD/YYYY format - ^(0?[1-9]\\|1[0-2])\\/(0?[1-9]\\|[12][0-9]\\|3[01])\\/(19\\|20)\\\\d{2}$| The pattern  (0[1-9]\\|1[0,2]) represents 1 to 12 digits which is applicable for month validation. '|22/03/2003| 22/03/2003\n||DD/MM/YYYY format - ^(0?[1-9]\\|[12][0-9]\\|3[01])\\/(0?[1-9]\\|1[0-2])\\/(19\\|20)\\\\d{2}$|( 0[1-9]\\|[12][0-9]\\|3[01]) - This Regex represents numbers from 0 to 31 for date validation.|32/07/2005 | None\n||YYYY/MM/DD format - ^(19\\|20)\\\\d{2}\\/(0?[1-9]\\|1[0-2])\\/(0?[1-9]\\|[12][0-9]\\|3[01])$|(19\\|20)\\d{2} - This Regex accepts dates from 1900's and 2000's, with bar separates '/|2022/12/22|2022/12/22\n\n---\n**CHEAT - SHEET**\n|REGEX|DESCRIPTION|\n|---|---|\n|  .| Matches Any Character\n|\\d|  Digit (0–9)\n|\\D| Not a digit (0–9)\n| \\w | Word Character (a-z, A-Z, 0–9, _)\n| \\W| Not a word character\n| \\s| White space (space, tab, newline)\n| \\S| Not white space (space, tab, newline)\n| \\b| Word Boundary\n| \\B| Not a word boundary\n| ^| Beginning of a string\n $| End of a String\n [  ]| matches characters or brackets\n [^ ]| matches characters Not in brackets 14. | = Either Or\n ( )|  Group\n *|  0 or more\n +| 1 or more\n ?|  Yes or No\n {x}|  Exact Number\n{x, y}| Range of Numbers (Maximum, Minimum)"
"**Efficient Resolution**: Make posts for addressing service, technical, product & consumption inquiries to receive comprehensive solutions."
"Hello there, Faramir Community!\n\nI hope you're all as excited as I am about this engaging journey we're on together. As we gear up towards working together in offering support, I wanted to take a moment to guide you through some quick links that will help you navigate our community seamlessly.\n\n1. **Code of Conduct:** Our community thrives on mutual respect and collaboration. Please take a moment to familiarize yourself with our guidelines to ensure we all have a positive experience. :arrow_heading_down:\n\nhttps://community.tresata.com/t/creating-a-harmonious-community-our-guidelines-code-of-conduct-for-an-incredible-journey/479\n\n2. **Clarity on username:** :arrow_heading_down:\n\nhttps://community.tresata.com/t/unleashing-the-power-of-your-unique-identity/478\n\n\n3. **Welcome to the Community:** :arrow_heading_down:\n\nhttps://community.tresata.com/t/welcome-to-tresata-community/210\n\n4. **Community Experience & Engagement:** :arrow_heading_down:\n\nhttps://community.tresata.com/t/welcome-to-tresatas-community-lets-make-the-most-of-our-community-experience/384\n\n5. **Community Engagement Tips:** :arrow_heading_down:\n\nhttps://community.tresata.com/t/12-tips-on-how-to-make-an-engaging-post/425\n\n\nRemember, this community is all about you, and your active participation is what will make it truly shine. Don't hesitate to dive into discussions, ask questions, and share your valuable insights. Together, we're creating a hub of knowledge and innovation..\n\nSee you around the community!\n\nCheers, \nVarun - Your Faramir Community Manager 🚀\n\n#faramir"
"****\n\nLet's have a quick recap of the journey we had in Connect step. We started by understanding the significance of Connect, moved on to learn the screen controls, and finally went ahead to add our resolution logic to link data together.\n\nIn this post, we shall look into the criteria on which the selected fields are coming along together.\n\n**Configure Connector Type**\n\n![Connect_Configure_Connector_Type|690x359](upload://zQ0b057YFZOiOvP3Gs6ifZtgMXh.png)\n*Connect: 4.0*\n\nNow that canonical fields are added, and even created internal linkages by adding parentheses in required places, but on what condition are these fields getting linked together? In addition to connect when the field values are same, there are a few other ways in which values in two fields match for two records, some specific to certain types of fields.\nYes, here is the answer to it. Tresata offers different type of Connectors that decides how records will be linked. Here is the brief insight on all connector types.\n\n1. Same: This connector type converts strings into lowercase and creates a match when there is exact comparison. This matcher type doesn't consider *null* values.\n\n2. Name: The name matcher can be applied to only Name field. The main functionality of this is that it parses raw name string into firstName, lastName, middleName, middleInitial, nameTitle, suffix and performs matching on the parsed values.\n\n3. Address: This Address matcher works similar to Name Matcher, and can be applied only on address fields. This address matcher will accept the raw address string and parse that into house number, road, level, unit, city, postcode, state and country. Then continues to match values based on the parsed fields.\n\n4. CompanyName: Company Name Matcher can be applied to only Company Name. As the above matchers, this will parse raw company name string into Company name, activity, region, structure and legal. After parsing, it performs matching operation on the basis of these parsed field values.\n\nUsers can choose one of the Matcher type from the above mentioned options to match their records.\n\n**Initiate Connect**\n\n![Initiate_Connect|690x359](upload://cva8UWrwOUFHy66jZnJGq4v3ZhB.png)\n*Connect: 4.1*\n\nWe have successfully reached the final part of CONNECT. After choosing the right canonical fields and its corresponding  connector types we are all good to execute the logic. As shown in screen 4.2 click on Initiate Connect button to start the execution. Users can go back to make any changes to their logic anytime before clicking on this button.\n\nThis concludes the Connect Step. Execution could take place anywhere between 30 to 1 hour for a small dataset, sit back and relax for a bit until Tresata completes doing this magic resolution. Feel free to post any queries you may have about this step on Community.\n\nCheers..!!!"
"Now that all the elements of the Connect step have been explained, it's time to have a walk through configuration in connect step and dive deep into creating connection logic to link records together.\n\n**CREATE CONNECT CONFIGURATION**\n\nThe first step to configuring connect is identifying what type of step or combination of steps best suits the user's case. Details about the available types of steps can be found in this post, but to summarize:\n* *Resolve*: Each record within the selected sources can connect with all the other records included in the sources according to the logic specified in the resolve step. Each cluster of records that connect together here get a unique id that differentiates one cluster from another which is known as a **Tresata Id**.\n* *Against*: Directional linkage, it connects one or more records from the selected sources on the \"From\" directory to at most one from the \"To\" directory according to the logic specified in the against step. It is mostly used to link records from transactional sources to those in reference sources. These transactional records are also given the same Tresata Ids as the record from the reference source to which they are connecting.\n\nThe choice of the step type, is heavily dependent on the sources, the user plans to connect with that logic. If the user wants to connect on only reference sources, they will use the *resolve* step for that but if the user then additionally want to connect the transactional sources on the resolved reference sources, they can go with the *against* step and specify *\"from\"* as the transactional sources and *\"to\"* as the reference sources.  The end result the connect step will be generation of Tresata Ids that will uniquely identify an entity and consists of multiple records from different sources (reference and transactional), everything about that entity under a single Id.\n\n**Source Selection Process**\n\n In *screen 3.0* the source selection process is shown for a *resolve* step. \n\n***PRO TIP***: *Remember, the **against** step allows connecting many records from transactional sources (transactions, purchases, bookings) to at most one record from reference data sources (clients, customers, members) since each transaction is unique to a specific type of entity.*\n\n![Connect_Add_Data|690x359](upload://hhpIAkbt9c8W74XXMlPxHAP5Vvp.png)\n*Connect: 3.0*\n\nAs shown in *screen 3.0*, the user can select the required dataset by clicking on the **(+)** in the *\"Add Data Sources\"* section. Once clicked, it will present the list of available sources and allow for real-time filtering while typing the name of the one to be added. Without choosing at least one source for each category (one category for *resolve*, two for *against* - *from* and *to*) the rest of the configuration panel is in a disabled state.\n\n-------------------------------\n**Creating Connect Logic**\n\nUpon selecting data sources, the canonicals included to **at least one** of the selected sources gets enabled on the *Canonicals* right panel. The panel includes:\n\n* ***Selected Sources (#)***: The canonicals included in **at least one** of the selected sources for this step.\n* ***Others (#)***: The canonicals not included in any of the selected sources for this step.\n* ***Color Coded Indication***: Based on the population of the canonicals. If the canonical is included in multiple sources, it is color coded based on the average across all of them. It allows user to plan whether to include a canonical in the logic. In *screen 3.0* the source selection process is shown for a *resolve* step.\n\nHovering over the canonicals, the user is able to see the exact percentage populated for each source, which enables more efficient connection logic.\n\n***PRO TIP***: *Choosing highly populated fields for Connect logic, ensures that most of the records will have values for those fields and thus, will be eligible to be checked for connections. If the least populated fields are present in the connection logic, least connections will be created, as empty values result in records being excluded from the linking process.*\n\n\n![Connect_Analyse_Canonicals|690x359](upload://83cgTO7lGRsgNRvpyzw9pqgGFlf.png)\n*Connect: 3.1*\n\n--------------------------------------------------------------------\n\nNow, After the user has decided the step type and has seen what canonicals are available in the selected sources to configure connection logic for that step, what is left is actually identifying what combination(s) of fields, if matching, define two records as describing the same entity and thus, form a connection. \n\nThe CONNECT strategy is heavily related to each use case. For example, different fields are used define a unique person than those used for a unique company. On a high level:\n\n* ***Connecting people***: name, phone, email, ssn (or id, passport etc), address, state, city, country, brithdate, gender and more.\n* ***Companies***: name, address, state, city, country, classification, date_of_incorporation, tax_code and more. \n\nSo for the first case, *name+phone* can identify a unique individual and so does *name+email* or just *passport id*. In the second case, *name+address* can identify an individual company and so does *name+date_of_incorporation* or just *tax_code*. So there can be multiple such combinations which identify a unique entity. This is where the *Add Logic* option is used to add as any such combinations as possible to identify more and more connections. \n\nOf course this is not an exhaustive list but rather, an indication of some of the fields could be interesting for each use case. Permutations of those plus other fields not included in the list above might be enough to define that two records describe the same entity. But how does a user actually configures that logic? \n\nAs shown in *screen 3.2*, the user can drag and drop fields from the Canonicals to the specified area in the Add Resolution Logic panel. Each white box represents a combination that, if fully satisfied, will create a link between records. \n\n\n* **Add Fields to Resolve:**\n\n![Connect_Add_Fields_for_Resolution|690x359](upload://reqUw36JRiyaS9E5WLePMJ8Rn9t.png)\n*Connect: 3.2*\n\nAs we have left with only required fields for resolution, let's start to drag and drop the fields from Canonical List to Resolution Tab.\n\n* **Parentheses Action:**\n\n![Connect_Parentheses_Action:|690x359](upload://9LUfxDTDjqQp7y44x431m6a7l16.png)\n*Connect: 3.3.1*\n\n![Connect_Parentheses_Action:|690x359](upload://tmGGIC2k3aKmjYht55y1v33FI5m.png)\n*Connect: 3.3.2*\n\nAdding a Parentheses to a canonical field, creates an internal resolution. Screen 3.3.2, denotes that, a record link is created  only when of both address and city meet the threshold value that is set. This helps users to create multiple complex resolutions in just one step. There could be individual fields can also be used with fields within parentheses that leads to both internal and external resolution.\n\nThe above described steps shows an example of using Resolve method of resolution. Let's look into adding another step in the resolution process with Against method.\n\n* **Adding another Step of Resolution:**\n\n![Connect_Add_Step_2|690x359](upload://n1NsMePoy6de9X7GugpzVCsvpsC.jpeg)\n*Connect: 3.4*\n\nClick on the *Plus button* to add a new step as shown in the screen 3.4\n\n![Connect_Against_Method|690x359](upload://5SFlKf6MEyTIyFhMIkWk30RRhG6.jpeg)\n*Connect: 3.5*\n\nLet's choose Against Method of Resolution this time as an example. The steps to add fields shall remain same as we performed for Step-1. Drag and drop the required fields and their Target fields.\n\n\n---------------------------------------------------------------------\n* **Using Shields**\nAs explained in [4.2 Getting Started With Connect](https://community.tresata.com/t/4-2-getting-started-with-connect/544), *Shields* are essential to maintain integrity of connections and avoid forming incorrect links. They ensure that, regardless of whether connection logic is satisfied, if specific integrity safeguards are broken then links are removed between records. More specifically:\n\nShields operate across all steps, regardless of sources picked or logic defined. They include canonicals, that if inconsistent across records, connection **should not** be formed (birthdate: one person can't have two different birthdates, ssn: one person can't have two Social Security Numbers and more). While Connection Logic emphasizes on what **should** match, Shields define what **should not** match.\n\nA user should include in Shields fields that should be consistent and always the same for connected records (birthdates, ssn and more). If **any** of those fields is inconsistent, then the connection will not be formed. \n\n***PRO TIP***: *Remember, Shields operate on a higher level, across sources and steps. Thus, when defining them the user might be looking for a canonical that is not included in the sources picked for the current step. By clicking on *Others(#)* the user can find the rest of the canonicals and still use them as Shields in their configuration.*\n\n![Connect_Add_Shield_Fields|690x359](upload://1gPGxx3163fhX63ax3U1hiyKSKS.png)\n*Connect: 3.6*\n\nIf the user attempts to proceed to *Build Pipeline* without configuring any shields, a reminder is shown to highlight the importance of Shields to accurate connections as shown below.\n\n![Connect_Shield_Pop_Up|690x359](upload://vR64su0Nh4peUBSSkMnGlursxWS.jpeg)\n*Connect: 3.7*"
"****\n\nIn the previous post, the importance of CONNECT for the process of gaining meaningful insights from user data is highlighted. But what are the main CONNECT configurations and how do they differ from each other? Answering the above is the main purpose of this post!\n\n**CONNECT**\n\nStarting from the top, first one notices the HeatMap, that can give meaningful insights as to what fields are populated & unique enough to contribute to accurate linkage.\n![Connect_HeatMap_Button|690x359](upload://dSmwoNDxURJhXdf4VB5tZtfDZCe.png)\n*Connect: 2.0*\n\n![Connect_HeatMap_View|690x359](upload://uVVBRPbd4aZQY37MX1eKxRFxt7d.png)\n*Connect: 2.1*\n\nAs shown in screen 2.0, click on the \"View HeatMap\" button to reach screen 2.1, including the HeatMap as generated in PROFILE. This way, the user can make informed decisions as to what fields will contribute the most to effective record connections.\n\n![Connect_Screen_Highlights|690x359](upload://raQZrBeLj5eh0FXUfpQ829kVIzd.jpeg)\n*Connect: 2.2*\n\nAs seen in screen 2.2, there are two standard methods in which users can create their logic to link the records together. They are:\n\n1.  **Resolve:**  This resolution type allows for any records within the selected sources to connect against any other records from the same and the rest of the selected sources according to the logic specified on the resolve step.\n\n* *For example, as seen in a previous post on [significance of connect](https://community.tresata.com/t/4-1-connect-its-significance/542), if user wants to know if Helen is the returning customer, they need to connect records' PII (name, address, birthdate and more) with past ticketing and booking purchases to identify whether she has purchased tickets in the past. Using  \"**Resolve**\" logic, they can match any record from the Ticketing source, to any other record from either Ticketing or Booking. If this matching process brings many records together, they can identify Helen as a returning customer.*\n\n2. **Against:** This resolution type, is a directional matching process. In detail, it attempts to connect records from a pool of source(s) (mentioned as \"***from***\") to at most on record of another pool of source(s) (mentioned as \"***against***\"). This step is particularly useful when connecting transactional data sources to reference ones (purchases to customers, transactions to clients and more). This is a many to one step, which means that since there can be multiple transactions corresponding to an entity, all these transactions can be attached to that same entity.\n\n* *After resolving all the booking and ticketing information together, there exists a common identifier for all the purchases Helen (or any customer for that matter) has with FunAirways. However, how to know whether those purchases are done by a member of their loyalty clients? They can attempt to connect all those records with one record on our Loyalty source (each record on the Loyalty source is unique, since each customer has a unique Customer_Loyalty_Number). Matching multiple transactions (bookings and ticketings) against client records is a classic example of an ***against*** resolution step.*\n\n3. **Steps:** CONNECT comprises of many steps, each one defining different ways to connect records for the selected sources within that step. Not all sources should connect with each other based on the same logic, look at the example above: Ticketing and Booking connected with each other with a resolve on PII information (canonicals like name, email, address and more). However, for Ticketing and Booking to connect to Loyalty (FunAirways loyal customers data source) the unique Customer_Loyalty_Number canonical is enough to bring them together with an against step. That flexibility is introduced to the user via steps.\n\n4. **Add Data Sources:** Prompts the user to add data sources to the selected step. Only the records from the selected data sources will be the only ones considered for matching with the configured logic within the step. \n\n5. **Canonicals:** Canonicals Tab is automatically populated immediately after the Data Source(s) is added. Those are the fields that can be used to build the logic for the step and define how should records for the selected sources connect. To further assist the process, upon hovering over the canonicals, the user can see how populated they are, which is an important indicator to how much each field can contribute to the connection process. \n\n6. **Add Resolution Logic:** That is the place where the connect logic is specified. As should in *screen 2.2*, there is a specified gray box where the user can drag and drop fields accordingly. Each box defines a combination of canonicals that, if matching, create a connection between two or more records. \n\n7. **Add Link SHIELDS:** In order to avoid incorrect connections, Tresata allows the user to define \"shields\", i.e. fields that if they have different values, the connection is incorrect and should be broken. \n\n* *Imagine a scenario where FunAirways reolves the list of passengers for one of their international flights with a list of people forbidden from entering that country. Now assume that for that logic, they use **name, email, phone, and citizenship**. According to that logic, one passenger, John Doe, seems to have connected with the list of **flagged** people and thus, should be denied during the boarding process. However, the manager of FunAirways, notices that the two records, while having similar PII (name, email, phone and citizenship) actually have different birthdate, idnicating that they are different people. Adding birthdate as a **SHIELD** would prevent that connection and improve the accuracy of the CONNECT step.*\n\n8. **Build Pipeline:** To proceed in CONNECT, the user has to first click on the \"**Build Pipeline**\"  button to enable Tresata's engine to calculate the pipeline for connections as defined by the user provided logic.\n\n\nWith this we have made ourselves comfortable with all the features Tresata provides for CONNECT phase. In our next post, lets see how to create our first resolution logic efficiently. Stay Tuned...!!!"
"**SUMMARY**\n\nAfter the data has been cleaned and prepared, the next step is to identify and connect records across various data sources and soiled systems, so that the user has a more holistic view of a unique entity. \n\n*Seems simple, right? Not quite...*\n\nWhen dealing with multiple, diverse datasets, records may contain different attributes or have variations in how they represent the same entity. CONNECT allows the user to integrate these disparate pieces of information for each record into a more complete and accurate representation of the entire entity. \n\nBut why is this important and how does this help businesses grow? This post will aim to answer those two questions!\n\n**WHAT IS CONNECT**\n\nTo better understand the power of CONNECT, it may be easier to describe the problem using a real world example:\n\nHelen is a travel enthusiast always flying with the same airline company, FunAirways. Now, let's imagine each interaction Helen has with FunAirways as she books her upcoming flight to Greece. First, she books her round-trip ticket via the FunAirways website. Then, on the day of her flight, she checks in to her flight at the airport and purchases two checked bags at the counter. Next, she settles in to her flight and orders a smoothie from the flight attendant. Lastly, she arrives in Greece and heads to the rent-a-car counter, where she plans to pick up the car she reserved through FunAirways' website (using their trusted partner, FunCars, of course!). \n\nThat amounts to four separate interactions with FunAirways during her travel journey! What makes things difficult is that the records for each of those interactions are located in separate systems (i.e. the booking system is completely different than the in-flight services system). Thus, if FunAirways wanted to gain a comprehensive understanding of Helen, or any of its other customers, they would have a difficult time doing so. \n\nNow, let's make this problem even *more* difficult. Let's imagine that Helen used her home phone number and home address to book her ticket. However, the \n\nHow can FunAirways:\n\n* Identify that Helen is a returning customer, even though not a loyalty member, that constantly chooses them to fly with?\n* Identify patterns in Helen's behaviour to personalise their services (perhaps offering her a free smoothie during her next flight or even points for her next car rental)?\n\nCONNECT is Tresata's step for solving this problem. All different source come together, utilising a powerful entity resolution engine to identify unique entities within all the records, based on ML algorithms and user-defined logic.\n\n**WHY IS CONNECT IMPORTANT**\n\nBy creating a comprehensive, 360-degree view of each unique entity, CONNECT enables enterprises to understand an entity's characteristics, behaviors, and interactions across various contexts. This understanding allows businesses to enhance customer loyalty, operational efficiency, and make smarter decisions, achieving these goals faster than their competitors. This business model is employed by companies like Amazon, Google, and other FAANG members. These companies optimize every process and interaction by gaining deep, dynamic insights into individual customer behaviors and preferences. This approach facilitates the hyper-personalization of each customer relationship, which we, as consumers, have come to expect, thereby fostering unparalleled loyalty. By demonstrating to customers that they know, understand, and appreciate their unique preferences and behaviors, these companies ensure customers keep coming back."
"[FAQs.pdf|attachment](upload://cATqZRqXH8xBLDiHfYuwCa4ava9.pdf) (31.5 KB)"
"**SUMMARY**\n\nAs we have established a functional workflow, the crucial next step is to monitor its progress and address any errors that may arise. This post outlines various methods to effectively check on the workflow.\n\n**USER ACTIONS**\n\n![Orchestrate_Progress Bar|690x359](upload://2Psgivq7Mtm4UANOvRZENnds1EB.png)\n*Orchestrate: 2.0*\n\n* *Progress Bar:*\n Users can choose just one stage of the workflow to all stages of the workflow. Based on the selected stages, user can view the progress of their running workflow as shown in the screen 2.0\n\n![Orchestrate_Restart_Workflow|690x359](upload://xyPwqtZ3wLI8gJexxTcdNI5FTy6.png)\n*Orchestrate: 2.1*\n\n![Orchestrate_Restart_Workflow_Approve|690x359](upload://decmLbKTaqzCNLdiDKUCSnxQJYw.png)\n*Orchestrate: 2.2*\n\n* *Restart Workflow:*\nWhen there is a change in workflow or rerun the workflow, users can easily do so by using the restart option as shown in the screen 2.1. After which there is a pop up on screen to confirm the restart option as seen in screen 2.2\n \n\n![Orchestrate_Cancel_Workflow|690x359](upload://rvaZ7ggh1kvt4F1Zh8mbUDYMp9r.png)\n*Orchestrate: 2.3*\n\n* *Cancel Workflow:*\n Users can just cancel the running workflow, if there is a situation that demands a change in the scheduled run.\n\n* *Start, Edit and Delete Methods*\n\n![Orchestrate_Start_Workflow690x359](upload://sdqEgZqYqGPutVqS3HyjtpO5XMb.png)\n*Orchestrate: 2.4*\n\n1. Start: Users can start the run whenever they prefer to irrespective of the scheduled time as shown in screen 2.4\n\n![Orchestrate_Edit_Workflow|690x359](upload://2g4cs8EM7ymGAqINH209cAyMJSX.png)\n*Orchestrate: 2.5*\n\n2. Edit: Users can edit the timelines by clicking on the “pencil button” to change duration, time etc to anything that is according to user requirements as seen in screen 2.5\n\n![Orchestrate_Delete_Workflow|690x359](upload://zpmENySIOJvrNrhvQ5xOwYKCOfQ.png)\n*Orchestrate: 2.6*\n\n3. Delete: Looks like user made a mistake while scheduling or chose a wrong workflow? Or user no longer want the schedule to happen ? Never mind, user can choose the workflow they just scheduled and click on delete option to remove them as shown in screen 2.6.\n\n**THINGS TO REMEMBER IN ORCHESTRATION**\n\nThere are few important points to remember while setting up the workflow to orchestrate\n\n* The name of the workflow can not be changed. Once chosen in the beginning shall be the same name to all of the scheduled runs too.\n*  The name of the workflow can not have any special characters or can not have duplicated names.\n* Scheduled workflow can be changed and deleted at any point of time.\n\nFinally this completes all stages in the workflow. We hope this journey was comfortable and smooth. Please feel free to post any queries that you could have in community and we will help you sort the issue.."
"Managing time efficiently is the key for the fast paced world. If we could automate the workflow created once, that shall run automatically at selected time. Yes. Tresata offers users to run their workflow from profiling their data to having an enriched data in just one click. The steps to be followed to orchestrate workflow is  discussed in this post\n\n**ORCHESTRATE THE WORKFLOW**\n\nAfter creating a complete workflow end to end, users can now automate them to run by itself in the internal that is set.  First click on *Automate Workflow* button to get started with Orchestration\n\n![Orchestrate_Automate_Workflow|690x359](upload://84NhJJmQXya7AI0AxIj25DvBicC.png)\n*Orchestrate: 1.0*\n\nThere are two types of Orchestration Techniques, that is provided in product. One being Manual and the other being Automatic Orchestration. Let's look into them individually to know how to use it.\n\n* MANUAL ORCHESTRATION\n\n![Orchestrate_Manual_Method|690x359](upload://mVRX3zZdNTvEUZtiyoxNIle3Vu7.png)\n*Orchestrate: 1.1*\n\nThe screen above shows how Manual orchestration method looks like. Whenever user has a workflow ready to be scheduled, they can select the workflow required to run in using the search bar. The workflow can just run one step like profiling or two steps like Profile and Prepare or user can even have an end to end workflow inclusive of all steps from from profile to enrich.\n\nNow select the schedule type to be *Manual* and then clicking on *Confirm & Orchestrate* button will trigger the workflow run. Since it is Manual method, workflow is set to run only when user desires.\n\n* AUTOMATIC ORCHESTRATION\n\nUnlike manual method, automatic method of orchestration as the name suggests, it runs automatically at the selected time interval.\n\n![Orchestrate_Automatic_Method|690x359](upload://c6tFl0nUgjiWj7I70PtIBi7jBhH.png)\n*Orchestrate:1.2*\n\nSelecting Automatic method, will open another window as seen in the screen 1.2, that allows the user to choose interval with the simple drop down button. \n\n1. Duration - Here users can select the duration, it could be day, month or even year. The workflow kicks off automatically when that time is met\n2. Time - Users can set hours and minutes at which they want to execute the workflow.\n3. If users select daily at 1 pm est, the workflow is run without any human intervention everyday automatically\n4. User has to be sure to know the steps that they want to run automatically.\n\nJust like we confirmed in Manual method, go ahead and click on  *Confirm & Orchestrate* to trigger the workflow run. From this point, the workflow gets kicked off automatically when the set time is achieved.\n\nThe more information about monitoring the scheduled runs will discussed in the next posts...!! \nStay Tuned..!!"
"[JULY 2023 - FARAMIR UAT_PRODUCTION PLAN.pdf|attachment](upload://qMuDKaP0sZpQD0HHSngnlK6ZMxB.pdf) (654.4 KB)"
"![Customer System Flow|690x427](upload://l2LpOT7vqMjDf3zOm8pvFLORX5N.png)"
"[2023 FEBRUARY - FARAMIR IMPLEMENTATION ARCHITECTURE.pdf|attachment](upload://iTYbG0V78tvXx3ZzHUbjIKNlSct.pdf) (2.2 MB)"
"[Faramir Tides User Guide.pdf|attachment](upload://5r4HKxuSuhtVPGcixkpa6yLRX8T.pdf) (5.4 MB)"
"[2023 MAY - SONOCO TECHNICAL DOCUMENTATION.pdf|attachment](upload://xl4ruckShyHxXzskrsGG5SKClqG.pdf) (9.8 MB)"
"**SUMMARY**\n\nOnce all raw fields have been tagged and canonical fields have been created, the user must start to think about how each field should be standardized. Given that each data source is different, it is likely that you will have various different canonical fields. Thus, your cleaning methods of these fields will inevitably vary (you would not want to clean the alphanumeric field, email, in the same way as a numeric field like a phone number). \n\nNow, you might be wondering...how will I know what cleaning should be applied to each field? Tresata to the rescue! Tresata offers users a wide range of cleaners that can easily be applied to remove outliers and inconsistent values. See below for a step by step walk through.\n\n**TYPES OF CLEANERS**\n\nThere are two standard types of cleaner that TRESATA offers as listen below.\n\n* **GLOBAL CLEANER**:\n\n![Prepare_Global_Cleaner|690x359](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)\n*Prepare: 2.0*\n\n As the name suggests Global Cleaners are those that are applied to all the canonicals of all the data sources automatically in one click. SAM chat-bot suggests the global cleaners like Null cleaner and Trim_Lower in order to standardise data across all canonical fields.\n\n* **FILED SPECIFIC CLEANER**\n\n![Prepare_Individual_Cleaners |690x359](upload://AeZofRunEkAlQ1gxMZA75UkPox1.png)\n *Prepare: 2.1*\n\nTRESATA offers a wide variety of pre-built cleaners that can be applied to your canonical fields. To make the process even easier. our SAM suggests you few standard cleaners based on the kind of data that you're working with. These cleaners help to clean the raw data making sure that data with correct format moves to the next stages of TRESATA. In order to use SAM suggestions you can simply hit on \"APPLY\" button so that the selected cleaners will be applied automatically for the chosen canonical fields. \n\n**CUSTOM CLEAN YOUR FIELDS**\n\nFirst, choose the field that should be cleaned from the list of canonical fields on the left. \n\n![Prepare_Select_Canonical_Field|690x359](upload://b5NaZM0LYdi5YsoQw7DJdep0UlN.png)\n*Prepare: 2.2*\n\nFrom the list of available cleaners, select the most appropriate based on the nature of your canonical. \n\n**TIP:** Click on each cleaner for a brief description and example so that you have a better idea of how your field will be cleaned. \n\nOnce the user decides on a cleaner, just click on, \"Add Cleaner To Field\" to apply it to the canonical.\n\n![Prepare_Add_Cleaner|690x359](upload://8WdkGOYhaD50zTb0pV35RQ0xMRu.jpeg)\n*Prepare: 2.3*\n\nIf a user wishes to edit the cleaner or add additional cleaners to the same field, simply use the \"Edit\" option as shown below. Additionally, if a cleaner is no longer required, users will be able to this same option to delete the unwanted cleaner. User can also view all the cleaners that were applied by clicking on the radio button \"View Applied Cleaners\" and check how the cleaners work together by providing a sample test string.\n\nEach time a cleaner is applied successfully, a check mark will be shown next to the Canonical, to represent that its clean. Once user feels happy about all the cleaned fields, they can click on \"*Mark Complete*\" so that users can move to next data source for cleaning messy data.\n\n![Prepare_Edit_Cleaner|690x359](upload://6gN7qiS7DWDhZRe0LkamaaYelMD.jpeg)\n*Prepare: 2.4*\n\nAs you are going through this process, it is easy to lose track of the cleaners that have already been applied. Use the \"View Applied Cleaners\" button option present at the bottom of the screen for a quick glance of all cleaners that have been applied. When user is happy about the cleaners used, click on *Mark Complete* to make sure the cleaning has been completed for the selected Data source. \n\n![Prepare_View_All_Cleaners|690x359](upload://ot6FwWr1KpFV29qOFShYJnbLF7a.png)\n*Prepare: 2.5*\n\nCheck out the next post for an in-depth look at some of the more advanced cleaners Tresata has to offer!\n\nThe final step of Prepare is to click on *Initiate Data Preparation* to apply all the cleaners that you just selected"
"**SUMMARY**\n\nAfter using TRESATA's profile engine to gain valuable insights (# of records, % populated, top values, and more), the next step is to leverage this intelligence to clean the data accordingly. Getting this step right is critical because uncleaned data can have serious repercussions on the accuracy and reliability of a data product - inaccurate values and inconsistencies can skew results, potentially leading to incorrect decisions. Additionally, valuable time and resources are spent manually correcting data quality issues and managing the data, rather than deriving valuable insights from it. \n\nBy cleaning and standardising the data, users enhance the accuracy of the record linkage process, promote data integrity, and create a solid foundation for future analyses and data products. This post is dedicated to learning more about the first step in cleaning up messy data: canonical fields.\n\n**CANONICAL FIELDS**\n\nCanonical fields refer to a set of standardised, commonly recognised fields used to represent specific attributes of the data. These canonical fields serve as fundamental elements to analyse data, create pipelines and derive valuable insights. They form a consistent and uniform representation of data across different sources, establishing a common language and structure for the profiled data (a common schema across all sources).\n\nFor example, let's say you have two sources (Table-1 and Table-2). \n\n* Table-1 contains the raw field name \"*Full_Name*\", representing customer names \n* Table-2 contains the raw field name \"*Cust_Name*\",  representing customer names\n\nSince both fields represent the same attribute, the user will tag each field as ***name***, resulting in the creation of a single, universal canonical field called \"name\" (refer to the Profile step for further information on tagging). \n\n***Note:*** The tags that were assigned by a user in the Profile step are **automatically populated as canonical fields** during the Prepare step. \n\n---\n**GLOBAL CLEANERS**\n![Prepare_Global_Cleaner_By_SAM|690x359](upload://957p6RtKoFz3x9BeKlV6nnKhcyI.png)\n*Prepare: 1.0*\n\nIn Screen 1.0, you can see an example of the canonical list for the data source **boromir-gb-1**. The user has tagged over 10 fields that will be used in the following steps. Before cleaning the data, the user can validate that these canonical fields are accurately tagged, and can add or delete new ones based on the specific use case. Once the user has confirmed the canonical list is complete, they will dive deeper into field-level cleaning.\n\nThe next post is dedicated to explaining different data cleaning methods in TRESATA. Check it out for more detailed insights on how to standardise your data!"
"**SUMMARY**\n\nHeatmap is the last part of Profiling stage. They are the visual representation of data patterns and relationship between different fields across multiple data sets that was profiled. HeatMap is created using different tags that were assigned by the user making it a one place destination to have complete view of your data\n\n**REVIEW HEATMAP**\n\n![Profile_Review_Heatmap|690x359](upload://AgT9S5a7KMK3qkrLzpGv16DHxAg.png)\n*Profile: 5.0*\n\nWhen the user has completed tagging their critical fields, users will get access to Heatmap.\nAs Heatmap is created based on the tags assigned, it is crucial to have at least one tagged filed before proceeding to Heatmap. \n\nAs shown in screen 5.0, clicking on Review Canonical Heatmap button will display the final heatmap\n\n![Profile_Heatmap_view|690x359](upload://r7lBAyIsfGdyeONwK9bxI1lurmw.png)\n*Profile: 5.1*\n\nScreen 5.1 shows the resultant Heatmap that is created for the profiled data sets.\n* **List of Tags:** List of Tags tab shows all the tags that was assigned by the user. The tags act as different categories and shows all the fields that was assigned the same tag. These tags are then considered as canonical filed names, which will be used for cleaning the field, creating pipelines etc. \n* **Sources:** In the screen 5.1 we see boromir-gb-1, ticket and sales being 3 different data sets. Each source is again broken down into 3 sub divisions like filed names, # of Uniques and % of populated\n* **Field Names:** This is the raw names of the fields that was profiled\n* **# of Uniques:** Shows the total count of unique values in the field. This details will help to identify the diversity of the data\n* **% of Populated:** This column shows the cumulative percentage of the data that is populated in the column. The color gradients are used to encode values. \nMore populated values are represented in shades of green to show that these fields are fit to consider for next stages. Gradually it turns orange when we see 50% of population and color red denotes the values are critically low data population \n\n**HEAT MAP ACTIONS**\n\n![Profile_Heatmap_Sort_Filter_action|690x359](upload://zNXq7Pi7LWZyUvEKD2tlgvN0n2K.png)\n*Profile: 5.2*\n\n* The Heatmap allows users to sort values according to the sources. As shown in screen 5.2, clicking on upward or downward arrows will sort the values in Heatmap according to the preference chosen.\n* Hide and un-hide sources by clicking on radio button that present next the source name. The heatmap updates itself with every new change made\n* The lock button is used to make any data source to remain unchanged.  \n* Similar to the actions on source level, hide and un-hide options are available to field names, # of uniques and % of populated values. One click to change the heatmap according to requirement.\n* After making the changes, simply click on Apply button to update the heatmap\n\n**HEATMAP HOVER ACTIONS**\n\n![Profile_Heatmap_Hover|690x359](upload://dl41Hz0rgOFDc8FHQq46CMHJMXJ.png)\n*Profile: 5.3*\n\nUsers can hide any tags on the Heatmap by easily clicking on the eye button in front of every tag\n\n![Profile_Heatmap_Hover_FiledNames|690x359](upload://4t4R6GtiaaZBsPwcCHVufkTTkiq.png)\n*Profile: 5.4*\n\nWhen hovered on the name of the field, users can view the Top values with the total percentage of that value present in the data\n\n![Profile_Heatmap_Hover_FiledNames|690x359](upload://3TvfD7KnSbunMOVxKEErVLy1oWk.png)\n*Profile: 5.5*\n\nIn addition the details shown in screen 5.4, users can also view Top patterns of the field with the percentage of population for more understanding.\n\n![Profile_Heatmap_Hover_Population|690x359](upload://hiuETNWAQ1z0lvJrdxH63QU59eg.png)\n*Profile: 5.6*\n\nScreen 5.6 shows the action when hovered over the percentage of population column. This shows the details of the percentage used for color code\n\n**NEXT STEP**\nWhen the user is happy about the over all heatmap, they will be able to export them to a csv in just one click for further reference of their holistic profiled data.\n\nThis brings us to the end of Profiling step. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the PROFILE step in this category or reach out to the Community!"
"[2023 TPS REPORT - FARAMIR MVP.pdf|attachment](upload://s6QibcfJD3jlXeEFyvrgho4ZNHE.pdf) (8.1 MB)"
"**SUMMARY**\n\nAfter a detailed investigation on different fields of the dataset, organising and categorising them is essential to start working on our data. In order to do that, TRESATA offers a special feature known as \"Tags.\" These Tags are essentially keywords or labels that users can assign to critical fields, such as name, email, phone, ID, and more.\n\nWhen these Tags are assigned, they serve as canonical fields, enabling users to generate a comprehensive Heat-Map at the end of Profiling phase. Not only that, these tags become canonical fields, upon which the logic to connect datasets together created.\n\n**ADDING NEW TAG**\n\n![Profile_Add_New_Tag|690x359](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)\n*Profile: 4.0*\n\nAs shown on screen 4.0 above, to all the critical fields, users can easily add a new tag simply by clicking on the add button shown at the bottom of the screen\n\n![Profile_Save_Tag|690x359](upload://s0kYJ9fVB9oX8e84eEKnjFqky50.png)\n*Profile: 4.1*\n\nWhen user has finished adding a tag, clicking on save button will retain the tags that are assigned as shown in screen 4.1\n\n![Profile_Edit_Tag|690x359](upload://rPVijBKDbIAwdYMZDg8xnRUv22a.png)\n*Profile: 4.2*\n\n![Profile_Delete_Tag|690x359](upload://1Y9JH8QNhktHn57xbO6kclETseC.png)\n*Profile: 4.3*\n\nThe screen 1.1 shows that, the tags that are already assigned can be edited to a new tag at any point while doing the investigation of Fields. Similarly, as shown in screen 4.3, user can also remove the tags when found otherwise.\n\n**Best Practices To Add a Tag**\n\n![Profile_Tags_with_no_special_characters|690x359](upload://96JTaN0JPzU4YgjOhjklt6plRQK.png)\n*Profile 4.4*\n\n* **No Special Characters**: As shown in the screen 4.4, refrain using any symbols or special characters as tags, instead use more descriptive names \n\n* **Only One tag per Field**: Each field can have only one Tag assigned to it."
"This category consists of some Frequently Asked Questions"
"This category consists of guidelines relevant to Tresata tools & the Community platform."
"This category covers all relevant documentation related to the project."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"This category consists of all the relevant documentation that is required to complete the final “Orchestrate” step in the process of creating a data product. This includes:\n\n7.1 Workflow Orchestration\n&nbsp;&nbsp;7.1.1 Manual Orchestration\n&nbsp; 7.1.2 Automatic Orchestration\n\n7.2 Monitoring Orchestrated Workflow"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"just a quick update: latest argoscript 0.7.0-SNAPSHOT supports creating workflowtemplates using --wt switch"
"Thanks @sophia.lang, will keep in mind to get everyone to stay still for a picture :laughing: way forward. Infact we should aim to have a nice portrait haha :framed_picture:\n\nThanks again for the shout out & hoping to see our interns in action here :desktop_computer:"
"LOL at my picture! :rofl: @varun.community.mngr, HUGE shout out to you and your team for leading this conversation around the community and sharing about all the behind-the-scenes that made this community a success!\n\nWhile I'm not an intern - my favorite part of the talks was all of the ideas and topics that are available to be discussed in the community. The sky is the limit on what can be discussed, I look forward to learning more about what's new in the tech world!"
"\nHey there, amazing interns! 👋\n\nWe had a couple of incredible Tech Team Talk sessions over recently, where we introduced you to our thriving online community. We're thrilled to have you join us on this exciting journey of innovation and learning!\n\nDuring our virtual calls, we discussed the power of collaboration, shared valuable insights about our DE industry, and highlighted the endless possibilities that lie ahead. But this is just the beginning!\n\nWe want to hear from you, our talented interns! 🗣️ Share your thoughts, questions, or even a cool tech-related idea that has been buzzing in your mind. Don't hesitate to reach out and ignite conversations within our community.\n\n💡 **Here are a few ways you can engage with us:**\n\n1️⃣ Comment below with your key takeaways from the Tech Team Talk. We're eager to know what inspired you the most!\n\n2️⃣ Start a discussion thread on a tech topic you're passionate about. Share your insights, ask questions, and let's dive deeper into the fascinating world of technology together.\n\n3️⃣ Introduce yourself and tell us a bit about your tech interests, aspirations, or any cool projects you've worked on. We're excited to get to know you better!\n\nRemember, our community is a supportive space where ideas flourish, connections thrive, and learning never stops. We're here to uplift, inspire, and empower one another.\n\nSo, let's make the most of this opportunity and embark on an incredible journey of growth and collaboration. We can't wait to hear from you and witness your contributions shaping the future of technology!\n\n🌐 Join the conversation and be part of something extraordinary. Together, we'll transform the world, one innovation at a time! 💪\n\n#TechCommunity #FutureInnovators #JoinTheConversation #TechTalks #CollaborationMatters @Interns @sophia.lang \n\n**![|624x300](upload://1cgINCTTI0TvGUy3IKumhOv3Eyn.jpeg)**\nLooking forward to connecting with you all."
"**SUMMARY**\n\nLooking at statistics on a source level is not granular enough to really understand what's in the data, identify data quality issues or even categorise fields to fit one's needs and requirements. To do all that, the user needs an understanding of information on a field level, with overall statistics per field, visibility into actual values etc. All that an more, are available in TRESATA, enabling the user to get a holistic understanding of the data.\n\n**LEFT SIDE PANEL**\n\n![Profile_field_landing_correct|690x359](upload://lmHNwJrR3e3fjwwEKULF45f4Uss.png)\n*Profile: 3.0*\n\nAs shown on screen 3.0 above, in order for the user to investigate a source per field, they need to click on the expand button next to the source name on the left side navigation panel. Doing so, the source's view gets expanded, revealing all the fields included as well as a search bar to quickly search for a specific one. Notice that on the left side panel, around each field name there are some elements:\n\n* **Tags**: Under each name, there is a tag to categorise what this field is about. If there is no tag, that means the field hasn't been tagged yet (default state). You can learn more about this **crucial** tagging step ***here***. \n* **% Populated**: Represents the cumulative percentage of the data that is populated in the column. It gives an idea of the data's completeness. Notice that the values are color coded: the greener the color is, the more populated the field is. As the color is gradually approaching red, that mean there are more and more missing values.\n* **# of Uniques**: Shows the total count of unique fields present in the column. It helps identify the diversity or distinctiveness of the data.\n\nNOTE: You can use the sorting functionality next to the search bar to re-order the fields according to population and uniqueness, with flexibility on the order (ascending or descending).\n\nUsually, highly populated and unique fields are considered good to use later in the TRESATA processes. While that depends on the use case, it's always good to note down findings utilising the Notes functionality, as shown ***here***.\n\nWhile those metrics are good for high level field understanding, there still could be problems that the user can't identify unless seeing the actual values within a field. Imagine a scenario when a phone company is testing their phone numbers and for that exercise, create ten thousand records with 0000-000-000.. the data would look complete but in reality, this is bad placeholder data. Or even a scenario where, user input on phone numbers doesn't specify format, and users end up putting their numbers like 1234-123-123 while other prefer 1234123123. To solve this problem, TRESATA allows the user to go **even more granular** to the actual values within the fields. \n\n---\n**FIELD LEVEL INFORMATION**\n\nAs shown on screen 3.0, the middle panel reveals overview level statistics on the top bar and value based information on the rest of the middle panel. Often patterns or values can get very long which can cause truncation. Hovering over the truncated value will reveal the whole string, as in screen below:\n\n![Profile_Field_Level_Hover_Over|690x359](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)\n*Profile: 3.1*\n\nLooking at the value based information, the user can quickly see:\n\n* **Top Values**: Provides the top values that are most frequently populated in the column. It helps identify common or frequently occurring data values. often, by looking at the top values, it is easy to identify placeholder or bad values (i.e. 1234567890 phone number).\n* **Top Patterns**: The top recurring patterns observed in the column's data. Investigating this gives visibility on whether data within a column has standardised format (23-02-1999 or 23/02/1999).\n* **Top Formats**: Displays the most commonly used data format in the column. It helps identify the dominant data format or structure present in the column. \n\nKnowing that there is a problem isn't that useful if you can't understand how big of a problem it really is. That's why next to each of those, the user can see the actual percentage those values hold within the data. Additionally, the user can in real-time filter those values out of their data to identify how that would affect the overall statistics of that field as shown below:\n\n![Profile_field_unselect|690x359](upload://7ogxSFdoZ4dXZiy2MFidb9BexkD.png)\n*Profile: 3.2*\n\n![Profile_Field_Level_Hover_Over|690x359](upload://ps8mVq8uxOL7ZHOEFMbedGv4eIF.png)\n*Profile: 3.3*\n\nInvestigating that middle panel is a **very important step for a successful TRESATA workflow**, especially when the field being investigated is a critical data element (name, email, phone, id and more). During this process the user identifies Data Quality problems, trend and more.\n\n---\n\n**SAM : SIMPLE AUGMENTED MODE**\n\nTresata's very own chatbot SAM, suggests the you to take make effective choices with it's AI powered  suggestion. When user lands on profile page, SAM suggests the tags that are most important for your data source based on the resolution type on which the product was created.\n\n![Profile_SAM_Tag_Suggestion|690x359](upload://9xDFgMboaXm2ryPasCnxpRt7W8T.png)\n*Profile: 3.4*\n\nThe screen above is the representation on how SAM is suggesting most critical fields, where user can tag them from their raw data. Additionally, completely understanding the data is important for **tagging**, which is what powers the remaining TRESATA steps and will be explained ***here***."
"**SUMMARY**\n\nThe first step to understand your data.. is ensuring all of it is present to be investigated! TRESATA enables the user to do that by showing Overview Statistics, as a whole or per source, as well as information to ensure that this is the latest and greatest of your data.\n\n**PROFILE LANDING STATISTICS**\n\n![Profile_Overview_Statistics|690x359, 100%](upload://m1TgdgmuOsMC0JByXs0Q1Bmsep0.png)\n*Profile: 2.0*\n\nAs the user proceeds from SOURCE to PROFILE, this is the landing screen showing the high level statistics of what has been processed so far by the PROFILE job. At this point, there are two possibilities:\n\n* All of your sources are successfully profiled and the left \"Profiled Data Sources\" reflects all of them.\n* Some of your data sources are profiled and for the rest, the job is still pending. The left side panel shows only the sources for which the job has been completed and the statistics reflect those sources accordingly.\n\nNotice that on the left side panel, nothing is selected. That way, Tresata's engine will sum up the available metrics to give you a high level idea of **everything** that has been processed so far, across all the completed sources. Available metrics:\n\n* Number of Sources (Tables)\n* Number of Records (Rounded Up)\n* Number of Fields (Columns) \n\nBy looking at those statistics, the user can identify whether records have been dropped, tables have been skipped or even if there are fields that haven't been processed. However, those statistics become more interesting when looked on a source level.\n\n![Profile_Overview_Per_Source|690x359](upload://aPtmte89jjM7lFgPwg4vrrBHAdh.png)\n*Profile: 2.1*\n\nNotice that to get to that view, the user has now enabled the \"Selected All\" option on the left panel, meaning that the information on the right side will include all the completed sources statistics, per source. The elements included on the right tab now are:\n* **Number of Records**: Records (rows) for this source.\n* **Number of Columns**: Columns (fields) for this source.\n* **Path**: Location for this source on the cloud storage system (provides information about the directory or folder structure). For usability, the \"Copy to Clipboard\" functionality is present.\n* **Profiled On**: The last profile date for this source. For the early versions of TRESATA, this is to ensure that what you are looking at is the latest version of your data.\n\nTo enable the user quickly navigate through the right side panel, an auto scroll option is available. By clicking on the top right drop-down, the user can see a list of all their sources (the ones selected from the left panel) and decide which one they want to investigate.\n\nLooking at those statistics on a source level makes it easier to identify **if** anything is missing and **where** it is missing from. Once the user has identified that nothing is missing, they can proceed to have an even more granular investigation of the data, on a field level (checkout this post to learn more)."
"# BEST PRACTICES:\n\n1. **PROVIDE VALUE**: When posting content or sharing ideas, make sure it provides value to the community. This could include sharing relevant articles, research, or insights that are applicable to the community.\n\n2. **FOLLOW COMMUNITY GUIDELINES**: Be sure to follow the guidelines set forth by the community, including rules around behavior, language & content. This helps to maintain a positive & productive environment for all members.\n\n3. **USE TAGS & CATEGORIES**: When posting content or creating a new discussion, use relevant tags & categories to help organize content & make it easier for others to find.\n\n4. **SHARE SUCCESSES & CHALLENGES**: Share both successes & challenges with the community. This helps to create a culture of transparency & encourages others to share their experiences as well.\n\n5. **PARTICIPATE IN EVENTS:** Participate in events such as webinars, online meetups & other community events. This helps to build relationships & establish yourself as an active and engaged member of the community.\n\n6. **ENCOURAGE INTERACTIVE DISCUSSIONS**: Actively engage with community members by asking open-ended questions, seeking opinions, and encouraging discussions. This fosters a sense of belonging and stimulates meaningful exchanges. This can be done by facilitating group projects, shared initiatives, or collaborative problem-solving.\n\n**We are actively contemplating running a feature like \"Member of the Month\" on highlighting exceptional content. If we get good amount of responses we can make it happen! Vote now :point_down:**\n\n[poll type=multiple results=always min=1 max=2 chartType=bar]\n* Like It\n* Love It\n[/poll]\n \n\n\n**Call To Action** : \"Join the Conversation Now and Make Your Mark in the Community!\""
"Thank you for your thoughtful comment @alex! I appreciate your perspective on the overlap of responsibilities, It's true that there are quite a few shared activities and responsibilities between the two.\n\nAnd, I absolutely agree both department need to go hand in hand in order to deliver high-quality results."
"Love the PB&J analogy @alex, brings some real insight into this 💯..."
"Indeed it does Varun. A bit late to this post, but I don't disagree with the synopsis @Shantanubose laid out above. It's a very good high level overview of some of the differences. I'd also like to note that some of the things listed in the Comparisons could easily be argued on if those are distinct to one or the other discipline as I've found quite a bit of overlap in responsibility for the activities listed.\n\nI would like to emphasize your statement in the last paragraph though in that I personally don't think there can be an \"edge\" between the two disciplines. That's like saying peanut butter has an edge over jelly in a PB&J. While some may have a stronger preference to one over the other, I view them as absolutely complementary and necessary to effectively deliver any software implementation.\n\nYou can't do DE without an infrastructure and you don't necessarily need an environment if no engineering/ processing is going to occur. If anything, I consider them catalysts and enablers for the other and they both must coexist to deliver high quality products :handshake:"
"**SUMMARY**\n\nPROFILE is a crucial step in the flow. In this post, we will identify:\n\n* **WHAT** is the PROFILE step\n* **WHY** is it important\n* **HOW** should it be interpreted\n\n**WHAT**\n\nProfiling refers to the process of analyzing and understanding data sources to extract valuable insights. It involves examining the structure and content of the data. More specifically:\n\n1. **Structure**: refers to high level information about your data (# of records & fields), as well as field names and types (numeric, string and more). \n2. **Content**: refers to the actual values included on those fields. PROFILE will provide information to the user in regards to the Top-Values, Patterns and Formats included, overall non-empty population, number of unique values and more, all on a field level. \n\n**WHY**\n\nEverything described above is key metrics, but why? Why is seeing and understanding those metrics crucial for downstream operations? \n\nGaining knowledge about values for fields with millions or billions of records is an extremely difficult task. Identifying problems in the data (test values, placeholder \"bad\" data, multiple formats), data completeness (non-null values for a field) as well as the unique values per field (is there duplication? how much?) are all findings that will drive decisions on PREPARE and CONNECT. Consider the following example:\n\n***SCENARIO**: An airline wants to test their new promotion system, where all loyal customers get an automated email with personalized template and offerings. In order to test that system, they create 10000 different records with the value \"test\" for customer_name field and \"test@airline.com\" for customer_email. Two months later, a Data Engineer is tasked to clean the sources for downstream usage, but how will he know about those test records among millions of data points?*\n\nFor the above scenario, running PROFILE workflow and looking at the Top Values & Top Patterns returned, the user should identify that:\n\n* There is an unusual 4-letter pattern for full name\n* Among the top values for email, test@airline.com is there and needs to get cleaned\n\nWith that knowledge, they can now proceed to cleaning the data, enabling accuracy on later usage.\n\n**HOW**\n\nSo, WHAT is PROFILE and WHY is important has been established. But HOW to utilize PROFILE flow to maximize what you get from it?\n\nAs a user, the most powerful way to use PROFILE output is:\n\n* Ensure all of your data is profiled using the overview numbers of # records and fields profiled across all sources.\n* Making sure you look through your fields for unusual patterns, placeholder values, irregularities on formats or low populated fields and not those down for each source on your notes (*\"Notes\" icon on top right of your screen*) in order to not lose those findings. \n* Making sure you check on the Tresata Suggested Tags for each field (when they are available) and either approve them, if they accurately reflect the field you are investigating, or create your own tag (*PRO TIP:* There are some pre-configured common tags at your disposal to use). tagging is **essential** for a successful workflow\n* Investigate and understand the PROFILE Heatmap view as it is critical for decision making on the rest of the steps."
"@gstvolvr awesome work on this. I have a few thoughts on some other things to try to see if there are some multi-hop links that could be made beyond just using a fuzzy name. Not sure if you are doing so already, but some simple cleaning might help as well such as removing the punctuation. That would get companies 3 & 5 in your example of top duplicate company names above.\n\nOftentimes, those IDs (cik in this case) can help associate more severe discrepancies in text values like a name or address. If you were to add a join using the cik code it could increase the group size and potential universe of values that could be compared for a given tresataId.\n\nFurther, I'm willing to bet using the address data would add further lift and potentially resolve things over and above that. Like you mentioned though, just using name or an address could lead to bad matches so it might be worth qualifying those with additional canonicals like a country/ jurisdiction field.\n\nI'd be interested if some hairballs come out with that or if you see similar TID distribution to what you just ran on pure name.\n\nLastly, another interesting way to approach this might be to simulate hierarchies. If you use your output from this run as the input to the next, you could join on tresataID to recreate the initial grouping you just made and then loosen that logic further to create a higher abstraction that could span tresataIDs and create somewhat of the umbrella structure you mentioned. Think of it like the classic parent-child relationship, except for corporates in this case.\n\nOnce you've tried that, some interesting stats to check would be -\n\n1. Parent TIDs w/ multiple child TIDs\n2. Parent TIDs w/ multiple cik codes\n3. Cik codes with multiple parent TIDs\n4. Parent  TID record distribution (i.e. top X tids by records)\n5. Parent TIDs w/ multiple addresses\n6. Addresses w/ multiple Parent/ child TIDs"
"Thanks for sharing @gstvolvr, I'm sure our @DataEngineering team would love to share some thoughts on this discussion. :unlock:"
"# Intro\nI was looking to test out some of Twig's features and found this open source [SEC Company Name](https://www.kaggle.com/datasets/dattapiy/sec-edgar-companies-list) dataset. \n\nI outline my current process here – let me know if there's something else I should test out / try. \n\n# Data\nThe schema is pretty simple:\n```\nline_number: primary key\ncompany_name: whatever name they submit to the SEC\ncompany_cik_key: non-unique identifier for any company that submits filings to the SEC\n```\n\n`company_cik_key` can be repeated, and so can `company_name`, but a combination `(company_cik_key, company_name)` is unique. \n\nThese are the top duplicate `company_name`s: \n```\n{\n    \"hedge fund select\": 54,\n    \"nhit\": 29,\n    \"northern trust multi-advisor funds-series\": 18,\n    \"o'connor fund of funds\": 16,\n    \"northern trust multi-advisor funds- series\": 14\n}\n```\n\nThese are the top duplicate `company_cik_key`'s:\n```\n{\n    \"350001\": 19,\n    \"746631\": 11,\n    \"926844\": 11,\n    \"826683\": 10,\n    \"317788\": 10\n}\n```\n\nThere are `663,000` records in the original dataset. Of those, there are `610,499` unique `company_cik_key`s, and `657,159` unique `company_name`s.   \n\nWe can think of `company_cik_key` as providing one hierarchy level for the dataset, but I'm curious what resolution would look like on a scrubbed `company_name`, and if we can find another level in that hierarchy. \n\n# Findings\nAfter resolving the data set using the `company_name` I found some rather large groups, larger than anything indicated by `company_cik_key` or `company_name` after basic transforms. Here are the sizes for the top `tresataID`s:\n```\n{\n    \"e9480e3229fb105e4a9968d384c921aa\": 979,\n    \"d04f5e1c72ca730abfba6e4252277d2b\": 600,\n    \"37dfdd2472361d1868d4f10a0970128c\": 233,\n    \"48dd4d4404c649135f98eb51e1bfcc3c\": 187,\n    \"2d4802bbb819fd9f1afe65cfef23ba1c\": 187\n}\n```\n\nI looked at the top groups and they all seem reasonable. Here's a sample from the `tpid == d04f5e1c72ca730abfba6e4252277d2b`. \n```\n[\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1344127,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 626\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1460780,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 813\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1518105,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 1020\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1480631,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 913\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1487879,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 979\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1550259,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 1133\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1473464,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 890\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1518110,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 1024\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1480618,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 924\"\n    },\n    {\n        \"tresataId\": \"d04f5e1c72ca730abfba6e4252277d2b\",\n        \"company_cik_key\": 1537740,\n        \"company_name\": \"VAN KAMPEN UNIT TRUSTS, MUNICIPAL SERIES 1069\"\n    }\n]\n```\n\nOne can search for these on the SEC website, for example the [first](https://www.sec.gov/edgar/browse/?CIK=1344127) and [second](https://www.sec.gov/edgar/browse/?CIK=1460780) record. If you look at the `Company Information` section you'll see that they map to the `Business Address` but the phone number is different. \n\nI'm not sure if these should fall under the same `company_cik_key`, but they are certainly related. If we had more data other than name, e.g. company address, we could come up with a fairly good umbrella identifier that would capture all  `VEN KAMPEN UNIT TRUSTS` records -- only using name like I did during this exercise could lead to bad matches. \n\n\n# Configs \n#### canonical.yaml\n```\n---\ncanonical:\n- name\nsources:\n  sec__edgar_company_info:\n    pkey: line_number\n    transforms:\n    - type: scrubber\n      from: company_name\n      to: company_name\n      scrubber:\n        type: companyName\n      scrubber:\n        type: trimLower\n    mapping:\n     name:\n       field: company_name\n```\n\n#### pipeline.yaml\n```\n---\nsteps:\n- type: resolve\n  sources: [sec__edgar_company_info]\n  config:\n    weighted:\n      components:\n      - field: name\n        weight: 1.0\n        type: name\n      threshold: 1.0\n```\n\n#### output.yaml\n```\n---\nsources:\n  sec__edgar_company_info:\n    stagingFields: []\n    canonicalFields: [name]\n    summaryFields: []\n```"
"@DataEngineering team, looking forward to some support in having this take off as the @Interns are eager to see this kickoff. :muscle: :rocket:"
"With great power comes great responsibilities... :spider_web: :melting_face:"
"@noopur \nYou're welcome! I'm glad that the post was helpful in understanding the dynamics between the two departments."
"@varun.community.mngr I can feel the pressure! It's reaching new levels."
"Struggling to make confident posts in our data engineering community? 😟\n\nNo worries, I've got your back! Let's explore some creative ways to captivate your audience and make your posts shine. 🌟\n\nHere are some tailored ways to share your insights within our community:\n\n#1 Compile a list of data engineering trends and insights, and present it as a post or visually appealing infographic.\n\n#2 Pose thought-provoking questions related to data engineering, inviting our members to share their valuable insights and opinions.\n\n#3 Start a video or Tresata podcast series where we can interview our industry experts, allowing them to share their unique perspectives and insights.\n\n#4 Share amusing or intriguing anecdotes from your experiences in data engineering, offering valuable insights along the way.\n\n#5 Bust common misconceptions or myths about data engineering in a post, providing insightful explanations to debunk them.\n\n#6 Discuss recent news articles or reports relevant to data engineering, sharing your own insights and opinions on the matter.\n\n#7 Provide practical tips and advice in your posts, helping fellow community members enhance their skills and knowledge.\n\n#8 Recap and share key takeaways from industry events or conferences you attend, offering valuable insights to our community.\n\n#9 Shed light on new technologies or innovations impacting data engineering, sharing your insights on their potential industry implications.\n\n#10 Showcase case studies or success stories highlighting positive impacts made by Tresata or the data engineering industry.\n\nRemember to make your posts engaging and informative, adding value to our community. Visuals like infographics, videos, and images can make your posts more appealing and shareable.\n\nLet's inspire and empower each other in our data engineering journey! 🚀💡 \n\n**Call To Action: Let's use these 10 steps to help make our first or second...post today, shall we?**\n\n#makingposts #communitybuilding #dataengineering"
"I see another post in the making :muscle: @Shantanubose :rocket:"
"Thanks @Shantanubose for this post. This brings in clarity of the two departments and their responsibilities. \nThis helps us align our understanding of how each team co relate with each other."
"Thanks @JustyceL \nSure will definitely contribute more..."
"Congratulations on your first post :1st_place_medal: Justyce. Welcome to the Community & we should also be seeing more posts coming our way that a non-technical member like myself will easily understand too :muscle:"
"Great summarization here @Shantanubose! :slightly_smiling_face:\n\nReally breaks down the differences in a way that a non-technical user can understand with ease. Looking forward to reading more about the work these two teams take on!"
"**SUMMARY**\n\nAt this point, the user has validated their source is the one needed for this Data Product and no problems have been detected. Now, the only thing remaining is adding it and proceeding to PROFILE.\n\n**ADDING A SOURCE TO CART**\n\nTo add a source to the Cart, the user has to complete the whole journey of:\n\n* Choosing a format and checking the schema\n* Checking at least one field for the First 10 values\n\nOnce both those actions are done, the *\"Add To Selected Data Sources\"* button will be enabled for the user to use as shown below:\n\n![SOURCE-values-filled|690x359](upload://yVDGPF3Ijq4Se6rZSwTBAne5DUV.png)\n*Screen: 3.0*\n\nClicking on the *\"Add To Selected Data Sources\"* button, will reflect on the counter seen next to the \"Continue to Profile\", and the source boromir-gb-1 will be visible on that panel upon clocking on it as shown below:\n\n![SOURCE-CART-remove-one|690x359](upload://nqSAMWit9bHkEthtTlHqEp1aAC2.png)\n*Screen: 3.1*\n\nWhile in the cart, the user has the ability to perform specific actions:\n\n* **Individual removal**:\nIf the user click on the button as shown on screen 3.1, then they will see a pop up asking for confirmation for this action (screen 3.2). According to that action, the source will either be removed from the cart or will remain as initially selected.\n\n![SOURCE-remove-one-messages|690x359](upload://2KGxHAJZCIt0aQieVme7X5HDaa4.png)\n*Screen 3.2*\n\n* **Mass removal**:\nWhen the user click on the *Select All* check box besides *\"Data Source(s)\"*, a \"*Remove Selected*\" button appears, giving the option for the user to remove all the selected Data Sources. The user can adjust their selection by unselecting specific ones and then perform the action by clicking on that button. Screen 3.3 shows that view while screen 3.4 shows the confirmation pop up.\n\n![SOURCE-remove-all-button|690x359](upload://13djyuXo7jVqFW475ls6oEiNvPt.png)\n*Screen: 3.3*\n\n![SOURCE-remove-all-pop-up|690x359](upload://t72PAg0lgDNFcg0GLqYtDh1nQT.png)\n*Screen: 3.4*\n\n* **Continue to PROFILE**:\n\nWhen selecting this button, the user has finalized their selections and is ready to move to PROFILE. Once clicked, the user will get a confirmation pop up as shown in screen 3.5 and upon validating, they will be redirected to the Profile job monitoring panel where they can track the progress of the PROFILE job (screen 3.6).\n\n![SOURCE-proceed-profile|690x359](upload://q1brrm6kWlgdX8dfnMfrO4ylk0k.png)\n*Screen: 3.5* \n\n![PROFILE-job-status|690x359](upload://57wEWqhXD2JmsFd2KzcePHFPB2y.jpeg)\n*Screen: 3.6*\n\nThat concludes the SOURCE journey. If you are experiencing any problems sourcing problems, checkout the post dedicated to error on the SOURCE step in this category or reach out to the Community!"
"**SUMMARY**\n\nLooking at the file structure, it's easy to navigate and reach to a file, but how can the user ensure that they are using the correct file for their Data Product? Tresata enables that through:\n\n* Easy access to the source schema to verify the fields included\n* Easy access to the source first 10 records of user's file, to avoid corrupted files and give visibility on the actual values!\n\nBut how to do all that?\n\n**GET SOURCE SCHEMA**\n\nIn Tresata, the schema of a source will reveal the source's structure, i.e. the fields included in that source. Looking at the schema before In order to do that, the user has to navigate to the file they want to investigate by using the left side navigation panel or applying a direct search query using the whole path. Once the user has reached the desired, the screen should look like screen 2.0:\n\n![Sourcing-nested-expanded-final|690x359](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)\n*Screen: 2.0*\n\nNotice in the middle panel, the user is now prompted to choose the format their file has. A list of acceptable file formats will be available in the expand dropdown next to the *APPLY* button. The list of the available formats are:\n\n**Formats**: *parq*, *csv*, *csvu*, *bsv*, *bsvu*, *tldsv*, *tldsvu*, *casv*, *casvu*, *avro*, *json*, *orc*\n\nThe user can use the applicable to their source format as shown below:\n![SOURCE-schema-apply|690x359](upload://veh4chAvVcdqYPpcc37l8xOSdwS.png)\n*Screen: 2.1*\n\nOnce the format is applied, if correct (if not, checkout the dedicated post on errors in SOURCE), a list of the fields for that source will be available as shown on screen 2.2 below:\n\n![SOURCE-schema-fields|690x359](upload://9vSr2fssCAtpVWJGAaONfNutzCU.png)\n*Screen: 2.2*\n\n**NOTE**: If the source has multiple partitions, user can still run this on the parent folder and as long as all the partitions have the same format and schema, there should be no failures.\n\nThis is the first verification point, where the user can validate whether the source they are looking at actually has the fields needed for their Data Product. If yes, the next step is checking out the First 10 records for those fields...\n\n**GET FIRST TEN**\n\nNow the user knows the structure of the data, but what about the underlying values? What if the data is corrupted to if, for some reason, the whole file is empty? All those and more can be answered by simply looking at the first 10 record values for the selected fields.\n\nTo check out the data the user has to select the fields (up to 4 fields at one time) as shown in screen 2.3 below:\n\n![SOURCE-search-selection|690x359](upload://jjsu3MSNy7kQpIuyMa2PYWQTEFf.png)\n*Screen: 2.3*\n\nNotice that in the screen above, the user has utilised the available search box to quickly search and select the fields they are interested in exploring. Upon selecting the fields, the user can now *APPLY* the selection and the screen will look like this:\n\n![SOURCE-values-filled|690x359](upload://7zhEYcMbKpJT8iigkXCKTgX9Ysb.png)\n*Screen: 2.4*\n\nAt this point, the user can take multiple actions:\n\n* **Unselect a field**: will reorder the middle panel, putting the unselected field first after all the selected ones. The right side panel showcasing the values won't change until the *APPLY* button is clicked. The user can now pick another field(s).\n* **RESET**: will unselect all the selected fields, allowing the user to select new fields. The right side panel showcasing the values won't change until the *APPLY* button is clicked. \n* **Expand**: will expand the right side panel to provide more real estate for the values of the 4 fields selected. The screen of the view is shown below in screen 2.5.\n\n![SOURCE-values-expanded|690x359](upload://ddyPaynyQveShjQB9p4564tUW1D.png)\n*Screen: 2.5*\n\nAt this point, the user has all the necessary information to make a good choice whether this is one of the sources needed for this Data Product. In the next post, we will explore how to add a source to the Cart and the *Continue to Profile* flow."
"**SUMMARY**\n\nNavigating a directory structure is critical for anyone who interacts with files regularly. A directory structure, also known as a file system, is a hierarchical organization of folders and files that helps users manage and access data. Being able to effectively find and identify sources within a file system can lead to significant benefits in terms of efficiency and productivity. While the process might seem straightforward, it can become challenging as directory structures become more complex. \n\nTRESATA alleviates this complexity by providing an easy-to-use, flexible interface for users to quickly navigate through all levels of their file system.\n\nIn the following post we'll give a walk through of how TRESATA enables the user to:\n\n* View all objects (folders and files) existing on a user's back-end\n* Navigate quickly through a given file structure \n* Search a directory for a file path\n* Star important files and selectively access them\n\n\n**VIEW FILE SYSTEM**\n\n![Landing-Sourcing|690x360](upload://at20ZiFZOBnKMX7VuMuU4qynGr2.png)\n*Sourcing: 1.0*\n\nLooking at Screen 1.0, user will see an overview of the their files system. The left side panel mirrors the object structure existing on the back-end, giving visibility to the user on what folders and files exist on their system. In this example, the user has six directories: *\"usecases\"*, *\"tresata-generated\"*, *\"user\"*, *\"ascii-directory\"*, *\"tmp\"* and *\"demos\"*.  \n\nFurther breaking down the left side *\"Objects\"* panel:\n\n1. Each table is a file that can't include more files nested inside, while a directory is a folder including other directories or files within it.\n2. Each directory will have an *expand* indicator on the top right of it box, which when clicked will reveal the nested folder and tables (see example below in Screen 1.1)\n\n**NAVIGATE FILE SYSTEM**\n\nOnce the user is familiar with their file structure, the next step is to move in to a specific directory so that they can view its contents and easily find the file they are interested in. \n\n![SOURCE-no-sources-added|690x361](upload://k0dJvVr7T6nf7gJpkCYwOAlUQ9E.png)\n![SOURCE-no-sources-added|690x359](upload://pbtOycwsOtSGzHhua11JxVGUt65.png)\n*Sourcing: 1.1*\n\nIn Screen 1.1, we can see that *usecases* has been expanded to reveal the directories nested within. Notice that there is a search box where the user can search for a specific file or directory and see the results returned in real-time. However, if the user does not know the exact name of the file they are looking for, they can continue to navigate through the available objects until they reach the specified file. \n\n![Sourcing-nested-epanded-final|690x362](upload://fleCAkjW9HCGbUHJlrw8FFQZUro.png)\n![Sourcing-nested-epanded-final|690x359](upload://tl2d4euShoojjeRYhU7CVcHHGSr.png)\n*Sourcing: 1.2*\n\n*Tip:* To return to the structure where *usecases* is shown as the parent directory, the user has to click to \"**Go to usecases**\" and the subdirectories under *usecases* will be displayed.\n\nGiven the complexity of a file structure, it is critical for the user to know the exact location of file so that they can return to it with ease. TRESATA automatically displays the location of their file as the user navigates further down the nested structure. Screen 1.3 shows a clear example of this. Notice that the path is hyperlinked - clicking on any of the directories there will filter the left side panel and render the nested folders/files within that directory. \n\n![SOURCE-truncated|690x359](upload://mxzd5MtHwTDb87PFDa4oPMqFXQh.png)\n*Sourcing: 1.3*\n\nAside from redirecting to a specific directory through the path, the user can also copy to clipboard by clicking on the button next to it as shown on screen 1.4.\n\n![Source-clipboard|690x359](upload://g3p0j7NbWxIQkwvCavvSuDUGVw9.png)\n*Sourcing: 1.4*\n\n\n**ADVANCED SEARCH**\n\nTRESATA has a universal search box with a \"Search using file path\" message. The user can paste a file path which, enabling a direct search that will be reflected on the left side *Objects* panel.\n\n**NOTE**: This is just the beginning. TRESATA's advanced search capabilities for SOURCE will be enhanced on new releases, allowing for many more ways for the users to directly search for their sources.\n\n**STAR RELEVANT FILES / DIRECTORIES** \n\nUsers may often need to access the same set of files or directories repeatedly, making it cumbersome to navigate through a complex file system each time to locate and apply transformations to those files. To simplify this process, TRESATA offers a feature that allows users to star files. Once starred, these files can be viewed separately, saving time and effort.\n\nTo star a file or directory, locate the desired file or folder and click on the star symbol located just before its name. When clicked, the star will turn blue as shown in screen 1.5.\n\nNow for displaying only starred files, click on the toggle next to *\"Objects\"* that has \"Show Starred only\" written beside it as shown on screen 1.6."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"This category consists of all the relevant documentation that is required to complete the *\"Source\"* step in the process of creating a data product. This includes:\n\n1.1 Navigate & Select Your Source\n&nbsp;&nbsp;1.1.1 View all objects (folders and files) existing on a user’s back-end\n&nbsp;&nbsp;1.1.2 Navigate quickly through a given file structure\n&nbsp;&nbsp;1.1.3 Search a directory for a file path\n&nbsp;&nbsp;1.1.4 Star important files and selectively access them\n\n1.2 Get Information About Your Source\n&nbsp;&nbsp;1.2.1 Get Source Schema\n&nbsp;&nbsp;1.2.2 Get First Ten Records\n\n1.3 Add Source And Continue To Profile"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"We are thrilled to have you join our incredible community of talented individuals! As you embark on this exciting journey, we want to extend a warm welcome and help you connect with your fellow interns. Here's how you can start building valuable relationships and making the most of your time here:\n\n1️⃣ **Introduce Yourself:** Take a moment to introduce yourself in the comments below. Share a bit about your background, interests, and what you hope to gain from this internship. Our community is filled with supportive members eager to get to know you!\n\n2️⃣ **Engage in Discussions:** Explore the various discussion threads and dive into conversations that pique your interest. Whether it's sharing insights, asking questions, or providing feedback, your voice matters. Feel free to contribute your unique perspectives and learn from others.\n\n3️⃣ **Connect with Peers:** Use this opportunity to reach out and connect with your fellow interns. Collaborate on projects, exchange ideas, and support each other throughout this journey. Building strong relationships with your peers is a key ingredient for success.\n\n4️⃣ **Seek Guidance and Mentorship:** Our community is a hub of knowledge and experience. Don't hesitate to seek guidance from seasoned professionals and mentors within the community. They are here to help you grow and excel in your field.\n\n5️⃣ **Participate in Events and Activities:** Stay updated on our community events {we have an events category to dive deeper}, webinars, workshops, and other exciting activities. These events provide great opportunities to expand your network, learn new skills, and have some fun along the way.\n\nRemember, the power of our community lies in the connections we build and the knowledge we share. We encourage you to be proactive, supportive, and open-minded as you navigate this enriching experience together.\n\nOnce again, a warm welcome to our community! We can't wait to witness your growth, contributions, and the meaningful relationships you'll form.\n\n**✨ Together, let's make this internship unforgettable! ✨**\n\n#WelcomeInterns #CommunityConnections #BuildingRelationships"
"\nDear fantastic community members,\n\n\nWelcome to our thriving online community where collaboration, growth, and a sprinkle of data magic await you! We're here to share an important compass that will guide us on this incredible journey together: our **Community Code of Conduct.** 📜\n\n\n**Summary:TLDR (Too Long, Didn't Read) section**\n🤝 Respect and inclusivity are at the heart of our community guidelines. Share knowledge, engage respectfully, and avoid spam or plagiarism. Together, let's build a safe space for data engineers to thrive!\n\n\n**Detailed version**\n\nAt its heart, this code serves as a shining beacon, outlining the behaviors and standards that foster a respectful and inclusive environment. It's our collective commitment to creating a space where everyone feels valued, inspired, and empowered to be their authentic selves by defining acceptable conduct, addressing unacceptable behaviors, and providing mechanisms for reporting and enforcement. Let's dive in and discover the key principles that make our community sparkle:\n\n\n🤝 **BE RESPECTFUL**: Treat fellow members with kindness, respect, and empathy. We embrace diversity and celebrate the unique perspectives that each of you brings to the table. Let's foster an environment free from personal attacks and offensive language, making room for constructive discussions and positive connections.\n\n\n🚫 **NO SPAMMING OR SOLICITING:** Our community thrives on genuine interactions and meaningful discussions. Let's refrain from bombarding our fellow members with unsolicited promotions or irrelevant content. By keeping the community free from intrusive spam, we create a space that encourages engagement, growth, and long-lasting connections.\n\n\n🎯 **STAY ON TOPIC**: As members, our focus is centered around data engineering best practices, insights, and information. Let's keep our posts and discussions aligned with our community's purpose. By staying on topic, we ensure that each interaction provides value, knowledge, and inspiration to our fellow data enthusiasts.\n\n\n📚 **SHARE KNOWLEDGE & EXPERIENCES**: The true strength of our community lies in our collective wisdom and experiences. We encourage you to generously share your knowledge, insights, and experiences related to data engineering. Engage in meaningful discussions, spark curiosity, and inspire others by posting articles, tutorials, and other valuable resources. Together, we can create a powerful learning ecosystem that elevates everyone's skills and expertise.\n\n\n🔒 **NO PLAGIARISM**: Originality is the name of the game! Let's honor the hard work and intellectual property of others by avoiding plagiarism. Post only original content or give proper credit to the creators when sharing external resources. By nurturing a culture of authenticity and integrity, we uphold the highest standards of professionalism within our community.\n\n\n💼 **RESPECT PRIVACY**: We hold privacy in high regard. Please refrain from sharing personal information about others without their consent. Let's create a safe space where confidentiality is respected, and our fellow community members can trust in the professionalism and discretion of their peers.\n\n🤝 **ENGAGE WITH OTHERS**: Embrace the true spirit of community by actively engaging with your fellow members. Respond to posts, participate in discussions, and foster connections. Your involvement and support inspire others to contribute and create a vibrant atmosphere buzzing with enthusiasm. Remember, a simple comment or question can ignite powerful conversations and forge lifelong connections.\n\n\n🗣 **COMMUNICATION WITHIN THE COMMUNITY**: Effective communication is the lifeblood of our community. Let's keep it clear, concise, and courteous. Proofread your messages for spelling and clarity to ensure your thoughts shine through. Use inclusive and appropriate language, avoiding technical jargon that may confuse others. Above all, let's treat one another with respect, kindness, and understanding, even in times of disagreement or conflict.\n\n\n🙌 **BE HELPFUL & INCLUSIVE**: Our community is built on the pillars of support and inclusivity. Let's go the extra mile by sharing our knowledge, experience, and insights. Help your fellow community members grow, learn, and achieve their goals. Embrace diversity, be welcoming to people from all backgrounds and cultures, and stand against discrimination or harassment in any form\n\n\n.🌍 **REPORTING A CONCERN / VIOLATION: Your Voice Matters!** 🌍\n\nShould you come across any issues within our community, we encourage you to speak up. \n\nReporting problems helps us maintain a safe and positive environment for everyone to thrive. \n\nTo report a violation of our guidelines, simply fill out our \" **[Contact Me](https://tresata.com/contact/)** \" form. \n\nYour privacy is important to us, and you have the option to remain anonymous by using aliases instead of real names. \n\nPrompt action will be taken upon receiving your report, and we will keep you informed about the progress and any necessary measures we take. \n\nAdditionally share any relevant details in the message section that might help address the problem. \n\nSharing screenshots below to assist. :arrow_heading_down:\n\n**![|624x308](upload://qVzHmyQupcycelOSyLwn5CHYZv6.png)**\n\n**![|624x308](upload://eCnceC7YKuTLTw9sFsUsSg3lw7G.png)**\n\n\n🔔 **ENCOURAGE MEMBERS TO SPEAK UP**: Remember, you are the eyes and ears of our community. Your active involvement in maintaining a respectful environment is vital. If you witness any behavior that goes against our community guidelines, we urge you to speak up. Your courage empowers us to address violations swiftly and effectively, ensuring a safe and welcoming space for all.\n\n\n⚖️ **ENFORCE THE GUIDELINES**: Upholding the Integrity of Our Community ⚖️\n\nEnforcing guidelines is paramount in maintaining the integrity of our community. Transparency is key, and we want you to be fully aware of our enforcement process. To ensure fairness and effectiveness, we undertake periodic reviews to assess the number and types of violations reported, our response time, and the consistency of enforcement actions. By keeping an open log of enforcement actions, we promote transparency, accountability, and a shared commitment to upholding our guidelines.\n\n\n **CONSEQUENCES FOR VIOLATING GUIDELINES :loudspeaker::**\n\n  1. Our Community has consequences for violating guidelines to ensure transparency and consistency in enforcement.\n  2. A warning or notification from a moderator or administrator.\n  3. A temporary suspension of posting or participation in the community.\n  4. A permanent ban from the community.\n  5. Removal of the violating content.\n  6. Requiring the user to publicly apologize for their actions.\n  7. Legal action if the violation is severe enough to warrant it.\n\n\n🚀 ****REVIEW & UPDATE GUIDELINES**: A Living Document for an Evolving Community** 🚀\n\nOur guidelines are a living document, evolving alongside our community's needs and aspirations. Regular reviews ensure that they remain relevant and effective in maintaining a safe and inclusive space for all members. We value your input and will actively involve you in discussions when updates are necessary. Together, we can shape our guidelines to reflect the dynamic landscape of our members and nurture a community that resonates with your expectations.\n\n\n💪 **PROVIDE SUPPORT & RESOURCES: Guiding You on Your Journey** 💪\n\nWe're here to support you every step of the way. Navigating community guidelines and engaging in meaningful discussions can sometimes be a challenging endeavor. That's why we offer resources to help you thrive within our community. Whether it's providing information on reporting violations, offering learning and development opportunities, or connecting you with our dedicated support team for any questions or concerns, we're committed to making your journey with us as smooth and enriching as possible.\n\n\n🤝 **Join Us in Shaping the Future of Our Community!** 🤝\n\nTogether, we are building more than just a community; we are shaping the future of data engineering. By adhering to our code of conduct, fostering meaningful connections, and sharing your knowledge and experiences, you contribute to a vibrant and supportive ecosystem. Let's continue to inspire, learn, and grow together, making our community an exceptional space for all members.\n\n\nThank you for being a valued member of our extraordinary community. Your contributions are cherished, and we look forward to embarking on this exciting journey with you!\n\n#CommunityEngagement #TLDR #InclusiveCommunication\n\n\nWarmest regards,\nVarun\nCommunity Manager"
"Hey there, members!\n\nWe've noticed that some of you are using default usernames like \"user1\" or \"user2.\" While these names are functional, we believe that your **unique identity deserves to shine**! It's time to step into the spotlight and make your mark on our community. 🌟\n\nWhy is having a distinct username important, you ask? Well, let us tell you:\n\n1️⃣ **Personalization**: Your username is the window to your personality. It's a chance to express yourself and showcase your individuality. By choosing a unique username, you create a personal brand that sets you apart and allows other members to recognize and remember you. It's like leaving a digital footprint that reflects who you are and what you bring to our community.\n\n2️⃣ **Connection Building**: A username is not just a random string of characters; it's an invitation for connection. When you have a memorable username, it becomes easier for others to engage with you, mention you in discussions, and build meaningful relationships. It sparks conversations and facilitates collaboration, turning our community into a thriving hub of interactions.\n\n3️⃣ **Professional Reputation**: Our community is a hub for professionals like yourself to network, learn, and grow. Having a username that reflects your professional identity enhances your reputation within the industry. It demonstrates that you're serious about your craft and committed to contributing valuable insights. This can open doors to new opportunities, collaborations, and even career advancements.\n\nSo, how can you transform your generic username into something extraordinary? It's simpler than you think! Follow these quick steps shared below with screenshots :arrow_heading_down:: \n\n1️⃣ Click on your current username at the top right corner of the page.\n\n2️⃣ Select \"**Preferences**\" from the drop-down menu. When you click on preferences you can see the window where you can edit the username. \n\n3️⃣ In the \"Username\" field, enter a unique and memorable username that represents **YOU.** Changing username is allowed for **first 60 days** since joining our community. \n\n4️⃣ Hit the \"Save Changes\" button at the bottom, and voila! You've successfully unleashed your authentic identity!\n\nIf you need any assistance or have questions post that timeline, our friendly support team is here to help. Just drop us a message, and we'll guide you through the process. Remember, this is your chance to make a lasting impression and be part of a thriving community where everyone knows your name (or username, in this case). Let your creativity run wild, and don't be afraid to embrace your uniqueness.\n\n\nJoin us in celebrating the power of individuality & see you on the other side with your brand new username! 💫\n\n![Screenshot 2023-06-13 at 13-06-01 Tresata Community Forum (1)|391x500](upload://5qAbZd50KDTfqbAMQJsGD3K2WLr.png)\n\n\n\n![Screenshot 2023-06-13 at 13-06-57 Tresata Community Forum (1)|690x182](upload://5LWc9u8Ypn9VNTpYVowD4onkk4.png)\n\n\n#community #username #learn"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Dear Hackers, \n\nWe would like to extend our sincerest gratitude and appreciation to each and everyone of you who participated in hackathonBLR MMXXIII. Your passion, dedication, and innovative spirit made this event an unforgettable experience that exceeded all expectations. Congratulations to all participants for this incredible effort & a **thunderous applause** for our **winners**. {Sharing some images of their celebrations below.) \n\n\nTogether, we embarked on a journey to unite the brightest minds in tech talent from all corners of Bangalore and beyond. With BIG Data, BIG Ideas, and a shared commitment of BIG Community Impact, we witnessed the power of collaboration and problem-solving at its finest.\n\n\nWe were blown away by the overwhelming response and enthusiasm from the hacker community. With a remarkable turnout of 120+ people in attendance & an impressive 44 teams that submitted their final projects for judging, hackathonBLR MMXXIII truly showcased the incredible talent and potential within our city.\n\n\nYour hard work and dedication resulted in something truly remarkable. Not only did we bring together like-minded individuals with a common goal of improving population health, but we witnessed firsthand the transformation of ideas into tangible solutions. The impact of your collective efforts will continue to resonate within our community for years to come.\n\n\nWe are incredibly proud of what we accomplished together, but this is just the beginning. The success of hackathonBLR MMXXIII has laid the foundation for an even grander event in 2024. We are already hard at work, planning an event of larger scale and more impactful challenges. Stay tuned for updates and details on how you can join the official organizing committee and be part of shaping the future of this incredible event.\n\n\nOnce again, we extend our deepest appreciation to each and every one of you. Your time, talent, and unwavering commitment to excellence have truly made a difference. Thank you for being part of Bengaluru's biggest, boldest, and most transformative tech event.\n\n\nWe look forward to having you join us again for next year's hackathonBLR. Together, we will continue to push boundaries, drive innovation, and create a positive impact in our city and beyond.\n\nWith heartfelt gratitude,\n\nThe hackathonBLR + Tresata Team\n\n**Yours in Code...**\n\n![PXL_20230610_130121798|281x500](upload://rHT33ee6dA03TfI7R6gryUbuPNq.jpeg)\n\n![IMG_7907|666x500](upload://rfzPs9fCCpCuHJ2PPAauvZP6JQr.jpeg)\n\n![IMG_7906|666x500](upload://e6gyBZl10rKP12LL30LtUPRa1EZ.jpeg)\n\n![IMG_4162|690x460](upload://x4MwRNeaeNwVNLKP5lztiFs4qAc.jpeg)\n\n![IMG_4161|690x460](upload://5K7msvxtqB89Yb3H8vVdSunFdN4.jpeg)"
"Thanks for sharing your thoughts Abhilasha & excited to see the first of many posts in the near future :1st_place_medal: :rocket: ."
"A little late to post but very excited about community and the opportunities this platform brings for everyone!"
"All the best Kritik & looking forward to hosting you. 👍"
"Looking forward to the event"
"Thanks for sharing that Rashmi, looking forward to hosting you tomorrow.\n\nCongratulations on your first post. 🥇"
"Excited for the event!!"
"🌟 Join us at HackathonBLR MMXXIII, the most exciting tech event of the year! 🌟\n\n🏆 While we believe that the true **reward** lies in the **knowledge** gained and the **connections** made, we also have some exciting surprises in store for our participants! Check out the image attached to this post for a glimpse of the **incredible prizes** that await you.\n\n✨ But remember, it's not just about winning prizes. It's about the journey, the learning experience, and the opportunity to contribute to a greater cause. Let's harness the power of technology to make a difference and create a better tomorrow for all.\n\n🗓️ Save the Date: June 10th, 2023 **11 am IST** (kindly be on time)\n📍 Venue: **Wework, ETV in Bellandur**\n⌛ Duration: 4 hours of non-stop hacking fun\n\n🌐 To register and learn more about the event – > https://www.eventbrite.com/e/tresatas-hackblr-mmxxiii-tickets-633252594557 \n\n🔗 Share this post with your fellow tech enthusiasts and let's create a vibrant community of hackers, united by a common goal to innovate, collaborate, and impact lives!\n\n🙌 Together, we can make HackathonBLR MMXXIII a resounding success! See you there!\n\n#HackathonBLR #TechForACause #InnovationUnleashed #CommunityImpact\n\n![image|431x500](upload://gCkhtv2unHZH3jHYX3MQEB6j6cO.jpeg)"
"**Groups** organize members around a particular interest or goal. This helps in creating sub-communities within a larger community & for members to connect with others who share similar interests. Best practices include:\n\n1. Creating clear & **descriptive group names** & descriptions.\n\n2. Setting **guidelines** around appropriate use of groups like no spamming, no harassment.\n\n3. Encouraging members to **join groups** that are relevant to their interests.\n\n**Users** are members of the community & it's important to create a welcoming & inclusive environment for all members. Best practices include:\n\n1. **Responding** to questions and comments in a timely manner.\n\n2. Providing **resources** & support to help members get the most out of the community.\n\n3. Celebrating & **recognizing contributions** of members through badges or rewards.\n\n**Badges** - **Recognize & reward members** for their **contributions** to the community. This is useful for motivating members to participate more actively & for creating a sense of community pride. Best practices include:\n\n1. Creating **badges** that are meaningful & **aligned** with the **goals** of the community.\n\n2. Setting clear **criteria** for earning badges.\n\n3. Celebrating & **recognizing** **members** who earn badges.\n\n**Review:** **content** needs to be **reviewed regularly** for maintaining a healthy & engaged community. Best practices include:\n\n1. Encourage members to **review existing content**, discussions & resources before posting new questions or starting new threads & keep this monitored.\n\n2. Guidelines outlined for **review & moderation** of user-generated content to maintain a high quality of information & discourage spamming or inappropriate behavior.\n\n3. Use **review data & feedback** to improve user experience & address any concerns that may arise.\n\n\n**About:**\n\n1. Clearly **defined purpose & goals** of the community including intended audience & key topics of discussion.\n\n2. Overview of **community guidelines** emphasizing importance of respectful & constructive communication.\n\nHighlight special features or resources available to the community members, such as access to **exclusive content** or opportunities to **network** with other professionals.\n\n**Call to Action**: Discover the hidden gems in the 'More' section and level up your community engagement! Share your favorite 'More' features or any tips you have in the comments below. Let's learn, collaborate, and make the most of our community experience! \n\n#CommunityPostHowTo #MaximizeEngagement #CommunityGuide #CommunityTips #EnhanceEngagement #MoreSectionFeatures #CollaborateAndLearn"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"**How to tag:**\n\n1. Use **relevant** & specific tags to make it easier for others to find & engage with your content.\n\n2. Avoid **over-tagging**, as this can be seen as spammy and make it harder for others to find what they're looking for.\n\n3. Use both **broad & specific** tags to maximize visibility & engagement.\n\n**How to engage:**\n\n1. Be **respectful & professional** in all **interactions** with the other community members.\n\n2. **Read & respond** to others' posts in a timely manner to foster a sense of engagement.\n\n3. Use reactions to **like, comment, bookmark** & show **support** for others' posts. Additionally, one can share their own perspective, ask questions, or offer feedback.\n\nShare their **own content** to contribute to the community & showcase their expertise.\n\n**Call to Action**: Share your favorite tagging tips and strategies in the comments below! Let's connect, engage, and expand our community reach together. Don't forget to tag someone who would benefit from this post! \n\n#TaggingTips #EngagementStrategies #TaggingTechniques #ExpandYourReach #ConnectAndEngage"
"**How to tag:**\n\n1. Use **relevant** & specific tags to make it easier for others to find & engage with your content.\n\n2. Avoid **over-tagging**, as this can be seen as spammy and make it harder for others to find what they're looking for.\n\n3. Use both **broad & specific** tags to maximize visibility & engagement.\n\n**How to engage:**\n\n1. Be **respectful & professional** in all **interactions** with the other community members.\n\n2. **Read & respond** to others' posts in a timely manner to foster a sense of engagement.\n\n3. Use reactions to **like, comment, bookmark** & show **support** for others' posts. Additionally, one can share their own perspective, ask questions, or offer feedback.\n\nShare their **own content** to contribute to the community & showcase their expertise.\n\n**Call to Action**: Share your favorite tagging tips and strategies in the comments below! Let's connect, engage, and expand our community reach together. Don't forget to tag someone who would benefit from this post! \n\n#TaggingTips #EngagementStrategies #TaggingTechniques #ExpandYourReach #ConnectAndEngage"
"**# How to make an engaging post:**\n\n1. Start with a clear & concise **title** that accurately reflects the content of your post.\n\n2. Begin with an engaging **introduction** to grab the attention of other community members.\n\n3. Use clear & concise **language** to communicate your message effectively.\n\n![COMMUNITY 1|690x332](upload://OwjCkVuplA8wgRkqiMCCGhGGhS.png)\n\n\n4. Include relevant links, images, or other multimedia content to **support your post**.\n\n5. Ensure to paste youtube or other suitable **links** on separate lines.\n\n6. Use **formatting** (headings, bullet points, or bold text) to make your post more readable.\n\n7. End your post with a **CTA - call to action** or a question to encourage engagement from other members\n\n![COMMUNITY|690x333](upload://sIS5WzEjuOlnxMCnRkImF1tb1P9.png)\n\n\n8. To **reply** to a community post, click on the post you want to reply to\n\n9. Scroll to the bottom of the post to view a text box labeled \"**reply**.\"\n\n10. Click on the **text box** to type your response & follow that by clicking the \"reply\" button.\n\n11. Your reply will now be posted as a **comment** on the original post.\n\n12. The community platform has additional features, such as **quoting** the original post or **tagging** other users which can be utilized as well.\n\n![reply|690x331](upload://qaGYKQhplSMcSpcqNiuegJXIWL7.png)\n\n\n\n**Call to Action** : Share your favorite tips for creating engaging posts in the comments below! Let's learn from each other and level up our community game. #EngagingPosts #tips #ContentCreation #AudienceEngagement"
"Loving the energy Deidra, congratulations on your first post :1st_place_medal: as well. And yes, our community will be \"the place\" for all things data engineering."
"Let's gooooo!! It's always nice to have a community where I can ask questions about all things related to data engineering. Looking forward to learning and sharing with everyone!"
"\nWe are delighted to be bringing back a **Tresata** family favorite - our very own community-based, data-centric hackathon celebration - bigger & better than ever after a pandemic-induced hiatus… and we’ll be doing it for the first time ever in **Bengaluru**. \n\nAs is the norm with these events, **hackBLR** aims to create a community of tech and data enthusiasts looking to go beyond the status quo and use their expertise to uplift their communities. It’s a fun, low risk way to put your tech chops to the test, network with fellow data gurus & developers, & show your **data flex**.\n\nThe **HackathonBLR** event is a celebration of innovation, collaboration, and technological advancements. It brings together talented individuals from various backgrounds to showcase their skills, solve real-world challenges, and create impactful solutions. Whether you're a seasoned developer, a creative designer, or a tech enthusiast, this event offers a unique platform to showcase your talent, network with like-minded individuals, and make a difference.\n\n\n\n 🌟 **hackBLR - Brought to you by our Amazing Sponsors!** 🌟 \n![image|477x480](upload://2BQRoO2JmhAczZ0piFjfbJJjd5r.png)\n\n\n\n\nWe would like to extend our heartfelt gratitude to our incredible sponsors who have made the upcoming Hackathon Bengaluru event possible. Their generous contributions and unwavering support have helped us create an exceptional experience for all participants. Let's take a moment to recognize these outstanding sponsors:\n\n 💾 **Terabyte Sponsor: Wissen Technology** 💾 **[Wissen](https://www.wissen.com/)**\n\nWe extend our deepest gratitude to Wissen Technology for their invaluable contribution as our Terabyte Sponsor. Their commitment to fostering technological innovation and excellence is commendable. With their support, we are able to recognize and reward the outstanding efforts of our participants. Thank you, Wissen Technology, for inspiring our participants to reach new heights! Check out their IG handle **[Wissen](https://www.instagram.com/wissentech/)**\n\n3. 🍽️ **Food Sponsor: Khan Saheb** 🍽️ **[Khan saheb](http://khansaheb.co.in/)**\n\nA special thanks to Khan Saheb for coming onboard as our Food Sponsor. Their culinary expertise and delectable offerings will ensure that all participants are fueled and energized throughout the hackathon. Your support in providing quality food experiences is truly appreciated. Thank you, Khan Saheb, for keeping our participants nourished and satisfied! Check out their IG handle **[Khan Saheb](https://www.instagram.com/khansahebrolls/)**\n\n4. 🍪 **Snacks & Desserts Sponsor: Caketime** 🍪 **[Cake Time](https://caketime.co.in/)** \n\nWe would like to express our gratitude to Caketime for their contribution as our Snacks & Desserts Sponsor. Their mouthwatering treats and delightful snacks will keep our participants energized and motivated during intense coding sessions. Thank you, Caketime, for adding a touch of sweetness to our hackathon! Check out their IG handle **[Cake Time](https://www.instagram.com/caketime.co.in/?hl=en)**\n\n5. 🎉**Swag Sponsor: Kreative Banyan** 🎉 **[Kreative Banyan](https://kreative-banyan.business.site/)**\n\nA huge thank you to Kreative Banyan for their support as our Swag Sponsor. Their creative designs and high-quality merchandise will not only make our participants look stylish but also serve as a memorable keepsake from this remarkable event. Thank you, Kreative Banyan, for adding a touch of flair to our hackathon!\n\nWe are incredibly fortunate to have such dedicated sponsors who share our vision for innovation and social impact. Their contributions have made it possible for us to create a transformative experience for all participants. We encourage everyone in our community to support these amazing sponsors and show appreciation for their commitment to driving positive change. Let's come together to celebrate the spirit of collaboration, creativity, and technology at the Hackathon Bengaluru event thanks to the support of our incredible sponsors.\n\n**Check the details below to learn more**\n\n1. **Event** details from [**eventbrite**](https://www.eventbrite.com/e/tresatas-hackblr-mmxxiii-tickets-633252594557) shared for those who have not registered yet. \n\n2. **Communication Tools**: We have a dedicated [slack channel](https://join.slack.com/share/enQtNTM5NDI2MTIxNTIwMS1mYTNhNzY5ZTgyZDZjMGE2ODFhZDdjMDM4YTU3NGUwNjdlZjE4Njk5MmFjODk5MzU4NjQyMmI5OWNkN2E4M2Fl?cdn_fallback=1)  called hack_blr_2023, kindly join in and use reach out to us if you need any support."
"Welcome to our Next Up category, where we offer a glimpse into the exciting events shaping the future of technology. As seasoned hosts of tech events ourselves, we understand the importance of staying ahead in this ever-evolving industry.\n\nStay connected and inspired by exploring the latest trends, innovations, and advancements through our upcoming events. Check our Events Section regularly for updates, registration details, and more.\n\nFor any questions or assistance regarding our events, our team is here to ensure your seamless and enjoyable experience.\n\nJoin us in celebrating the power of technology, collaboration, and innovation. See you at our upcoming events!"
" **Important Announcement :loudspeaker: - All participants in hackBLR MMXIII agree to abide by all event terms and conditions** Kindly read through the below details. \n\n**NO PURCHASE NECESSARY TO ENTER OR WIN. VOID WHERE PROHIBITED. ENTRY IN THIS COMPETITION CONSTITUTES YOUR ACCEPTANCE OF THESE TERMS AND CONDITIONS.**\n\nThe HACKBLR(“Competition”) is a competition provided by Tresata Technologies Pvt. Ltd supported by Wissen Infotech, Cake Time, Creative Banyan & Khan Saheb (“Organizers”) to engage programmers in a creative solutions building competition. For the purposes of these Terms and Conditions, the “Data” means big data gathered from multiple sources.\n\n**1. BINDING AGREEMENT**\n\nIn order to enter the Competition, you must agree to these Terms and Conditions (“Terms”). Please read these Terms carefully to ensure you understand and agree. You agree that submission of an entry or otherwise participating in the Competition (a “Submission”) constitutes agreement to these Terms. You may not participate in the Competition or submit a Submission and are not eligible to receive any prize unless you agree to these Terms. These Terms form a binding legal agreement between you and the Organizer with respect to the Competition.\n\n**2. ELIGIBILITY**\n\nThis Competition is open to all individuals who have agreed to these Terms. The Competition is void where prohibited by law. Organizer reserves the right to relax any of these restrictions if permitted by law before the start of the Competition. All communications between Organizer and competitors will be in English, including but not limited to the Competition website content and email communications. Current employees, contractors, and official office-holders of Organizer, and its subsidiaries, affiliates, and their respective directors, officers, employees, advertising and promotion agencies, representatives, and agents (“Entities”), and members of the Entities’ immediate families (parents, siblings, children, spouses, and life partners of each, regardless of where they live) and members of the Entities’ households (whether related or not) are ineligible to participate in this Competition. Organizer reserves the right to verify eligibility and to adjudicate on any dispute at any time.\n\n**3. CONDITIONS OF PARTICIPATION**\n\nCompetitors must meet the eligibility requirements set forth above. By participating in the Competition, you agree to be bound by these Terms and to all decisions of the Organizer, which are final, binding and conclusive in all matters. Competitors must not cheat; all ideas for Submissions must be original, solely the Competitor’s, and Competitors from different teams may not collaborate on a Submission. All decisions relating to the viability of Submissions, the ranking of Submissions and all other matters pertaining to the Competition are within the sole discretion of the Organizer and shall be final and binding in all respects. As a condition of participating in the Competition and by submitting a Submission, you warrant to Organizer that your Submission:\n\n* Does not include any unsuitable or offensive content, including nudity, sexually explicit, disparaging, libelous or other inappropriate content.\n* Does not include any content that is in violation of or infringes third party intellectual property rights including, but not limited to copyrights, including music copyrights, trademarks, and rights of publicity.\n* Has not been entered in previous contests, or won previous awards, and has not been published or distributed previously in any media.\n* Is suitable for a general audience and does not contain any unsubstantiated, false and/or misleading claims.\n\nAs a condition of participating in the Competition and by submitting a Submission, each Competitor grants the Organizer the right to contact the Competitor and use and publicize his/her Submission as described further below.\n\n**4. LICENSE TO SUBMISSIONS**\n\nEach competitor will retain ownership of and all intellectual and industrial property rights to his or her Submission; provided that, as a condition of participation, Competitors agree that Organizer shall have a perpetual, irrevocable, world-wide, royalty-free, transferable, sublicensable right to use, copy, distribute, modify and make publicly available the Submission in connection with the operation, conduct, administration, and advertising and promotion of the Competition. Competitors also agree that all submitted source code will be made available for anyone to view and download at the end of the Competition. Competitors further grant Organizer a worldwide, royalty-free right to use, copy, and modify all submitted source code to members of the public after the Competition ends. Competitors agree and acknowledge that the downloading and running of any source code from the Competition is undertaken at the sole risk of the downloading competitor and not Organizer or any other competitor. You agree to sign any necessary documentation that may be required by Organizer or its designees to make use of the rights you granted above. You also understand and acknowledge that Organizer and/or other Competitors may have developed or commissioned materials similar or identical to your Submission and you waive any claims you may have resulting from any similarities to your Submission. Nothing in these Terms shall be construed as granting you any right or license under any intellectual property right of Organizer or in Organizer products and services.\n\n**5. NOTIFICATION OF WINNERS; PRIZES**\n\nWinners will be notified on the day of the Final event. Prizes (if any) for Onsite Competitors who attend and compete in the Onsite Final Round shall be determined by the Organizer and announced prior to the start of the Onsite Final Round. All prizes are in kind and there are no monetary prizes involved in this competition. Prizes are awarded without warranty of any kind from the Organizer, express or implied, without limitation may be required to sign and return an Affidavit or Declaration of Eligibility, and Liability/ Publicity Release within 30 days following the date of first attempted notification. When applicable, failure to comply within this time period may result in disqualification and selection of an alternate winner. In completing the Affidavit or Declaration of Eligibility and Liability/Publicity Release, a Competitor who wins a prize (a) confirms his/her eligibility, (b) represents and warrants that he/she has not cheated, (c) verifies the accuracy of the demographic information submitted to Organizer, (d) authorizes Organizer to publicize the results of the Competition, (e) agrees to sign any applicable forms required by tax authorities, (f) grants Organizer a license to all information submitted during the Competition, and (g) releases Organizer from liability arising out of any prize won. Providing false information in the registration process or otherwise to the Organizer in connection with participation in the Competition or in the required forms described in this paragraph will result in a forfeiture of any prize. Acceptance of any prize constitutes permission for, and competitor’s consent to, Organizer and its agencies to contact competitor and use a competitor’s name and/or likeness and Submission for advertising and promotional purposes without compensation, unless prohibited by law. To the extent permitted by law, competitors agree to hold Organizer, its subsidiaries and affiliates and their respective directors, officers, employees and assigns harmless for any injury or damage caused or claimed to be caused by participation in the Competition and/or use or acceptance of any prize won. Organizers will not be liable in the event a typographical or other error occurs in the administration of the Competition or the announcement of the winners.\n\n**6. TAXES**\n\nANY PAYMENTS TO POTENTIAL WINNERS ARE SUBJECT TO THE EXPRESS REQUIREMENT THAT THEY SUBMIT TO ORGANIZER ALL DOCUMENTATION REQUESTED BY ORGANIZER TO PERMIT IT TO COMPLY WITH ALL APPLICABLE STATE, LOCAL AND FOREIGN (INCLUDING PROVINCIAL) TAX REPORTING AND WITHHOLDING REQUIREMENTS.\n\n**7. ELIMINATION; DISQUALIFICATION**\n\nA competitor may be prohibited from participating in this Competition if, in Organizer’s sole discretion, it reasonably believes that the competitor has attempted to undermine the legitimate operation of the Competition by:\n\n* Providing false information concerning his/her identity, postal address, email address or phone number;\n* Breaching or otherwise refusing to comply with any of the provisions set forth in these Terms;\n* Threatening, harassing or interfering with the ability of other competitors to effectively participate in the Competition;\n* Threatening, harassing or interfering with Organizer administrators or other employees; or\n* Communicating or publishing information concerning the solutions to the problems, with other competitors, either directly or indirectly, before the end of the Round.\n\nOrganizer further reserves the right to disqualify any Submission that it believes in its sole and unfettered discretion infringes upon or violates the rights of any third party or otherwise does not comply with these Terms. Claims or concerns about suspicious activity or cheating in any Round must be delivered to the Competition administrator immediately after the Round. If Organizer suspects cheating or violation of the rules from competitors, Organizer will in its sole discretion research all claims and take the appropriate action. All decisions of Organizer in these matters are final and binding. Organizer reserves the right, in its sole discretion, to revoke any and all privileges associated with the Competition, and to take any other action it deems appropriate including but not limited to disqualification of a Submission or terminating or suspending a Competitor’s use of the Organizer Service or the Competition for no reason or any reason whatsoever, including improper use of its website(s) or failure to comply with these Terms or the Statement.\n\n**8. WARRANTY; INDEMNITY**\n\nCompetitors certify that their Submission is original and that they are the sole and exclusive owner and right holder of the Submission and that they have the right to enter the Competition. Each competitor certifies that all parts of his/her code, compilers and other binaries used in the processing of inputs into outputs are either original, or are licensed for use by both Organizer and all competitors in a way that does not: (1) infringe any third party proprietary, intellectual property, industrial property, personal rights or other rights, including without limitation, copyright, trademark, patent, trade secret or confidentiality obligation; or (2) make use of ideas, hints or solutions for the Competition presented, other than by Organizer, for the first time after the start of the Competition in any form, including without limitation from other individuals, competitors, websites, listservs or blogs; or (3) otherwise violate applicable laws and regulations. To the maximum extent permitted by law, each competitor indemnifies and agrees to keep indemnified Organizer at all times from and against any liability, claims, demands, losses, damages, costs and expenses resulting from any act, default or omission of the competitor and/or a breach of any warranty set forth herein. To the maximum extent permitted by law, each competitor agrees to defend, indemnify and hold harmless Organizer from and against any and all claims, actions, suits or proceedings, as well as any and all losses, liabilities, damages, costs and expenses (including reasonable attorneys fees) arising out of or accruing from (a) any entry or other material uploaded or otherwise provided by the competitor that infringes any copyright, trademark, trade secret, trade dress, patent or other intellectual property right of any person or defames any person or violates their rights of publicity or privacy, (b) any misrepresentation made by the competitor in connection with the Competition; (c) any non-compliance by the competitor with these Terms; (d) claims brought by persons or entities other than the parties to these Terms arising from or related to the competitor’s involvement with the Competition; (e) acceptance, possession, misuse or use of any prize or participation in any Competition-related activity or participation in this Competition; (f) any malfunction or other problem with the Competition Site; (g) any error in the collection, processing, or retention of entry information; or (h) any typographical or other error in the printing, offering or announcement of any prize or winners.\n\n**9. TRANSMISSION ERRORS; INTERNET**\n\nOrganizer is not responsible for electronic transmission errors resulting in omission, interruption, deletion, defect, delay in operations or transmission. Organizer is not responsible for theft or destruction or unauthorized access to or alterations of Submission materials, or for technical, network, telephone equipment, electronic, computer, hardware or software malfunctions or limitations of any kind. Organizer is not responsible for inaccurate transmissions of or Organizer’s failure to receive Competitor’s output information on account of technical problems or traffic congestion on the Internet or at any Web site or any combination thereof. If for any reason a portion of the Competition website is not capable of running as planned, including infection by computer virus, bugs, tampering, unauthorized intervention, fraud, technical failures, or any other causes which corrupt or affect the administration, security, fairness, integrity, or proper conduct of this Competition, Organizer reserves the right at its sole discretion to cancel, terminate, modify or suspend the Competition. Organizer reserves the right to select winners from eligible Submissions received as of the termination date. Organizer further reserves the right to disqualify any individual who tampers with the Submission process. Note: Any attempt by a competitor to deliberately damage any website or undermine the legitimate operation of the Competition is a violation of criminal and civil laws and should such an attempt be made, Organizer reserves the right to seek damages from any such competitor to the fullest extent of the law.\n\n**10. PRIVACY**\n\nCompetitors agree that any personal data entered during registration for the Competition or otherwise provided to Organizer in the course of participation in the Competition, including name, mailing address, phone number, birth date and email address, may be used by Organizer to verify competitors’ eligibility and contact competitors and otherwise processed, stored, and otherwise used for the purposes and within the context of the Competition.. By entering, competitors agree to the transmission, processing, sharing and storage of this personal data. Competitors also understand this data may be used by Organizer in order to verify a competitor’s identity, postal address and telephone number in the event a competitor qualifies for a prize. Competitors have the right to access, review, rectify or cancel any personal data held by the Organizer. If a competitor does not provide the data required by the Organizer to register or otherwise participate in the Competition, that competitor’s entry will be ineligible. By accepting a prize, competitor agrees and consents to Organizer and its agencies use of competitor’s name and/or likeness to name the competitor for a reasonable time after completion of the Competition in promotional and advertising material of Organizer (or its agents) as a winner of the Competition without additional compensation, unless prohibited by law.\n\n**11. GENERAL; MISCELLANEOUS**\n\n**11.1 Changes; Termination**\n\nThe Organizer reserves the right to limit the participation of any person in the Competition, amend or interpret these Terms or official communications governing the Competition or cancel the Competition for any reason with prior notice. Notices for any such amendment, interpretation or cancellation will be deemed given by posting on the Competition website and by virtue of a Competitor’s continued participation in the Competition. A Competitor may terminate participation in the Competition upon written notice to Organizer.\n\n**11.2 No Relationship**\n\nNothing in these Terms, nor any entry of a Submission or awarding of a prize shall be construed as an offer or contract of employment with Organizer. You acknowledge that you have submitted your entry voluntarily and not in confidence or in trust. No confidential, fiduciary, agency or other relationship or implied-in-fact contract now exists between you and Organizer, and no such relationship is established by your participation in the Competition or entry of a Submission under these Terms.\n\n**11.3 No Judicial Procedures**\n\nTo the extent permitted by law, the rights to litigate, seek injunctive relief or make any other recourse to judicial or any other procedure in case of disputes or claims resulting from or in connection with the Competition are hereby excluded, and any competitor expressly waives any and all such rights."
"\nWelcome and congratulations to all Hackers on becoming a member of the **Tresata community!** \n\nTresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. \n\nYou will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community.\n\nYou can click on the tresata logo to get to home page from any section. \n\n![image|690x256](upload://yLrimInCaqtRe2McWsH31viwmPd.png)\n\n\nFor all event related contents including hackathon, visit Events category from the home page.\n\n![image|690x227](upload://mouOgxX1BfVKtzQk7FohsPocoCz.png)\n\n\nYou can also [Click Here](https://community.tresata.com/c/hackathon-2023/36) to checkout events category."
"\n1. **Understand the problem statement**: Take the time to thoroughly understand the problem statement provided for the hackathon. Read it multiple times, break it down into smaller components, and clarify any doubts before diving into the solution.\n\n2. **Form a diverse team**: Build a team with members who bring different skills and perspectives to the table. Look for individuals who excel in programming, design, project management, and communication. A diverse team will foster creativity and collaboration.\n\n3. **Plan your time effectively**: Time management is crucial during a hackathon. Break down your project into smaller tasks, set realistic deadlines, and allocate time for testing and iterations. Use project management tools or techniques like Agile or Scrum to stay organized.\n\n4. **Communicate and collaborate**: Maintain open and effective communication within your team. Regularly update each other on progress, challenges, and ideas. Collaborate on problem-solving and decision-making to make the most of your collective expertise.\n\n5. **Focus on user experience:** Keep the end-users in mind while developing your solution. Aim for a user-friendly interface, intuitive design, and seamless functionality. Test your solution with potential users to gather feedback and make improvements.\n\n6. **Leverage available resources**: Make use of the resources provided during the hackathon such as APIs, datasets, and hardware. Explore online documentation and supporting documents to enhance your knowledge.\n\n7. **Think creatively**: Hackathons encourage innovative thinking. Don't be afraid to explore unconventional ideas and solutions. Push the boundaries of what's possible and think outside the box to come up with unique and impactful solutions.\n\n8. **Prioritize functionality over perfection**: While it's important to strive for quality, remember that you have limited time. Focus on delivering a functional prototype that solves the core problem. You can refine and polish your solution later if time permits.\n\n9. **Practice pitching your solution**: Prepare a concise and compelling pitch for your solution. Clearly communicate the problem you're solving, the value proposition, and the unique features of your solution. Practice your pitch to ensure confidence and clarity during the final presentation.\n\n10. **Embrace the learning experience**: Hackathons are not just about winning but also about learning and growing as a professional. Embrace the opportunity to expand your skills, collaborate with others, and gain valuable experience in a high-pressure environment.\n\n#learn #hackathon #Tipsandtricks"
"This is the flagship annual event organized by Databricks - one of the world's leading data & AI companies, whose software platform helps customers unify their analytics across business, data science and engineering. This year the event is being organized at the Moscone Center in San Francisco between June 26-29 2023. \n\nThis summit will provide an opportunity to explore everything - from the latest **innovations & technologies** ....to **thought-provoking panel discussions**....to **networking opportunities** where one can connect with other data professionals in one's industry.....and even more.\n\n**Drop us a line if you’re planning to be there. We’d love to connect!”**\n\n*Complete List of Events* at the Databricks AIO Summit 2023 may be found [here](https://www.databricks.com/dataaisummit/sessions)."
"We are excited to bring you a diverse range of technology-focused events that aim to inspire innovation, foster collaboration and provide valuable learning opportunities. Explore our upcoming events and get ready to embark on a journey of knowledge, creativity, and networking. Let's dive into the exciting subcategories within our Events Section:\n\n1. **HackathonBLR**\n\nJoin us for the highly anticipated **HackathonBLR** event, where talented individuals come together to solve real-world challenges through technology. This hackathon is not only a platform for showcasing your skills but also a chance to make a positive impact on society. Collaborate with like-minded participants, engage with industry experts and create innovative solutions that can shape the future. Stay tuned for updates and registration details.\n\n2. **Databricks AI+Data Summit**\n\nDiscover the power of Artificial Intelligence (AI) at the Databricks AI event. Immerse yourself in the world of machine learning, data analytics, and advanced AI technologies. Learn from renowned experts, explore case studies, and gain insights into the latest trends in AI. Whether you're a data scientist, a developer, or an AI enthusiast, this event will provide valuable knowledge and networking opportunities. Don't miss out on the chance to unlock the potential of AI.\n\n3. **Next Up: A Glimpse into the Future**\n\nGet ready to explore the exciting events that lie ahead in our Next Up category. This section is dedicated to giving you a sneak peek into the future of technology and the upcoming events that will shape our industry. Stay ahead of the curve by staying informed about the latest trends, innovations, and advancements.\n\nWe strive to create exceptional experiences for our community members, and your participation is instrumental in making our events a success. Stay connected, stay inspired, and join us on this exciting journey of technological exploration. Keep an eye on our Events Section for updates, registration details, and more.\n\nIf you have any questions or need assistance regarding our events, feel free to reach out to us.  We are here to ensure your event experience is seamless and enjoyable.\n\nLet's come together to celebrate technology, collaboration, and innovation! See you at our upcoming events."
"For our hackathon events, Tresata makes some of its products available for hackers to use to understand and use their data faster (call it a competitive edge). For this event, participants will be allowed to use TREK - Tresata’s Record Exploration Kit. \n\n# TREK & how to use it\nTREK is Tresata’s Data Inventory Engine. TREK is designed to rapidly profile and inventory as-is data stored in Hadoop across all rows and columns to create an informed view of all valuable enterprise data feeds stored in HDFS. With TREK, you’ll be able to inventory large volumes of data and assess the quality of a data asset.\n\n# Data Quality\nData Analytics is only as valuable as the quality of the data itself. TREK's field statistics, project timelines, and Predictive Data Ontologies are just a few tools produced by TREK to help you determine the value of your data.\n\n# Data Inventory\nTREK's hierarchical structure (projects, partitions, and fields) provides you with a full inventory of all your enterprise data assets in an organised, intuitive UI.\n\n# Navigating through TREK\n\n1. Login Page: You will get Username and password to login to TREK UI, you will be using trek to perform quality check and Inventory check on your data. After logging in on top right side you can check for TREK and click on it to open the TREK UI.\n\n* ![Login Page|690x361](upload://5TaHOZOM2wSLYSbul4Y0RAIRtLe.png)\n\n* ![Navigating to TREK|690x350](upload://yDy92UvB2HBYZI83DAO8hv9rXLH.png)\n\n\n2. Inventory Overview: After you reach on TREK UI, you can search for the dataset and open it.\n\n* ![Inventory Overview|689x346, 100%](upload://8C9pNuLbD4mUsqKpEt08Zl0ZIE3.png)\n\n3. Table Overview: Once you open your desired table for Inventory check you can go through the overview about the table you have selected. You can check what all fields are there and other criteria around percentage populated, number of uniques etc.\n\n* ![Table Overview|690x342](upload://oCBZpKS8eYnnfDl7Yf2xwUN9rz5.png)\n\n4. Field Overview: If you want to deep dive into understanding a specific field and data present in that field you can go for Field level overview. Here you can explore top values, top patterns inside the specific field.\n\n* ![Field Overview|689x345](upload://uvU6oATvrBVSPn7vVJ0xpnJKUT.png)\n\nIf you have any questions comment below."
"## [ **HACKATHONBLR MMXXIII FAQ**](#be-civil)\n\n## [ **WHAT IS A HACKATHON?**](#be-civil)\n\nA hackathon is a social coding event where programmers, designers and developers collaborate to solve a problem and compete for exciting prizes. It’s one part party, one part work-your-butt-off battle against the clock kind of competition.\n\n## [ **WHY WOULD YOU PARTICIPATE IN A HACKATHON?**](#be-civil)\n\nPeople participate in hackathons for lots of reasons: the challenge, the creative outlet, the community collaboration, the networking, the swag…\n\n## [ **WHO IS THIS EVENT FOR?**](#be-civil)\nThis event is open to both participants and spectators from the community who want to cheer them on and see what the hackathon is all about.\n\n\n## [ **IS THE EVENT OPEN TO ALL AGES?**](#be-civil)\nThis event is open for all participants. Wristbands will be provided for both Participants and  Spectators. Spectators of all ages are able to attend the kickoff party and awards ceremony on Saturday.\n\n## [ **WHO CAN PARTICIPATE? ARE THERE ANY PREREQUISITES OR REQUIRED SKILLS?**](#be-civil)\nAnyone is welcome to participate in this event. While experience coding and programming is a huge plus, teams will also need people with strong presentation skills and brilliant ideas.\n\n\n\n\n\n## [**DO I NEED A TEAM** ](#be-civil)\n\nWhile you do have the option of working alone, hackathonBLR is a team event so working with others is encouraged. There are a few ways to find teammates: You can pick your team from people you already know, find a team member via this Community, or at the kickoff party. The maximum number of participants allowed in one team is 2.\n\n ## [ **WILL THERE BE TRAINING?**](#be-civil)\nThere is no pre-event training, but we’ll have Tresata team members on site who will be more than happy to answer your questions throughout the event. There will also be some additional information provided at the pre-event mixer, which we encourage everyone to attend.\n\n ## [ **IS THERE ANYTHING THAT I NEED TO PREP?**](#be-civil)\nOther than finding fellow brilliant minds with complimentary skills to team up with, there is nothing that you need to prepare in advance.\n\n ## [ **HOW MUCH DOES IT COST?**](#be-civil)\nFREE. Free for spectators. Free for participants.\n\n ## [ **WHERE CAN I FIND DIRECTIONS TO THE BUILDING?**](#be-civil)\nOur debut hackathon will be taking place at WeWorks, Embassy Tech Village, Bellandur. Address: BLOCK-L, Embassy TechVillage, Outer Ring Rd, Devarabisanahalli, Bellandur, Bengaluru, Karnataka 560103\n\n## [**WHEN DOES IT START?**](#be-civil)\nThe hackathon will officially begin at 11:00 AM. Contestants should arrive at the venue latest by 10:30 AM to allow enough time for check-in.\n\n## [**WHEN DOES IT END?**](#be-civil)\n The hackathon program is scheduled to conclude at 4:30 PM. This includes the hacking time, presentations, and any closing remarks or announcements. Please note that these timings are subject to change, and any updates or modifications will be communicated to participants through the appropriate channels. Make sure to stay connected and be attentive to any announcements or notifications regarding the schedule.\n\n\n## [ **WHERE DO I PARK?**](#be-civil)\nWeWorks has its own parking basement but it follows first come first served. It's free for the first few and then the Parking lot fees vary but average Rs 250 per day. We strongly recommend taking public transport.\n\n ## [ **WHAT IS THE PROBLEM WE WILL BE HACKING TO SOLVE?**](#be-civil)\nDetails of the business problem will be announced on the day of the event, but we can tell you it relates to making Bengaluru the healthiest city in the world!\n\n ## [ **WHAT ARE THE CATEGORIES OF HACKATHON?**](#be-civil)\n\n**HACK**\nFor Hackathon purists, this category demands hacking, coding and analytical talent where the winners will be judged on the quality of predictions and scalability of code (remember “Big Data Hackathon”) that is able to solve the business problem.\n\n**FREESTYLE**\nWe believe that creativity can be hacked as well!! So this one is for the artists, the vagabonds who can’t code (yet) but still dream about making the world a better place with lateral, “never-before-thought-of” ideas. Deliver a killer-press backed by a solid, scalable, business model that convincingly proves that you can solve the business problem, and you too can win.\n\n## [ **WHERE DO WE DO THE WORK?**](#be-civil)\nYou’ll have almost all of the Ground Floor WeWorks Office (minus a few roped off areas) to work your hacker magic!\n\n## [ **DO I HAVE TO STAY THE WHOLE TIME?**](#be-civil)\nWhile Spectators can plan to just come for the party and/or judging, Participants must stay the entire duration of the event to be eligible to win.\n\n ## [ **WILL THERE BE PRIZES?**](#be-civil)\nAbsolutely. More on it, as we reach the D Day!\n\n## [**WILL THERE BE FOOD?**](#be-civil)\nYes, delicious food and beverages will be provided throughout the event, courtesy of our sponsors.\n\n ## [ **HOW WILL TEAMS BE JUDGED?**](#be-civil)\nYou will have 90 seconds to pitch/present your idea to the Judges.\n\n ## [ **WHO WILL BE JUDGING?**](#be-civil)\nThe judge panel composition will be announced closer to the date of the hackathon.\n\n## [**WHERE DO I SIGN UP?**](#be-civil)\nIn case you missed it, here’s the [link](https://www.eventbrite.com/e/tresatas-hackblr-mmxxiii-tickets-633252594557?utm-campaign=social&utm-content=attendeeshare&utm-medium=discovery&utm-term=listing&utm-source=cp&aff=escb) to register.\n\n![Eventbrite|16x16](upload://sfrO4RlRl6rEvHIxgMNOLKRwo5a.png)Eventbrite\n\n[TRESATA'S HACKBLR MMXXIII|attachment](https://www.eventbrite.com/e/tresatas-hackblr-mmxxiii-tickets-633252594557?utm-campaign=social&utm-content=attendeeshare&utm-medium=discovery&utm-term=listing&utm-source=cp&aff=escb)\n\nHACKBLR is an in-person hackathon that unites developers, hackers, designers, & tech leaders to tackle tough challenges in our own backyard\n\n\n## [**WHERE ?**](#be-civil)\n\nBlock \"L\", Embassy Tech Village, Devarabisanahalli Outer Ring Road, Bellandur, Bengaluru, KA 560103, India\n\n## [**When ?**](#be-civil)\n\nSat, Jun 10, 2023 at 11:00 AM"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Calling all innovators! We want to hear your brilliant ideas and insights. Share your expertise and contribute to the vibrant discussions in our community. Together, let's shape the future of data analytics.  \n\n1. As members we encourage members to contribute ideas, questions & expertise to the community.\n\n2. We can provide clear instructions & guidelines for submitting content or proposals, including any relevant formatting requirements or deadlines.\n\n3. We've established & outlined a process for reviewing & responding to user submissions, through a moderator or a designated committee of community members.\n\n**Call to Action**: We value your ideas and expertise! Submit your content and proposals to Tresata Community and be a part of our collaborative environment. Share your experience with submitting content and let's generate valuable discussions together.\n\nComment below and let us know what's on your mind.\n\n#IdeasSubmission #CommunityCollaboration #ShareYourIdeas #DataAnalytics #CommunityEngagement"
"This appears to be a one stop shop post for everything community related, super useful. :1st_place_medal: :rocket:"
"Thanks Shonan, rightly said on the collaboration & learning from Community.\n\nCongratulations on your first post :boom: & looking to see a lot more coming."
"Thrilled to be a part of this community!! Look forward to engaging with fellow members! I am sure this would be a great platform to connect, collaborate, and learn together!"
"**Introduction**: Our discussion groups are the heart of our community, where you can share ideas, ask questions, and connect with fellow members & thereby engage with each other. We encourage respectful and constructive interactions. Check out the available groups and get involved today! 💡\n\nBest practices include:\n\n1. Creating clear and descriptive group titles & descriptions. \n\n2. Encouraging members to stay on topic & follow **community guidelines**. \n\n3. Designated group **moderators** to manage discussions & resolve conflicts. \n\n**Call To Action:** Find a discussion that resonates with you, read through the existing comments, and reply to at least one member with a thoughtful response or additional insight. Let's keep the conversation going! 🗣️\n\nI have attached a screenshot that makes it a lot more clear. :muscle: \n![Screenshot 2023-06-06 at 4.20.21 PM|690x338](upload://bgscxBY6okfNVvoFYrmYOm4PLN6.png)\n\n\n\n\nHashtags: #CommunityDiscussions #IdeaSharing #EngagementOpportunity"
"**Introduction**: To make it easier for you to find discussions that interest you, we've organized our community into categories. 🗂️ Browse through the categories and participate in conversations that align with your preferences. Take a look below! 👇\n\nOrganize discussions & content within the community making it easier for members to find & participate in discussions that interest them. Best practices include:\n\n1. Under categories we can choose from below whichever are more in tune with Tresata. \n  1a. Submit your idea\n  1b. Discussions\n  1c. General\n  1d. Overview\n  1e. Get started\n   1f. What's new\n  1g. Learn\n  1h. Install\n   1j. Watch Videos \n\n2. Creating clear, descriptive category names & descriptions. \n\n3. Limiting number of categories to avoid overwhelming members. \n\n4. Ensuring categories are organized in a logical way that makes sense to members. \n\n**Call To Action**: Explore a category you haven't visited before, find an interesting discussion, and share your unique perspective or ask a thought-provoking question.\n\nLet's dive deeper into our community's topics! 💬\n\nHashtags: #CommunityCategories #FocusedDiscussions #ExploreTopics"
"**Introduction**: Sometimes, you need to have private conversations within the community. These are useful for sharing sensitive information or having conversations that are not meant for public discussion. 📧 Our messaging feature enables you to connect privately with other members, share sensitive information, and collaborate effectively. Here's how to make the most of it! 🔐\n\n**Messages** are a private way for members to communicate. There is an inbox facility made available with this feature as well. \n\nBest practices include:\n\n1. Encouraging members to use messages sparingly, as they can be time-consuming.\n\n2. Setting guidelines around appropriate use of messages like no spamming, no harassment etc. \n\n3. Ensuring that messages are secure and encrypted to protect member privacy.\n\n**Call To Action:** Identify someone you haven't interacted with yet, send them a friendly message, and ask about their experience or expertise in the community. 📩\n\nHashtags: #PrivateMessages #SecureCommunication #CollaborationTool"
"Thanks for sharing Iniyan, couldn't agree more. \n\nThe fact that you also called out discussions vs personal chats is useful as it leads me to my next post on discussions coming within the next few hours. :timer_clock:\n\n[date-range from=2023-06-06T13:36:00 to=2023-06-06T15:00:00 timezone=\"Asia/Calcutta\"]"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Community personal chats is the ability to have focused discussions and receive personalized attention. In contrast to public forums or group chats, where conversations can quickly become overwhelming or diluted, personal chats enable members to have more in-depth exchanges and develop stronger connections. They provide a safe and confidential environment for individuals to share their thoughts, concerns, and aspirations with others who can relate to their experiences.\n\nUsers could make use of personal chats for private conversation within individuals and groups but when it comes to a discussion I would recommend to create topics and posts on appropriate categories to engage wide range of members to participate, I guess that's the whole purpose we are all here ;) @varun.community.mngr"
"Thanks for sharing this PJ & congratulations on your first post. :1st_place_medal:\n\nLooking forward to some valuable insights and see you collaborate with our wider community in the near future."
"  **Introduction**: Are you interested in focused discussions around specific topics? 📣 Join our channels to connect with like-minded individuals and stay up to date with the latest conversations in your area of interest. Helps organize discussions around a particular topic or theme for keeping conversations focused & making it easier for members to find & participate in discussions that interest them. Check out the available channels and their purposes below! ✨\n  \n   Best practices include:\n  \n  1. Encouraging members to join channels that are relevant to their interests, for our employees we have a couple of channels to explore.\n  \n     1a. **Employees only general**: To be utilized for internal discussion, technical questions & more importantly is **for employees only.**\n    1b. **General**: can be utilized by all members.\n  \n  2. Creating clear & **descriptive channel** names & descriptions.\n  \n  3. **Guidelines** around appropriate use of channels (e.g. no off-topic posts).\n  \n  4. **Stay on topic**: All posts in a channel should be related to the intended purpose. Off-topic posts can be disruptive & make it difficult for members to find relevant information.\n  \n  5. **Respect members & use appropriate language**: Treat members with respect & refrain from making derogatory comments. Do not harass or threaten members. Posts should be written in a clear & concise manner. Avoid using slang, jargon, or acronyms that may not be understood by all members.\n  \n  6. **No self-promotion or solicitation**: Posts in a channel should not be used for self-promotion or solicitation of any kind. This includes advertising, spamming, or direct selling.\n  \n  7. **Keep it professional**: Posts should maintain a professional tone & be appropriate for the channel's audience. Avoid offensive language, sharing inappropriate content, or engaging in inappropriate behavior.\n  \n  8. **Do not share confidential information**: Members should not share confidential/sensitive information in a channel. This includes personal/proprietary information that should not be shared with wider community.\n  \n  9. **Follow platform's terms of service**: All members should comply with platform's terms of service & community guidelines. This includes respecting intellectual property rights, avoiding spamming & refraining from engaging in any other prohibited activities.\n  \n  **Call To Action**: Pick a channel that aligns with your interests, introduce yourself, and share one valuable tip or insight related to the channel's topic. Let's start engaging! 💡\n\nHashtags: #CommunityChannels #TargetedDiscussions #SpecializedTopics"
"Thank you for the warm welcome! I’m excited to be a part of the Tresata community. I look forward to contribute my skills and collaborate with the talented individuals here."
"Thanks Shantanu, equally excited to revisit more engaging well written posts from you. The last post was very interesting and had good engagement. We'd like you to keep them coming :fireworks:"
"Appreciate the warm welcome and excited to contribute further to this thriving community of data enthusiasts. Let's get started!"
"I'd love to know how your experience turned out using this feature?\n\nEager to share inputs & observations if any with **@avi_tresata & @Iniyan.** \n\nTaking this opportunity to give a shout out to the rock stars who make things happen in community. :metal: :rocket:"
"Thanks Mukul, hoping you continue to actively engage & share insights thus keeping the excitement & passion going. :sunglasses:"
"Introduction: Want to have confidential discussions or give feedback privately? 🤫 Our community offers personal chats to facilitate one-on-one conversations with each other for private discussions, feedback & building relationships. Here's how you can make the most of this feature while ensuring privacy and security. 🔒 I have attached a screenshot further below to illustrate this.  \n\n**Guidelines** around appropriate use of personal chats.\n\n1. Ensuring that personal chats are secure and encrypted to protect member privacy.\n\n2. **No spamming**: avoid sending unsolicited messages or advertisements to other users. this includes sending multiple messages that are not relevant to the conversation or repeatedly sending the same message.\n\n3. **No solicitation**: users should not solicit money, products, services from other users without consent.\n\n4. **Respect privacy**: users should respect each other's privacy & not share personal information without consent. this includes not sharing private messages or personal information about other users without their permission. avoid using offensive language or making derogatory comments.\n\n5. **Report inappropriate behavior:** users should report any inappropriate behavior or violation of guidelines to the appropriate authority, such as a moderator or administrator.\n\n6. Encouraging members to use personal chats sparingly, as they can be time-consuming.\n\n\n**Call To Action**: Reach out to someone you admire in the community and start a personal chat to discuss a topic you're both passionate about! 🔥\n\n\nHashtags: #PrivateChats #learn Personalchats #ConfidentialConversations #CommunityPrivacy\n![image (10)|234x500](upload://wec1yos6dLrbHXKV6SUk2GlbtVB.png)"
"Excited to see where we take this platform, been with the people working on this from it’s starting days excited to see new people sharing that passion."
"Thanks for the kind words Naman. \n\nYou were one of the first ones to post on our community & hoping to see you share more valuable insights. Equally thrilled to hear from you. :muscle:"
"I’m thrilled to be a part of this incredible community that promotes knowledge sharing and personal growth. Engaging with like-minded individuals, I look forward to actively participating and offering valuable insights. Together, we can make this community a hub of inspiration and mutual growth."
"Thanks Jubin for sharing your thoughts & congratulations on your first post :1st_place_medal:. \n\nI’m sure you’ll be able to learn about **Data Science & more** from our community. We would also welcome you to share your thoughts & insights from a product perspective in the near future. Feel free to reach out to me if you have any questions."
"Thanks Rakesh for sharing your thoughts & congratulations on your first post :1st_place_medal:. I'm sure you'll be able to learn about **Data Science & more** from our community."
"I am glad to be part of the community and look forward to learning more about data analytics, data science, and more."
"It feels extremely good to be a part of Tresata's community. I look forward to learning more about **DATA** **SCIENCE** and improving my knowledge of how it can be used for business cases."
"Congratulations Noopur on your first post :1st_place_medal:.\n\nI'm glad to hear that you're excited about the Community and the opportunity to improve your knowledge! Our community is indeed a fantastic platform for learning and collaboration. With a diverse group of professionals and experts to assist you on your learning journey, you'll have plenty of resources and support to accelerate your growth. Stay engaged, participate actively in discussions, and make the most of the collaborative environment. \n\nKeep an eye out for new content and discussions within the community as well as the **LEARN** card under categories. There will be plenty of opportunities to learn, share ideas, and engage with fellow members.  If you have any specific questions or topics you'd like to delve into, feel free to ask."
"Sounds exciting! Tresata Community makes me feel like I can really upscale my data science and engineering knowledge! \nLooking forward for more content to learn from with accelerated collaboration!!"
"i tried this setup and it worked without issues. below are my notes... \ni have an existing s3 bucket, in my case its called koert-test.\n\n--- iam ---\ncreate an iam role for the aws service ec2 to use, with a single policy that gives it access to my s3 bucket.\nthe policy for me looks like this:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::koert-test\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::koert-test/*\"\n        }\n    ]\n}\n```\n\n--- ec2 ---\nlaunch an ec2 instance to which i assign the iam role just created. lets see if it worked by ssh'ing into new ec2 instance and accessing s3:\n```\n[ec2-user@ip-192-168-10-218 ~]$ aws s3 ls s3://koert-test/\n                           PRE docs/\n ```\nit works! note i don't remember what that `docs` dir is... some previous experiment. not important.\n\n--- data ---\nnow that we have access to s3 from ec2 i am going to show how to run with all configs and data on s3.\ni will re-use the silly little demo that comes with twig for convenience. so let me start by copying that demo to s3:\n```\n[ec2-user@ip-192-168-10-218 tresata-twig-1.1.0-SNAPSHOT]$ aws s3 cp --recursive demo s3://koert-test/demo/\nupload: demo/config/canonical.yaml to s3://koert-test/demo/config/canonical.yaml\nupload: demo/data/vacuum_transactions.bsv to s3://koert-test/demo/data/vacuum_transactions.bsv\nupload: demo/config/pipeline.yaml to s3://koert-test/demo/config/pipeline.yaml\nupload: demo/config/summary.yaml to s3://koert-test/demo/config/summary.yaml\nupload: demo/data/vacuum_clients.bsv to s3://koert-test/demo/data/vacuum_clients.bsv\nupload: demo/data/crock_transactions.bsv to s3://koert-test/demo/data/crock_transactions.bsv\nupload: demo/data/crock_clients.bsv to s3://koert-test/demo/data/crock_clients.bsv\nupload: demo/config/output.yaml to s3://koert-test/demo/config/output.yaml\nupload: demo/data/mdm_clients.bsv to s3://koert-test/demo/data/mdm_clients.bsv\n```\n\n--- script ---\nlets make a copy of `run-demo.sh` in my home dir as `run-s3.sh` so i can modify it:\n```\n[ec2-user@ip-192-168-10-218 tresata-twig-1.1.0-SNAPSHOT]$ cp bin/run-demo.sh  ~/run-s3.sh\n[ec2-user@ip-192-168-10-218 tresata-twig-1.1.0-SNAPSHOT]$ cd ~ \n[ec2-user@ip-192-168-10-218 ~]$ \n```\ni edit `run-s3.sh` to use configurations on s3, read data from s3 and write results to s3. this is what it looks like:\n```\n#!/bin/bash -e\n\nAPP_DIR=/home/ec2-user/tresata-twig-1.1.0-SNAPSHOT\n\nexport HADOOP_CONF_DIR=${APP_DIR}/conf\nexport ADD_SPARK_OPTS=\"--master local --conf spark.eventLog.enabled=false --conf spark.ui.enabled=false $ADD_SPARK_OPTS\"\n\nDEMO=s3a://koert-test/demo\nOUT=s3a://koert-test/twig-demo-out\nSTATE=$OUT/state\n\n# load\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --source vacuum_clients --input bsvu%${DEMO}/data/vacuum_clients.bsv \n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --source vacuum_transactions --input bsvu%${DEMO}/data/vacuum_transactions.bsv\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --source crock_clients --input bsvu%${DEMO}/data/crock_clients.bsv \n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --source crock_transactions --input bsvu%${DEMO}/data/crock_transactions.bsv \n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --source mdm_clients --input bsvu%${DEMO}/data/mdm_clients.bsv\n\n# resolve\n${APP_DIR}/bin/tres-resolve --state ${STATE} --pipeline ${DEMO}/config/pipeline.yaml --output bsv%${OUT}/prepwithids\n\n# summary\n${APP_DIR}/bin/tres-summary --state ${STATE} --summary ${DEMO}/config/summary.yaml --output bsvu%${OUT}/summary\n\n# output\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --output-config ${DEMO}/config/output.yaml  --summary bsvu%${OUT}/summary --source vacuum_clients --output bsvu%${OUT}/output/vacuum_clients\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --output-config ${DEMO}/config/output.yaml  --summary bsvu%${OUT}/summary --source vacuum_transactions --output bsvu%${OUT}/output/vacuum_transactions\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --output-config ${DEMO}/config/output.yaml  --summary bsvu%${OUT}/summary --source crock_clients --output bsvu%${OUT}/output/crock_clients\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --output-config ${DEMO}/config/output.yaml  --summary bsvu%${OUT}/summary --source crock_transactions --output bsvu%${OUT}/output/crock_transactions\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${DEMO}/config/canonical.yaml --output-config ${DEMO}/config/output.yaml  --summary bsvu%${OUT}/summary --source mdm_clients --output bsvu%${OUT}/output/mdm_clients\n```\nnote that in the script we use `s3a://` urls instead of `s3://` urls. this is because spark uses an opensource library for s3 access that chose `s3a` as the url scheme instead of `s3`.\n\n--- profit ---\nfinally its time to run it!\n```\n[ec2-user@ip-192-168-10-218 ~]$ ./run-s3.sh\n2023-06-03 22:52:54 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2023-06-03 22:52:54 INFO com.tresata.spark.sql.Tool$: creating class com.tresata.matching.spark.job.twig.LoadJob\n2023-06-03 22:52:54 INFO com.tresata.matching.spark.job.twig.LoadJob$: config Config(\n  state = s3://koert-test/twig-demo-out/state,\n  canonical = s3://koert-test/demo/config/canonical.yaml,\n  source = \"vacuum_clients\",\n  input = Arg(value = \"bsvu%s3://koert-test/demo/data/vacuum_clients.bsv\")\n)\n2023-06-03 22:52:54 INFO com.tresata.spark.sql.package$: creating spark session\n2023-06-03 22:52:54 INFO com.tresata.spark.sql.package$: hive support not available\n2023-06-03 22:52:54 INFO com.tresata.spark.sql.package$: enabling delta support\n2023-06-03 22:52:54 INFO com.tresata.spark.sql.package$: setting required spark property spark.serializer: org.apache.spark.serializer.KryoSerializer\n...\n```\n\nif you like you can make a snapshot of this ec2 instance to launch a new instance at any time with same setup."
"Congratulations on your first post & good to see representation from our sales team :saluting_face:\n\n\nWe have a small community building on emoji fans, that apart couldn't agree more that community needs to be a safe and respectful space for all members >> more on that in the next few posts. \n\n\nFurthermore building relationships & getting consistent with content is the larger goal of our community & it's something I'm hoping we can achieve together. :muscle:\n\n\nSo here's to more questions and emojis @SalesGuy4U."
"I personally like to engage with a community that's first and foremost -> safe and respectful towards it's members. \n\nAlso, consistent creation of content  ( you can thank me for the alliterations ! :wink: ) that I am interested in ....AND being recognized for my (useful) contributions will further cement my relationship with that community.\n\nLast, but certainly not the least, I TOTALLY AGREE with @sophia.lang .....I too am a sucker for using emoji's :stuck_out_tongue_closed_eyes: :rofl: and asking questions :see_no_evil:"
"Thanks Sophia, that is exactly what this community needs. More interaction FTW. (emojis never hurt :innocent: too)\n\nAsking questions are a sure shot way to engage and I think it also offers a teaching moment from the author, so that should be a given. Let's keep them coming!"
"I like interacting on posts, especially through using likes and emojis. :star_struck: I also enjoy commenting and asking questions about interesting posts - especially since I don't come from a tech background. I've found that everyone is willing to help further my understanding in topics I do not necessarily know  much information about! :brain:"
"Hello everyone! 👋 Up next we bring you 10 powerful strategies to engage effectively within community and thereby help build it. Whether you're a new member or a passionate existing member, these tips will help you foster connections, network in the community and drive participation. 💪\n\n\n**Call To Action**: Could you share your favorite community-building strategy in the comments below and let's learn from each other! 🌟\n\n\n#CommunityBuilding #EngagementStrategies #CommunityManagement"
"Congratulations are in order for your first post Sophia.  :fireworks:"
"Let's get this started !"
"Let's get this started !"
"Thanks Sophia, as the first to engage and display one of Tresata's early adoption skills already, we'll help you master being a key member. \n\n\nGoing to make a post in the next 10 minutes if you could get some shares and comments and get the TGIF energy going on an uptick!\n\n\n [date-range from=2023-06-02T20:54:00 to=2023-06-02T21:15:00 timezone=\"Asia/Calcutta\"]"
"TGIF - Happy Friday everyone! I'm excited to master being a key member of the community. I learn more and more each day at Tresata and look forward to navigating the community and participating in conversations! :smiley:"
"\nDear Tresata Community Members,\nIt's a friday & I don't intend to make this a TLDR post, fingers crossed. :slight_smile: \n\nI hope this message finds you all in good spirits! As your community manager, it brings me great joy to welcome each and every one of you to the Tresata Community once again. With our shared passion for data science and engineering, I am confident that we will make this an engaging space for knowledge sharing, collaboration, and building strong professional relationships.\n\n\nBefore we kick start this exciting journey together, I'd like to provide you with a comprehensive guide on how to make the most of our community features and functionalities. This will help you navigate through the platform seamlessly and contribute to meaningful discussions. So, let's dive in, shall we!\n\n\nGetting Started on How to Use Tresata Community:\nA comprehensive guide highlighting the various features and functionalities of the Tresata Community. Learn how to navigate and engage within the community with step-by-step instructions and helpful screenshots.\n\n\nTo kick things off, let's explore the how-to's to get you up and running. I will walk you through common tasks such as creating a post, replying to discussions, searching for content, and setting up notifications. We want to ensure that you feel confident and comfortable navigating the community right from the start.\n\n\nCTA: Let's share some of the TGIF energy and get some initial thoughts going here! \n\n<Hint - I'm looking to brand our first two ***Cool CATs*  or community ambassadors of Tresata**>\n\nLet's get commenting..."
"Thanks @koert for the the fix! We will share the new installer with @hunterc later today and inform you."
"this issue has been fixed. \n@saketm who can get @hunterc a fresh twig 1.1.0-SNAPSHOT?"
"i think you found a real bug. we will investigate and get you a new snapshot when fixed.\njust for reference for tresata internally, i create jira ticket CORE-2400 for this."
"If my name field only has Last, MI, First will the parser recognize it's not a full middle name?\nWhen I ran the name parser I got a UDF error: \norg.apache.spark.SparkException: Failed to execute user defined function (FieldsApi$$Lambda$3220/0x000000084143c840: (struct<PatientLast:string>) => struct<PatientLast:struct<firstName:string,firstInitial:string,middleName:string,middleInitial:string,lastName:string,gender:string,generation:string>>)\n\nNot sure if this is my doing in the Canonical, or the name parser not recognizing what's in the field. Especially with the data not being super uniform throughout."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Tresata is a cutting-edge data analytics software that leverages advanced machine learning algorithms for efficient record linkage. This powerful software is designed to tackle the complex task of matching and linking records across multiple datasets.\n\nWith Tresata, organizations can effectively identify and consolidate related records, enabling them to gain valuable insights from disparate data sources. By utilizing machine learning techniques it automates the record linkage process, significantly reducing manual effort and improving accuracy.\n\nThe software employs sophisticated algorithms to analyze various data attributes and determine the likelihood of record matches. It considers factors such as data quality, similarity metrics, and contextual information to make intelligent matching decisions.\n\n**Tresata** allows users to configure and fine-tune the record linkage process according to their specific requirements. It provides comprehensive visualization tools and analytics capabilities, enabling users to explore the linked records and derive meaningful insights\n\n\n# **30 day Free Trial**\n\nWe are pleased to offer a 30-day trial of Tresata. During this trial period, you can experience the capabilities firsthand and witness its impact on your data analysis workflows.\n\nTo get started, simply [Sign Up](https://tresata.com/dune-beta-free-trial/) for the trial on our website. Once you have signed up, you will have access to download and install it on your system. \n\nDon't miss out on this opportunity to experience these transformative capabilities of, sign up for our 30-day trial today and unlock the potential of your data analytics workflows.\n\n# Get Started\n\nWhether you have already signed up or are yet to do so, we highly recommend checking out our post titled [Get Started](https://community.tresata.com/c/get-started/27). This resource provides comprehensive information about the process and guides you through the necessary steps to make the most of your Tresata experience.\n\nIn the \"Get Started\" post, you will find detailed instructions on how to sign up for the trial, download, install and get acquainted with its features. It covers essential aspects such as setting up your account, system requirements, and accessing documentation to help you navigate through the software effortlessly.\n\nhttps://community.tresata.com/t/get-started-with-using-your-data/159\n\nYou can also watch the installation walk through video to get the software installed and running!!\n\nhttps://community.tresata.com/t/installation-walkthrough/182"
"i think the issue is in scrubber transform syntax\ninstead of:\n```\n  - type: scrubber\n    from: PatientLast\n    to: PatientLast\n    scrubber:\n    - type: trimLower\n\n```\ntry (syntax to apply just one scrubber):\n```\n  - type: scrubber\n    from: PatientLast\n    to: PatientLast\n    scrubber:\n      type: trimLower\n\n```\nor (syntax to apply one or more scrubbers):\n```\n   - type: scrubber\n    from: PatientLast\n    to: PatientLast\n    scrubbers:\n    - type: trimLower\n```"
"To me, this is a cool idea. It leaves less room for mistakes and less time consuming than having to potentially copy and paste the scrubber many times for multiple fields. \n\nSyntactically it make sense to me as well. Also wonder if it would also make sense to isolate each field in it's own brackets to signal it's a singleton action? i.e. \ntransforms:\n  - type: scrubber\n    scrubber: trimLower\n    from: [field1], [field2], [field3]\n    to: [field1], [field2], [field3]\n\nNot sure if that syntax could throw off the logic and think it's taking in 3 list-type fields, though."
"its an interesting idea. if the scrubber is used as a transform i dont see why not we could add support for doing multiple in one go. e.g.:\n```\ntransforms:\n  - type: scrubber\n    scrubber: trimLower\n    from: [field1, field2, field2]\n    to: [field1, field2, field3]\n```\nthis would apply scrubber to field1 and store result in field1, apply scrubber to field2 and store result in field2, apply scrubber to field3 and store result in field3.\n\nwhat do we/you think? is this good syntax or does it make it more confusing (more confusing because a scrubber is by nature one-to-one)?"
"note that ms sql server has a weird jdbc syntax including a back slash between server and database which broke our jdbc% source syntax. we fixed this in latest snapshots.\nyou should now be able to do:\n```\n--input jdbc:sqlserver://myserver.database.windows.net\\mydb:myport\n```"
"yes it is available in latest twig 9.1.0-SNAPSHOT\n\nnote, we dont have a 9.1.0 release yet, so we just keep updating the 9.1.0-SNAPSHOT, which means you also get all other changes."
"Hi all,\n\nThought my issue may have been not having a pkey while using Twig version 1.0.0, but still getting an error with a pkey added to the data:\n![Screenshot 2023-05-18 083016|690x43](upload://mBRpPwMd45Y79ax9IKVdcxxNrPe.png)\n![Screenshot 2023-05-18 083127|690x71](upload://fgjH57y5c3EBjdkrZTrclMJplZs.png)\n![Screenshot 2023-05-18 083153|256x500](upload://ywL4vmWnmArCW7u3uonP2onZelW.png)\n![Screenshot 2023-05-18 083205|338x400](upload://tiUk175wf3mTYLtOBIBwMojdvIs.png)"
"Is this functional in Twig 9.1.0?"
"in latest snapshots (9.1.0-SNAPSHOT) twig's output step now requires a yaml config file. this config allow you to select the staging columns to keep and to enrich the output with additional columns from prep and/or summary.\nfor additional info see:\nhttps://doc.tresata.com/9.1.0-SNAPSHOT/tresata-doc-twig-9.1.0-SNAPSHOT.html#_yaml_output_dsl\nhttps://doc.tresata.com/9.1.0-SNAPSHOT/tresata-config-output-schema-9.1.0-SNAPSHOT.html"
"Yeah, that was the error I passed along to him."
"What is the error that you are getting? Is it the one referenced by Mukul above?\n\ni.e. \n```\nException in thread \"main\" com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize value of type `scala.collection.immutable.Seq<com.tresata.scrubber.derive.DeriveGen>` from Object value (token `JsonToken.START_OBJECT`)\n at [Source: (StringReader); line: 18, column: 7] (through reference chain: com.tresata.matching.core.dsl.json.Canonical[\"sources\"]->com.fasterxml.jackson.module.scala.deser.GenericMapFactoryDeserializerResolver$BuilderWrapper[\"carle_patient_main\"]->com.tresata.matching.core.dsl.json.Source[\"transforms\"])\n```"
"If you are matching a parsed name field to a non-parsed name field, I would recommend that you map the attributes to common canonicals, like: \n\n\n```\ncanonical: [firstName]\nsources:\n  <name of source you want to use name parser for>:\n    pkey: pkey\n    transforms:\n    - type: name\n      from: <name of name field you want to parse>\n      to: [name]\n    mapping:\n      firstName:\n        field: name.firstName\n  <name of source you don't want to use name parser for>\n    pkey: pkey\n    mapping:\n      firstName:\n        field: <first name field>\n```\nThen, you can use nameFirst (and so on) in your YAML Config."
"[quote=\"Mukul, post:1, topic:275\"]\n||Parses the name into first-name, first initial, middle name middle initial, last name,gender and genderation\n[/quote]\n\nIf I use a parser, can I simultaneously use those new fields in my Pipeline? \nI.e. Use the Name parser on a field to extract the First, Last, MI and then use the 3 new fields in the pipeline to match on to another table with those 3 fields"
"Sorry for the delay...\n![Screenshot 2023-05-11 114850|220x500](upload://flSJ8ZeaWZQSfs3648tWtI9GQjB.png)\n![Screenshot 2023-05-11 114902|291x500](upload://qGxjvQVHHZiW3HbzAQxtqMfKqpt.png)"
" \n<h2 id=\"_background\">1. Background</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>In tree every level-1/level-1a/level-1b/level-1c link job produces output like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">var\n└── &lt\\;level&gt\\; <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    ├── relations\n    │   └── step=&lt\\;step&gt\\;\n    │       └── substep=&lt\\;substep&gt\\;\n    └── identifiers\n        └── step=&lt\\;step&gt\\;\n            └── source=&lt\\;source&gt\\;</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>&lt\\;level&gt\\; here can be l1, l1a, l1b or l1c</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>After this the relevant roll-up job produces output like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">var\n└── &lt\\;level&gt\\; <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    └── rollUp\n        └── source=&lt\\;source&gt\\;\n            └── table=&lt\\;table&gt\\;</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>&lt\\;level&gt\\; here can be l1, l1a, l1b or l1c</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Each process also has a prep source it reads from for linking and for roll-up. What this prep is exactly varies by job.\nCurrently for level-1 prep is <code>var/prep</code> while for level-1a it is <code>var/l1/rollUp</code>, so the rollUp of level-1 becomes the prep for level-1a.\nNote that contents of identifiers also has slightly different meaning by job. For level-1 identifiers has a mapping of <code>pkey</code> in <code>var/prep</code> to <code>tresataId</code>.\nBut for level-1a the prep was actually <code>var/l1/rollUp</code>, so level-1a identifiers maps a <code>pkey</code> in <code>/var/l1/rollUp</code> (which is actually a level-1 Tresata ID) to <code>tresataId</code> which is now a level-1a Tresata ID.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_configuration\">4. Configuration</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_yaml_canonical_dsl\">4.1. YAML Canonical DSL</h3>\n<div class=\"paragraph\">\n<p>The Canonical Configuration maps fields (columns) in input data into the canonical data model which will be used for record linkage.\nFor each data source it shows which fields from the canonical data model are present in that source, and which fields in the source data they map to.\nSo it&#8217\\;s a mapping from canonical fields to source-specific fields.\nOptionally, for each canonical field per source, scrubbers (data cleaning and/or data filtering functions) can be specified.</p>\n</div>\n<div class=\"paragraph\">\n<p>The YAML format for the Canonical Configuration is illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\ncanonical: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- key1\n- key2\n- other\nsources: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  source1:\n    pkey: id <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    mapping: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      key1:\n        field: id <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      other:\n        field: other\n        scrubbers: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n        - type: regexFilter\n          regex: ^\\w+$\n  source2:\n    pkey: key\n    mappings: <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      left: <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n        key1:\n          field: lfkey1\n        key2:\n          field: lfkey2\n      right:\n        key2:\n          field: rfkey2</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a list of all the fields that make up the canonical data model.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>All sources are mentioned by name in the sources mapping. In this case there are two sources (<code>source1</code> and <code>source2</code>).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>For <code>source1</code> the primary key is called <code>id</code>. Note that <code>pkey</code> is optional. If provided it must be a field for which the values are unique per row for the given source.  If <code>pkey</code> is not provided a primary key is automatically generated by hashing the contents of the row.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The mapping for <code>source1</code> from the canonical fields to the actual field within the data source. Note that not all canonical fields need to be present here.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The actual field in the <code>source1</code> data that the canonical field maps to.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can optionally be provided to clean and/or filter the data. They are applied in order. The full list of scrubbers is outside the scope of this document.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Data source <code>source2</code> has a <code>mappings</code> instead of a <code>mapping</code>, to indicate this data source has multiple mappings per row. This can be the case if a single row has fields for multiple entities (for example a transaction with an originator and beneficiary).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>If <code>mappings</code> is to provide multiple mappings for a source then these mappings needs to be named. Here they are called <code>left</code> and <code>right</code>.</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nNot providing <code>pkey</code> for the sources in the Canonical Configuration is acceptable for a snapshot (one-time) data asset. But if you plan to regularly update the data for your sources and recompute then the results will not be stable unless you provide primary keys.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf <code>pkey</code> is provided for a source then the primary keys must be unique per row for a given source. If they are not unique the software will not detect this and will produce incorrect results.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_canonical_transforms\">4.1.1. Canonical Transforms</h4>\n<div class=\"paragraph\">\n<p>Scrubbers only provide simple string-to-string transformations. To allow for more powerful and generic transformations use <code>transforms</code>, which is available per source. Below is an example of using <code>transforms</code>. This example will be continued in the section <a href=\"#_leveraging_transform_results_in_pipeline\">Leveraging Transform Results in Pipeline</a>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\ncanonical: [name, address, phone]\nsources:\n  default: <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    pkey: pkey\n    transforms: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n    - type: name <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n      from: name\n      to: [name] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    - type: address <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      from: address\n      to: [address] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n    - type: scrubber <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n      from: phone\n      to: phone\n      scrubber:\n        type: phone\n    - type: expression <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      to: name\n      expression: \"join(' ', $name.firstName, $name.lastName)\"\n    mapping:\n      name:\n        field: name\n      address:\n        field: address\n      phone:\n        field: phone</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This example has only one source called <code>default</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Transforms are provided per source just like <code>mapping</code>/<code>mappings</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>The <code>name</code> transform takes a single string and returns a name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The result of name parsing is written as a nested object to the <code>name</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The <code>address</code> transform takes a single string and returns an address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The result of address parsing is written as a nested object to the <code>address</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can be leveraged as transforms as well</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>A simple string manipulation expression can be used using values from existing fields as inputs</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in transforms are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Name parsing from single string to name object. Returns <code>firstName</code>, <code>firstInitial</code>, <code>middleName</code>, <code>middleInitial</code>, <code>lastName</code>, <code>gender</code>, <code>generation</code>. The <code>to</code> fields must be either size 1 for nested result or size 7 for top-level (un-nested) results. The minimum required parsing confidence can be set with <code>threshold</code>. Name parsing also inserts gender (if it can be derived from either title or first name) and generation (if it can be derived from a suffix such as <code>junior</code>).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Address parsing from a single string to an address object. Returns <code>houseNumber</code>, <code>road</code>, <code>level</code>, <code>unit</code>, <code>city</code>, <code>postcode</code>, <code>state</code>, <code>country</code>. Requires a single <code>from</code> field and multiple <code>to</code> fields. The <code>to</code> fields must be either size 1 for nested result or size 8 for top-level (un-nested) results.\ncompanyName: Company name parsing from a single string to a company name object. Returns <code>name</code>, <code>activity</code>, <code>region</code>, <code>structure</code> and <code>legal</code>. The <code>to</code> fields must be either size 1 for nested result or size 4 for top-level (un-nested)results.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncoalesce\n</td>\n<td class=\"hdlist2\">\n<p>Supports simple string coalesce (pick first non-null value). Requires multiple <code>from</code> fields and a single <code>to</code> field.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nscrubber\n</td>\n<td class=\"hdlist2\">\n<p>Re-use a scrubber as a transform. Requires a single <code>from</code> field, a single <code>to</code> field and a <code>scrubber</code> or <code>scrubbers</code>. If <code>scrubbers</code> are provided they are applied in order.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Supports creating a string expression using a small collection of string manipulation functions and references to existing fields.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_string_expressions\">4.1.2. String Expressions</h4>\n<div class=\"paragraph\">\n<p>In the <code>expression</code> transform we support a simple syntax for creating a string expression. Existing fields are referenced using dollar symbol. For example if you have a field <code>middleName</code> you can refer to it by <code>$middleName</code>. String literals can be given as either single quoted (<code>'this is a literal'</code>) or double quoted (<code>\"this is a literal\"</code>).\nSupported functions are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nlower\n</td>\n<td class=\"hdlist2\">\n<p>Lowercase the string. For example: <code>lower($somefield)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nupper\n</td>\n<td class=\"hdlist2\">\n<p>Uppercase the string. For example: <code>upper($somefield)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nconcat\n</td>\n<td class=\"hdlist2\">\n<p>Concatenate multiple strings. For example: <code>concat($a, $b, ' ', $c)</code>. All <code>null</code> inputs are dropped from the concatenation. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\njoin\n</td>\n<td class=\"hdlist2\">\n<p>Same as concat but also requires a delimiter which is provided first. For example using dash (<code>-</code>) as the delimiter: <code>join('-', $areacode, $phonenumber)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncoalesce\n</td>\n<td class=\"hdlist2\">\n<p>Pick the first non-null value. For example: <code>coalesce($a, $b, $c)</code>. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsubstring\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring from a given position (zero based) until a given position (exclusive). For example: <code>substring($a, 2, 4)</code> will extract the third and fourth characters of the string (characters <code>[2, 4)</code> zero based). Negative indices can also be used and are converted into <code>len + 1 - i</code> where <code>len</code> is the length of the input string and <code>i</code> is the negative index. For example <code>substring($a, 0, -2)</code> drops the last character of the string.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntake\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring by taking a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>take($a, 3)</code> takes first 3 characters, and <code>take($a, -4)</code> drops the last 3 characters.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ndrop\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring by dropping a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>drop($a, 3)</code> drops the first 3 characters, and <code>drop($a, -4)</code> keeps the last 3 characters .</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nscrub\n</td>\n<td class=\"hdlist2\">\n<p>Converts an empty string into a <code>null</code>. For example <code>scrub($somefield)</code></p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntrim\n</td>\n<td class=\"hdlist2\">\n<p>Trims white space on left and right. For example <code>scrub(trim($somefield))</code>.</p>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nAll functions that take a single string as input will return <code>null</code> if that input is <code>null</code>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_pipeline_dsl\">4.2. YAML Pipeline DSL</h3>\n<div class=\"paragraph\">\n<p>The pipeline details the steps that make up record linkage.\nEach step captures a particular relationship in your data and can involve one or more sources.\nRecords that participate in a step are either resolved (assigned to an entity), which is final, or they are not, in which case we call them <strong>singletons</strong>, which is not final, and they can still be resolved in a later step.\nBy this mechanism, a source can be resolved in multiple steps: in each later step only the singletons remaining for that source participate.</p>\n</div>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Record Linkage Pipeline Config.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\nsteps:\n- type: resolve <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  sources: [source1, source2] <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  config: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    joins:\n    - field: key1\n    - field: key2\n- type: against <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  sources: [source3]\n  targets: [source1, source2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  config:\n    joins:\n    - field: key1\n    - field: key2\n- type: relaxed <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  pipeline:\n    steps:\n    - type: resolve\n      sources: [source4]\n      config:\n        joins:\n        - field: key3\n    - type: against\n      sources: [source4]\n      targets: [source1]\n      external: true <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      config:\n        joins:\n        - field: key1</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a step of type <code>resolve</code>: all records within this step can link to any other records within the same step.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>For each step you have to specify the sources that participate in it.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Most steps require you to provide a record linkage config. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a> for details.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>This is a step of type <code>against</code>: it resolves some sources against other sources (that have already been resolved in a previous step). These records cannot link to each other.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>When a step is of type <code>against</code> you must provide the sources to resolve against (\"targets\").</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>This is a step of type <code>relaxed</code>, the 3rd and final type for steps. Type <code>relaxed</code> sits somewhere between a <code>resolve</code> and a <code>against</code> step in that it allows both internal linkages and linkages against targets. A full explanation of how <code>relaxed</code> works is outside the scope of this document. Note that <code>relaxed</code> requires you to provide a nested pipeline instead of a config.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Since a <code>relaxed</code> step can have nested <code>against</code> steps that are either internal to the <code>relaxed</code> step or external (meaning they resolve against records in previous steps), the setting <code>external</code> is used to indicate the latter.</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_leveraging_transform_results_in_pipeline\">4.2.1. Leveraging Transform Results in Pipeline</h4>\n<div class=\"paragraph\">\n<p>As mentioned in the <a href=\"#_canonical_transforms\">Canonical Transforms</a> section, more powerful transformations can be applied per source. This is a continuation of that section&#8217\\;s example to show how the results of these transformations can be leveraged in the record linkage pipeline.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: resolve\n  sources: [default]\n  config:\n    weighted:\n      components:\n        - field: name <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n          weight: 1.0\n          type: name\n          parsed: true\n        - field: address <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n          weight: 1.0\n          type: address\n          parsed: true\n        - field: phone\n          weight: 1.0\n          type: same\n        - field: name.gender <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n          weight: -100.0\n          type: same\n        - field: name.generation <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n          weight: -100.0\n          type: same\n        - field: address.country <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n          weight: -1.0\n          type: same\n      threshold: 1.9\n    efc: true</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>Name matcher uses already parsed name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Address matcher uses already parsed address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.gender</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.generation</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Penalty on nested <code>address.country</code> field.</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_relaxed_with_efc\">4.2.2. Relaxed with EFC</h4>\n<div class=\"paragraph\">\n<p>A relaxed nested pipeline supports EFC via a provided <code>breakup</code>.\nGiven the flexibility of relaxed (you can model different relations for different sources) and the power of EFC, it is generally recommended (if possible) to model problems as a single relaxed step with EFC.\nAn example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: relaxed <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  sources: [source1, source2, source3, source4] <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  pipeline: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n    steps:\n    - type: resolve\n      sources: [source1, source2]\n      config:\n        joins:\n        - field: key1\n    - type: resolve\n      sources: [source3]\n      config:\n        joins:\n        - field: key2\n    - type: against <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n      sources: [source3, source4]\n      targets: [source1]\n      config:\n        joins:\n        - field: key3\n        mode: manyToMany <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    breakup: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      weighted:\n        components:\n        - field: key1\n          weight: -1.0\n          type: same\n        - field: key3\n          weight: -1.0\n          type: same</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This is our single relaxed step</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>List all sources here</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Pipeline steps here do not run in order (one after the other). In a relaxed pipeline, the steps are independent of each other and run concurrently. The benefit is that we can model different kind of relationships for different sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The type <code>against</code> here is used to limit the relationships to those between records of <code>source3</code> and <code>source1</code>, or between records of <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Do not limit the relationships formed to only picking the best but allow all relations between <code>source3</code> and <code>source1</code> or <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>EFC is enabled for a relaxed pipeline by providing a <code>breakup</code> config with just penalties. The weights for components don&#8217\\;t matter here as long as they are negative. The penalties will be used for EFC.</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_config_dsl\">4.3. YAML Config DSL</h3>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Config object to express both joining and weighted matching rules.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\njoins: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: field1 <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n- fields: [field2, field3] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n- fields: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  - field4\n  - field5\nweighted: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  components: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - field: field6\n    weight: 2.0\n    type: same\n  - fields: [field7, field8]\n    weight: 1.0\n    type: overlap\n  - field: field9\n    weight: 1.0\n    type: fuzzy\n  - field: field10\n    weight: 1.0\n    type: typos\n  - weight: 2.0 <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    components:\n    - field: field11\n      weight: 1.0\n      type: same\n    - field: field12\n      weight: 1.0\n      type: same\n    threshold: 1.9\n    type: nested\n  - field: field13\n    weight: -1.0 <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    type: same\n  threshold: 2.999 <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\nmode: manyToMany <i class=\"conum\" data-value=\"11\"></i><b>(11)</b>\nefc: true <i class=\"conum\" data-value=\"12\"></i><b>(12)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>joins</code> element is optional. If defined, its value is an array of fields which can be used for joining. The array is in descending order of importance, meaning the first fields for joining that return a result win.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>A join on a single field. Note the key is <code>field</code> not <code>fields</code>. The key <code>field</code> expects a single string as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>A join on multiple fields. Note the key is <code>fields</code> not <code>field</code>. The key <code>fields</code> expects an array of strings as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Another join on multiple fields using an alternative syntax.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The <code>weighted</code> element is optional. If defined, it expresses probabilistic matching and it must contain <code>components</code> and <code>threshold</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>The <code>components</code> element has as its value an array of all possible rules that can contribute to probabilistic matching. Each component has a <code>field</code> or <code>fields</code> to indicate on which fields it operates. The syntax and rules for <code>field</code> or <code>fields</code> are the same as for <code>joins</code>. Each component also has a <code>weight</code> that indicates its relative importance and <code>type</code> to pick how a probabilistic comparison is performed for this component.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>The <code>nested</code> component type here is used to construct an OR rule for probabilistic matching: either one of the nested component rules on field11 and field12 needs to match. If both match that is OK too.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>A negative score means this component functions as a penalty when it does not match, and contributes nothing when it does match.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>The sum of the <code>weights</code> of all elements in the <code>components</code> array that were successful is called the score. If the score is at or above <code>threshold</code>, then it is considered a probabilistic match. If there are multiple probabilistic matches then the one with the highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"11\"></i><b>11</b></td>\n<td>The <code>mode</code> field is optional and only allowed for <code>tres-tik-relations</code>. It restricts how many relations are allowed to be formed from the perspective of the sources and targets. The default value is <code>manyToOne</code> and allowed values are <code>manyToOne</code>, <code>oneToMany</code> and <code>manyToMany</code>. For knowledge graph generation this should probably be set to <code>manyToMany</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"12\"></i><b>12</b></td>\n<td>Turns on Exclusive Facts Clustering (EFC), which is our methodology for keeping entities consistent (where an entity is defined as the collection of records that can reach each other via transitive/multi-hop relations). When EFC is turned on, a penalty that is larger than the matching threshold will automatically be enforced across transitive relations, resulting in entities where no 2 records within the entity would have this penalty when compared to each other. EFC is by default turned off.</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\njoins always win over weighted (probabilistic) matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in weighed (probabilistic) matching are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nexact\n</td>\n<td class=\"hdlist2\">\n<p>Exact comparison of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsame\n</td>\n<td class=\"hdlist2\">\n<p>Lower cases and trims strings before doing an exact comparison. Does not consider <code>null</code> to be the same unless a field is added to <code>nullable</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nfuzzy\n</td>\n<td class=\"hdlist2\">\n<p>Fuzzy string match allowing for a small difference between the strings. The fuzziness tolerance can be set with the numeric <code>threshold</code> (default 0.9).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntypos\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of strings with different sizes and a proportional amount of typos (0 typos for 0 to 3 characters, 1 typo for 4 to 10 characters, 2 typos for 11+ characters)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\noverlap\n</td>\n<td class=\"hdlist2\">\n<p>Treats both sides as sets of string values and considers them a match if the sets have any overlap (ignoring nulls). Will do lower-casing and trimming of strings just like <code>same</code>. If the input is multiple fields it is assumed each field contains string values and the set will be composed from the values from the multiple fields (ignoring nulls). Also supports splitting strings on a provided <code>delimiter</code>. If the input is a single field and <code>delimiter</code> is provided then the single field is assumed to be a string, otherwise the single field is assumed to be an array of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Uses an \"address\" signature field to do address matching. The input is assumed to be an address string that has to be parsed, unless <code>signatures</code> is <code>true</code>, in which case the input is assumed to be address signatures, or unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) parsed address object. Also supports a boolean <code>withoutUnit</code> switch to ignore unit numbers in address matching and a boolean <code>justCityOrZip</code> to match on just the city or zip/postal.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnested\n</td>\n<td class=\"hdlist2\">\n<p>Uses nested probabilistic rules in <code>components</code> and a <code>threshold</code> to set the minimum score of the nested components for this rule to be considered a probabilistic match. Note that the score of the <code>components</code> and the <code>threshold</code> are purely internal to this rule and only used to determine if this rule is considered a probabilistic match. The contribution of this rule to the overall score is solely set by its <code>weight</code>, just like any other rule. Nested can be used to construct AND or OR like constructs.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of people. Requires full names, so first and last. Other elements like middle name, middle initial, title and suffix are also supported. The input is assumed to be a name string to be parsed where <code>threshold</code> controls minimum confidence of parsing, unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) already-parsed name object. The parameters <code>nFirst</code> and <code>nLast</code> can be used to only match on <code>nFirst</code> letters of first name and <code>nLast</code> letters of last name. Also supports parsing of less usual name formats (like last, first) if <code>freeform</code> is <code>true</code>, in which case the input has to be a string (but this can lead to more false positives).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncompanyName\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of companies. Will remove legal (ltd., company, etc.) or structure (holding, enterprise, etc.) related elements before comparing. Also supports an <code>aggressive</code> switch to remove more from company name such as generic corporate activities (consulting, investments, operations, etc.) before performing the comparison.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_summary_dsl\">4.4. YAML Summary DSL</h3>\n<div class=\"paragraph\">\n<p>The Summary Configuration details the rules to pick the best values for canonical fields from all available values.\nThe rules are score-based, and the value with the best score wins.</p>\n</div>\n<div class=\"paragraph\">\n<p>The Summary Configuration is expressed in a simple YAML format.\nAn extensive example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\npicks: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: other <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  score: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    type: expression <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    value: other\n- fields: [key1, key2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  scores: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - type: prefer <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    field: source\n    values: [source1, source2, source3]\n  - type: expression\n    value: -date\n- field: key3\n  score:\n    type: count <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    field: key3\n- field: key4\n  score:\n    type: rank <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\n    field: key1\n    order: [a, b, c]</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>picks</code> field gives an array of rules to pick best values for canonical fields</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Each rule has to provide the canonical field (or fields) for which a best value is to be picked</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The score is used to pick the best value for a canonical field (or fields).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>A Spark SQL expression can be used to score. The highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>This is an example of multiple canonical fields being scored together. This can ensure consistency (e.g. address, city and zip should probably be picked together).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Multiple scores can be provided instead of a single score. The scores are applied in order. If the first score does not provide a winner the second score is used as a tie breaker, etc.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Type <code>prefer</code> simply means to prefer certain values in the given field (often used to favor one or more sources for records). It scores the preferred sources higher than any other source, but does not pick between the preferred sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>Type <code>count</code> uses an approximate value count to pick the most frequent value. In this example the most frequent value for <code>key3</code> is picked.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>Type <code>rank</code> uses a ranking where values in the array are ranked in order of preference (most preferred first). It assigns the same score to any values not present in the ranking. It is similar to the prefer type, but establishes an order between the preferred values.</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for score <code>type</code> are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\ncount\n</td>\n<td class=\"hdlist2\">\n<p>Pick based on the most frequent value. An approximate counting algorithm is used for this. Requires <code>field</code> for the canonical field to count and base the pick on (this field can be different from the canonical field we are actually picking for).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nrank\n</td>\n<td class=\"hdlist2\">\n<p>Rank values in order of preference. Requires <code>field</code> for the canonical field we are ranking and <code>order</code> with the array of values in order of preference (most preferred first).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nprefer\n</td>\n<td class=\"hdlist2\">\n<p>Prefer values from the array of preferred values above others. Requires <code>field</code> for the canonical field we are expressing a preference for and <code>values</code> with the array of preferred values.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Use a general Spark SQL Expression to score (highest wins). Requires <code>value</code> to provide the expression which can use any canonical fields (columns).</p>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2022-2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-04-25 21:25:09 +0530\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
" \n<h2 id=\"_invocation\">3. Invocation</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_typical\">3.1. Typical</h3>\n<div class=\"paragraph\">\n<p>A typical run of Twig could look like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-load \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --state myProjectDir \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --canonical myCanonical.yaml \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --source source1 \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --input bsvu%data/inputs/source1 <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\ntres-load \\ <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --input bsvu%data/inputs/source2\ntres-resolve \\ <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  --state myProjectDir \\\n  --pipeline myPipeline.yaml <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n  --output bsvu%data/outputs/canonical <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\ntres-output \\ <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source1 \\\n  --output bsvu%data/outputs/source1 <i class=\"conum\" data-value=\"11\"></i><b>(11)</b>\ntres-output \\ <i class=\"conum\" data-value=\"12\"></i><b>(12)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --output bsvu%data/outputs/source2\ntres-summary \\ <i class=\"conum\" data-value=\"13\"></i><b>(13)</b>\n  --state myProjectDir \\\n  --summary mySummary.yaml \\ <i class=\"conum\" data-value=\"14\"></i><b>(14)</b>\n  --output bsvu%data/summary</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td><code>tres-load</code> is used to load data into an application of Twig. It takes care of data cleaning and parsing, the mapping of columns into the Twig canonical data model, and the conversion into data formats used internally by Twig. It is assumed that more complex data transformations are done as needed before loading data into Twig.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Twig maintains state across commands. All of its state is maintained inside this directory. This directory should be unique to a particular application of Twig and not be shared between applications.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>This configuration file in YAML format describes the mappings of columns for each source into the canonical data model that is used internally by Twig, and any cleaning or other transformations needed to get the data ready for record linkage. See the section <a href=\"#_yaml_canonical_dsl\">YAML Canonical DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Name for this source. If the same source is loaded twice the second time it overwrites the data for this source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The actual location to load data from in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format. The data must have a unique primary key per row.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>A second invocation of <code>tres-load</code> for a different source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td><code>tres-resolve</code> runs record linkage (also known as entity resolution). This can be a multi-step process (a record linkage pipeline) that captures the specific relations between data sources and restricts unwanted linkages. Twig does not support hierarchical record linkage (parent-child relationships) at this point.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>The record linkage process is expressed as steps in a pipeline using a configuration file in YAML format. Deterministic and probabilistic record linkage are supported. See the section <a href=\"#_yaml_pipeline_dsl\">YAML Pipeline DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>The output of <code>tres-resolve</code> is data in the canonical data model with the results of record linkage in a column called <code>tresataId</code>. Each Tresata ID represents a unique entity and as such it presents a simple join key to find all records for an entity.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td><code>tres-output</code> produces the output per source after record linkage completes. The output is the same as the input but with new columns added called Tresata IDs that show the result of the record linkage. Each Tresata ID represents a unique entity. Note that <code>tres-output</code> expects the original input data for the source (as it was provided to <code>tres-load</code> using the <code>--input</code> flag) to still be present and unchanged as it will read it again.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"11\"></i><b>11</b></td>\n<td>The actual location to write the data to in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"12\"></i><b>12</b></td>\n<td>A second invocation of <code>tres-output</code> for the second source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"13\"></i><b>13</b></td>\n<td><code>tres-summary</code> provides summaries (also known as best values or golden records) per entity (and so per Tresata ID). This is rule-based, allowing you to specify for each canonical field what value to pick using a scoring mechanism.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"14\"></i><b>14</b></td>\n<td>The summary process is configured using scoring rules in YAML format. See the section <a href=\"#_yaml_summary_dsl\">YAML Summary DSL</a>.</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_houdini\">3.2. Houdini</h3>\n<div class=\"paragraph\">\n<p>If Twig is to be used for Houdini knowledge graph relations generation, then a typical run could look like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source1 \\\n  --input bsvu%data/inputs/source1\ntres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --input bsvu%data/inputs/source2\ntres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source3 \\\n  --input bsvu%data/inputs/source3\ntres-relations \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --state myProjectDir \\\n  --config myConfig.yaml \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --sources source3 \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --targets source1,source2 \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --output bsv%data/relations <i class=\"conum\" data-value=\"5\"></i><b>(5)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td><code>tres-relations</code> generates houdini relations (possible/soft edges) between records (entities).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This configuration file in YAML format descibes how the relations should be established. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>The sources (comma-separated) for which we are establishing relations.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The sources (comma-separated) for which we are establishing relations against. This argument is optional. If it is not specified, the relations are generated internally within <code>sources</code>. If <code>targets</code> is specified, also pay attention to the <code>mode</code> argument in the config. The mode is expressed from <code>sources</code> to <code>targets</code>. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The actual location to write the data to in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format.</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"matching-process\">2. The Matching Process</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The crux of Twig&#8217\\;s entity matching lies in the construction of a graph representing the records from the input tables, and its resolution into clusters that represent singular entities.\nEach table that is processed by Twig connects itself to this graph so that these clusters can each be mapped to a single unique and universal Tresata ID.\nThere are two variations in how we allow tables to attach themselves to the graph: Level-1 and Level-2.\nA source can only participate in either level-1 or level-2 matching.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nIn Twig, only level-1 matching is supported.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The process of matching can be divided into two separate stages:\n(1) the creation of <strong>edges</strong> that link records together in the graph, representing an equivalence relation in identity, and\n(2) the resolution of the resulting graph into clusters that determine how we create <strong>identifiers</strong>, or Tresata IDs.\nLevel-1 and level-2 matching differ slightly in their approach, but both accomplish the resolution of entities through these two stages.</p>\n</div>\n<div class=\"paragraph\">\n<p>In a matching pipeline for level-1, each step describes either an <em>internal</em> or <em>targeted</em> matching step.\nAn <strong>internal</strong> matching step allows for any two records in a set of sources to establish a relation, or edge, between them given the rules in a matching config.\nA <strong>targeted</strong> matching step allows for any record in a set of sources to establish at most one edge to a record in a set of (already resolved) target sources using the rules in a matching config.\nA level-1 source can participate (be resolved) in a combination of internal and targeted matching steps inside a level-1 pipeline.\nLevel-2 sources can only participate in one targeted matching step inside a level-2 pipeline.</p>\n</div>\n<div class=\"paragraph\">\n<p>In level-1 matching, a graph is built from a set of inputs that are designated as being level-1 sources.\nAny two records from these sources that satisfy the matching criteria described in the matching pipeline form an <em>edge</em> in the graph.\nThe resulting graph is divided into clusters and these clusters form <em>identifier</em> groups that receive a unique Tresata ID.\nOnce all the level-1 sources are joined together in this graph, the graph is \"locked\" throughout level-2 matching.\nIn particular, the subgraphs that form the identifier clusters are fully resolved\\; they can no longer become further connected or modified in any way.\nStated differently, two nodes of the level-1 graph are path-connected after level-2 matching if and only if they were path-connected at the conclusion of level-1 matching.</p>\n</div>\n<div class=\"paragraph\">\n<p>Level-2 matching is a process that acts on this \"locked\" level-1 graph.\nTables that undergo level-2 matching are allowed to form up to only one edge per node (record) to connect themselves to the level-1 graph.\nThe effect of this limitation is that level-2 matching does not allow for identifier groups to change.\nBy effectively only adding <strong>leaf nodes</strong> to the graph, the subgraphs that make up each identifier group do not form new connections to each other, and thus remain fixed across the level-1 graph.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA record can form edges to multiple other records in level-1 matching, but a record can form only one edge in level-2 matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The justification for two levels of matching has its basis in data quality.\nIn TREE, Level-1 matching is typically reserved for reference data, where the combination of all level-1 data constitutes the entirety of the entities that get resolved.\nHowever, if tables of lesser quality and more loosely populated fields are introduced, it is possible (and often the case) that in order to effectively attach its records to other level-1 data, we establish edges too aggressively and inadvertently connect two separate clusters belonging to different entities together, resolving them to the same entity.\nBy restricting the number of edges formed to one in level-2, we prevent records from altering the connective structure of the entity groups.\nLevel-2 matching&#8217\\;s robustness against polluting the graph&#8217\\;s entities allows for looser matching criteria: by lowering the threshold for establishing an edge, a record is more likely to form a connection to another record, and we only choose the edge with the highest score.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is important to specify a table as level-1 if at all possible.\nThe graph produced by level-1 matching establishes the set of possible Tresata IDs: if an entity is not represented by at least one level-1 record, it will not receive a Tresata ID, and without at least one level-1 record to serve as an anchor for attaching level-2 records to the graph, level-2 records that represent this entity will not get resolved to a Tresata ID.\n<em>A table must be specified as level-1 in order for level-2 tables to form edges to its records</em>.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf an entity is not represented by any records from level-1 sources, it will not receive a Tresata ID.\nIt is important to include enough reference data in level-1 to establish the entities we wish to resolve.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>It is worth noting that establishing a single edge per record (\"targeted matching\") is not what distinguishes level-2 matching&#8212\\;&#8203\\;we allow this behavior in level-1 matching as well, on a source-by-source basis.\nWe restrict level-2 by enforcing that we only allow one edge per record, but in level-1 we can also specify that a source only produces single edges per record in the matching pipeline.\nWhat distinguishes level-1 from level-2 is that it <em>creates the graph that establishes the Tresata IDs</em>.\nOnce level-1 matching is complete, level-2 matching works by attaching records to the level-1 graph, absorbing the Tresata ID of the subgraph to which it connects.\nA table that matches using level-2 does not affect level-2 matching for other tables\\; every instance of level-2 matching operates independently by connecting records only to level-1 records.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the initial graph consisting of edges between level-1 records is created, it is resolved into subgraphs that form Tresata IDs using a <strong>graph clustering</strong> algorithm that combines <strong>graph cuts</strong> with <strong>connected components</strong>.\nAfter graph clustering finishes, any two nodes that are connected by a path of edges, i.e. belong to the same connected component, are resolved to the same entity.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the level-1 graph is resolved into single-entity subgraphs, a Tresata ID is chosen to represent each group.\nThe Tresata ID is chosen by selecting a representative record (by default the record with the canonical primary key that is first in lexicographic order among the records in the subgraph) and by creating a hashed representation of this record&#8217\\;s primary key.\nThis results in a consistently chosen identifier that persists through reruns of Twig.\nHowever, any time a level-1 table is added or modified and matching is re-run, the Tresata IDs (as well as the entity groups themselves) are subject to change.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nAdding or modifying a level-1 table between runs of Twig can potentially alter the Tresata IDs.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>All level-1 records receive a Tresata ID, including those that do not form any connections to other records (which we refer to as <strong>singletons</strong>).\nTables that participate in level-2 matching assign records the Tresata ID of the level-1 record with which they form a connection (when multiple records satisfy the matching criteria, we choose the edge with the highest score).\nLevel-2 records that do not form any connections are not assigned a Tresata ID, and are referred to as <strong>orphan</strong> records.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"purpose\">1. Purpose of Twig and Record Linkage</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The aim of Twig is to provide a highly effective and automated approach to the <a href=\"https://en.wikipedia.org/wiki/Record_linkage\">Record Linkage</a> problem.\nTwig can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources.\nEntity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.</p>\n</div>\n<div class=\"paragraph\">\n<p>Twig contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"endpoints\">2. API endpoints</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Dune supports basic filesystem exploration and extraction of file schemas.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"ls\">2.1. ls</h3>\n<div class=\"paragraph\">\n<p>The <code>/ls</code> endpoint returns a non-recursive file and directory listing for the given path.\nThe <code>path</code> parameter is required and specifies the directory from which to extract a list of contents.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=hdfs:///path/to/directory\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:21:29 GMT\n\n{\n    \"files\": [\n        {\n            \"isDir\": true,\n            \"path\": \"hdfs://dev/path/to/directory/dir1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file2\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file3\"\n        }\n    ]\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=invalid/path\nHTTP/1.1 404 Not Found\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:24:49 GMT\n\npath: invalid/path does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=file:/restricted/path\nHTTP/1.1 403 Forbidden\nContent-Length: 44\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:31:05 GMT\n\naccess denied for path file:/restricted/path</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"schema\">2.2. schema</h3>\n<div class=\"paragraph\">\n<p>The <code>/schema</code> endpoint returns a JSON detailing the schema extracted from the given path using the given format.\nDune initializes a spark session that it uses to extract the schema.\nPaths using the local filesystem are not allowed.\nThe format can be any format prefix supported by Tresata Source (e.g., <code>csv</code>, <code>parq</code>, <code>delta</code>, <code>es</code>, etc.)</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=bsv\"\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:47:29 GMT\n\n{\n    \"fields\": [\n        {\n            \"metadata\": {},\n            \"name\": \"a\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        },\n        {\n            \"metadata\": {},\n            \"name\": \"b\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        }\n    ],\n    \"type\": \"struct\"\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=invalid&amp\\;format=bsv\"\nHTTP/1.1 404 Not Found\nContent-Length: 27\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:50:25 GMT\n\npath invalid does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=restricted&amp\\;format=bsv\"\nHTTP/1.1 403 Forbidden\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:19:01 GMT\n\naccess denied for path restricted</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an unsupported format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=xyz\"\nHTTP/1.1 400 Bad Request\nContent-Length: 22\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:28:49 GMT\n\nunsupported format xyz</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an incorrect format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/bsv&amp\\;format=parq\"\nHTTP/1.1 400 Bad Request\nContent-Length: 32\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:38:46 GMT\n\ninvalid format parq for some/bsv</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-04-23 23:05:09 -0400\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
" \n<h2 id=\"running-service\">1. Running the Dune Service</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Tresata can be run with authentication enabled or disabled (useful for debugging purposes).\nThe service is started by running the <code>tres-dune-api-service</code> script in the bin directory.\nThe arguments vary slightly for running in secure vs. insecure mode.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Secure mode</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-dune-api-service\n  --host &lt\\;string&gt\\; \\\n  --port &lt\\;int&gt\\; \\\n  [--threads &lt\\;int&gt\\;] \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --public-key $PUBLIC_KEY \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  [--name-claim &lt\\;string&gt\\;] \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  [--group-claim &lt\\;string&gt\\;] \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --auth-group &lt\\;string&gt\\; <i class=\"conum\" data-value=\"5\"></i><b>(5)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>number of threads to create for dune service (default 10)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>public key used for verifying signatures</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>field to extract username from in jwt payload (default \"cognito:username\")</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>field to extract group from in jwt payload (default \"custom:group\")</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>group claim to admit for authorization (requests not from this group will return a 403 Forbidden response)</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Insecure mode</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-dune-api-service\n  --host &lt\\;string&gt\\; \\\n  --port &lt\\;int&gt\\; \\\n  [--threads &lt\\;int&gt\\;] \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --public-key $PUBLIC_KEY \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --no-auth \\\n  --auth-group &lt\\;string&gt\\; \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --dummy-group &lt\\;string&gt\\; <i class=\"conum\" data-value=\"4\"></i><b>(4)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>number of threads to create for dune service (default 10)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>public key used for verifying signatures (the value will be ignored in insecure mode)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>group claim to admit for authorization</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>group claim to insert for all requests (this should match the value for <code>--auth-group</code>)</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>When running Tresata in secure mode, each request must be both authenticated (encrypted with the signer&#8217\\;s private key) and authorized (the group claim inside the request&#8217\\;s JWT token must match the value of the <code>--auth-group</code> argument).</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_source_code\">4. Source Code</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_git\">4.1. Git</h3>\n<div class=\"paragraph\">\n<p>Source code is maintained in git repositories. We use in-house \nGitolite over SSH to host the source code. Only SSH keys are used for \nauthentication. Gitolite ACLs are used for authorization. Authorization \nis configured so that:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Git repos never allow history rewrites (so no forced commits or rebases). This is to prevent any loss of source code.</p>\n</li>\n<li>\n<p>A select group of people has access to every repo, which gives them \nfull read access and the ability to create and update (but not delete) \nfeature branches.</p>\n</li>\n<li>\n<p>Only the maintainer of a master or release branch has the ability to commit to this branch</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_branching\">4.2. Branching</h3>\n<div class=\"paragraph\">\n<p>Development for the current release is done on the git master branch.\n For all previous minor releases (according to semantic versioning) we \nkeep separate branches to which patches (backports or hotfixes) are \napplied. The master branch and release branches have designated \nmaintainers. They are the only ones to commit to these branches, and \nthis is enforced with ACLs. When its time to do a new release a release \nbranch is created from the master branch and a release maintainer is \nchosen to maintain it.</p>\n</div>\n<div class=\"paragraph\">\n<p>The master branch maintainer is responsible for producing continuous \nintegration (snapshot/nighly) artifacts while release maintainers are \nresponsible for producing release artifacts. The master branch is \nchecked out by the branch maintainer at least once a week and fully \ntested and build so that CI artifacts (snapshots) are published to the \nartifact server.</p>\n</div>\n<div class=\"paragraph\">\n<p>Any significant work is done in feature branches based off master \nbranch, which are reviewed by the master branch maintainer before \ngetting squash merged (so single commit) into master branch. Feature \nbranches are reviewed using in-house hosted <a href=\"https://reviewboard.tresata.com/\">reviewboard</a>.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_planning\">3. Planning</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>All features, tasks, bugs and backports are in <a href=\"https://tresata.atlassian.net/projects/CORE/issues?filter=allissues\">JIRA</a> hosted online by Atlassian. A new release is planned by assigning JIRA tickets to it.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_release_cycle\">2. Release Cycle</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_releases\">2.1. Releases</h3>\n<div class=\"paragraph\">\n<p>The typical release cycle for core is about 4 months. This means that\n new versions of tree, trek, tools etc. are released about every 4 \nmonths, and these all have the same dependencies: they depend on the \nsame versions of tresata libraries, and run on the same spark version, \netc.\nEvery core release has a version number, which ties together all the \nreleased tresata libraries and applications. The mapping of core release\n version to the versions of tresata libraries and applications and open \nsource dependencies is in <a href=\"https://server02.tresata.com/versions.properties\">versions.properties</a>. For example it shows <code>tresata-trek.r8.4.version=4.4.0</code>.\n This should be interpreted as that when core release 8.4 was released \nthe version of trek in that core release was 4.4.0. And if you want to \nknow what version of spark trek 4.4.0 runs on you have to find \nspark.r8.4.version, which should give you <code>spark.r8.4.version=3.0.1-tres1</code>, so the supported spark version is 3.0.1-tres1 (and thats true for all applications in core 8.4 that require spark).</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_snapshots\">2.2. Snapshots</h3>\n<div class=\"paragraph\">\n<p>While core is being developed we also release snapshots, also known \nas nightly builds. These are unstable versions of our libraries and \napplications that get updated, changed and overwritten frequently \nwithout notice. These are by default deployed on server01 and server03. \nThe current snapshots can be found in versions.properties as well: they \ndo not have a core version number. So for example in versions.properties\n you might find <code>tresata-trek.version=4.5.0-SNAPSHOT</code>, this \nindicates the current snapshot (or nightly build) of trek is \n4.5.0-SNAPSHOT. Snapshots are easy to spot because the versions always \nend in SNAPSHOT and this is also reflected in the files you download.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tbody><tr>\n<td class=\"icon\">\n<i class=\"fa icon-important\" title=\"Important\"></i>\n</td>\n<td class=\"content\">\nSnapshots give you access to the latest and greatest, but should never be used for anything critical or production.\n</td>\n</tr>\n</tbody></table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tbody><tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nSnapshots are never acceptable for anything that gets shipped to a client.\n</td>\n</tr>\n</tbody></table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_libraries_and_applications\">1. Libraries and Applications</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>All back-end software is developed, tested and released together. We \ncall this collection of software core. Core software consists of \nlibraries and applications.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tbody><tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA subset of the core libraries is called the Tresata Spark SDK and made available to select clients.\n</td>\n</tr>\n</tbody></table>\n</div>\n<div class=\"paragraph\">\n<p>Libraries are released as a <a href=\"https://en.wikipedia.org/wiki/JAR_(file_format)\">jar</a> file with their dependencies specified in a <a href=\"https://maven.apache.org/guides/introduction/introduction-to-the-pom.html\">pom</a>\n file and their documentation in a javadoc-jar file. For example \ntresata-scala release 3.4.0 is a library for which the jar, pom and \njavadoc-jar can be found in artifactory <a href=\"https://server02.tresata.com:8084/artifactory/list/libs-release-local/com/tresata/tresata-scala_2.12/3.4.0\">here</a>.\n Notice that the official name of the library is tresata-scala_2.12: the\n 2.12 refers to the scala version. Normally you should not use these \nlibraries directly: SBT takes care of downloading these for you when you\n specify a dependency on the tresata-scala library in the build.sbt of \nyour project. <a href=\"https://en.wikipedia.org/wiki/Javadoc\">Javadocs</a> (or really <a href=\"https://docs.scala-lang.org/overviews/scaladoc/for-library-authors.html\">scaladocs</a>) are also unpacked automatically <a href=\"https://server02.tresata.com/javadoc/\">here</a>, so for example the scaladocs for tresata-scala release 3.4.0 can be found <a href=\"https://server02.tresata.com/javadoc/tresata-scala_2.12/3.4.0\">here</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Applications are basically jars with bash scripts to launch the \napplication and configuration files to configure it, all wrapped up \ntogether in a tarball (tar.gz file) that unpacks to a neat directory \nstructure. We only make the tarball available, not the jar file, since \nnobody should depend on an application like a library. For example \ntresata-trek release 4.4.0 tarball and html and pdf documentation can be\n found in artifactory <a href=\"https://server02.tresata.com:8084/artifactory/list/libs-release-local/com/tresata/tresata-trek/4.4.0/\">here</a>. The pom file is also published but serves no purpose and should be ignored.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_infrastructure\">9. Infrastructure</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The installation and configuration of all infrastructure is done \nautomatically using Chef. This includes Jenkins (CI server), Artifactory\n (artifacts repository), Gitolite (source code hosting), Reviewboard \n(code review) and SBT (build tool).</p>\n</div>\n<div class=\"paragraph\">\n<p>Copyright © 2012 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2021-04-16 16:53:29 -0400\n</div>\n</div>\n<link rel=\"stylesheet\" href=\"SDLC_files/prettify.min.css\">\n<script src=\"SDLC_files/run_prettify.min.js\"></script>"
" \n<h2 id=\"_build_process\">8. Build Process</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Our primary tool for building software (compiling source code, \ntesting and publishing artifacts) is SBT. SBT does not allow one to \npublish artifacts without running unit or integration tests.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_backups\">7. Backups</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>We have automated daily backups of all source code (git repositories)\n to Amazon S3, and automated monthly backups of Artifactory artifacts to\n Amazon S3. All backups are also encrypted using gpg public-private key \nencryption.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_artifacts\">6. Artifacts</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>We use an in-house hosted <a href=\"https://server02.tresata.com:8084/\">Artifactory</a>\n as our artifact repository. Authentication is done using passwords. For\n authorization Artifactory ACLs are used. We do not produce source code \nartifacts so Artifactory only hosts binaries and documentation.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_continuous_integration\">5. Continuous Integration</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Any changes on master branch are detected by an in-house hosted <a href=\"https://server04.tresata.com:8083/\">Jenkins</a>\n CI server which runs compilation and tests. If compilation or unit \ntests or integration tests fail, an email alert is sent. Note that \njenkins currently does not produce the CI (snapshot) artifacts.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"endpoints\">2. API endpoints</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Dune supports basic filesystem exploration and extraction of file schemas.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"ls\">2.1. ls</h3>\n<div class=\"paragraph\">\n<p>The <code>/ls</code> endpoint returns a non-recursive file and directory listing for the given path.\nThe <code>path</code> parameter is required and specifies the directory from which to extract a list of contents.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=hdfs:///path/to/directory\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:21:29 GMT\n\n{\n    \"files\": [\n        {\n            \"isDir\": true,\n            \"path\": \"hdfs://dev/path/to/directory/dir1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file2\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file3\"\n        }\n    ]\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=invalid/path\nHTTP/1.1 404 Not Found\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:24:49 GMT\n\npath: invalid/path does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=file:/restricted/path\nHTTP/1.1 403 Forbidden\nContent-Length: 44\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:31:05 GMT\n\naccess denied for path file:/restricted/path</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"schema\">2.2. schema</h3>\n<div class=\"paragraph\">\n<p>The <code>/schema</code> endpoint returns a JSON detailing the schema extracted from the given path using the given format.\nDune initializes a spark session that it uses to extract the schema.\nPaths using the local filesystem are not allowed.\nThe format can be any format prefix supported by Tresata Source (e.g., <code>csv</code>, <code>parq</code>, <code>delta</code>, <code>es</code>, etc.)</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=bsv\"\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:47:29 GMT\n\n{\n    \"fields\": [\n        {\n            \"metadata\": {},\n            \"name\": \"a\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        },\n        {\n            \"metadata\": {},\n            \"name\": \"b\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        }\n    ],\n    \"type\": \"struct\"\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=invalid&amp\\;format=bsv\"\nHTTP/1.1 404 Not Found\nContent-Length: 27\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:50:25 GMT\n\npath invalid does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=restricted&amp\\;format=bsv\"\nHTTP/1.1 403 Forbidden\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:19:01 GMT\n\naccess denied for path restricted</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an unsupported format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=xyz\"\nHTTP/1.1 400 Bad Request\nContent-Length: 22\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:28:49 GMT\n\nunsupported format xyz</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an incorrect format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/bsv&amp\\;format=parq\"\nHTTP/1.1 400 Bad Request\nContent-Length: 32\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:38:46 GMT\n\ninvalid format parq for some/bsv</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-04-23 23:05:09 -0400\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
" \n<h2 id=\"running-service\">1. Running the Tresata Service</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p> Tresata can be run with authentication enabled or disabled (useful for debugging purposes).\nThe service is started by running the <code>tres-dune-api-service</code> script in the bin directory.\nThe arguments vary slightly for running in secure vs. insecure mode.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Secure mode</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-dune-api-service\n  --host &lt\\;string&gt\\; \\\n  --port &lt\\;int&gt\\; \\\n  [--threads &lt\\;int&gt\\;] \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --public-key $PUBLIC_KEY \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  [--name-claim &lt\\;string&gt\\;] \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  [--group-claim &lt\\;string&gt\\;] \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --auth-group &lt\\;string&gt\\; <i class=\"conum\" data-value=\"5\"></i><b>(5)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>number of threads to create for Tresata service (default 10)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>public key used for verifying signatures</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>field to extract username from in jwt payload (default \"cognito:username\")</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>field to extract group from in jwt payload (default \"custom:group\")</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>group claim to admit for authorization (requests not from this group will return a 403 Forbidden response)</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Insecure mode</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-dune-api-service\n  --host &lt\\;string&gt\\; \\\n  --port &lt\\;int&gt\\; \\\n  [--threads &lt\\;int&gt\\;] \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --public-key $PUBLIC_KEY \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --no-auth \\\n  --auth-group &lt\\;string&gt\\; \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --dummy-group &lt\\;string&gt\\; <i class=\"conum\" data-value=\"4\"></i><b>(4)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>number of threads to create for dune service (default 10)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>public key used for verifying signatures (the value will be ignored in insecure mode)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>group claim to admit for authorization</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>group claim to insert for all requests (this should match the value for <code>--auth-group</code>)</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>When running Tresata in secure mode, each request must be both authenticated (encrypted with the signer&#8217\\;s private key) and authorized (the group claim inside the request&#8217\\;s JWT token must match the value of the <code>--auth-group</code> argument).</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"matching-process\">2. The Matching Process</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The crux of Twig&#8217\\;s entity matching lies in the construction of a graph representing the records from the input tables, and its resolution into clusters that represent singular entities.\nEach table that is processed by Twig connects itself to this graph so that these clusters can each be mapped to a single unique and universal Tresata ID.\nThere are two variations in how we allow tables to attach themselves to the graph: Level-1 and Level-2.\nA source can only participate in either level-1 or level-2 matching.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nIn Twig, only level-1 matching is supported.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The process of matching can be divided into two separate stages:\n(1) the creation of <strong>edges</strong> that link records together in the graph, representing an equivalence relation in identity, and\n(2) the resolution of the resulting graph into clusters that determine how we create <strong>identifiers</strong>, or Tresata IDs.\nLevel-1 and level-2 matching differ slightly in their approach, but both accomplish the resolution of entities through these two stages.</p>\n</div>\n<div class=\"paragraph\">\n<p>In a matching pipeline for level-1, each step describes either an <em>internal</em> or <em>targeted</em> matching step.\nAn <strong>internal</strong> matching step allows for any two records in a set of sources to establish a relation, or edge, between them given the rules in a matching config.\nA <strong>targeted</strong> matching step allows for any record in a set of sources to establish at most one edge to a record in a set of (already resolved) target sources using the rules in a matching config.\nA level-1 source can participate (be resolved) in a combination of internal and targeted matching steps inside a level-1 pipeline.\nLevel-2 sources can only participate in one targeted matching step inside a level-2 pipeline.</p>\n</div>\n<div class=\"paragraph\">\n<p>In level-1 matching, a graph is built from a set of inputs that are designated as being level-1 sources.\nAny two records from these sources that satisfy the matching criteria described in the matching pipeline form an <em>edge</em> in the graph.\nThe resulting graph is divided into clusters and these clusters form <em>identifier</em> groups that receive a unique Tresata ID.\nOnce all the level-1 sources are joined together in this graph, the graph is \"locked\" throughout level-2 matching.\nIn particular, the subgraphs that form the identifier clusters are fully resolved\\; they can no longer become further connected or modified in any way.\nStated differently, two nodes of the level-1 graph are path-connected after level-2 matching if and only if they were path-connected at the conclusion of level-1 matching.</p>\n</div>\n<div class=\"paragraph\">\n<p>Level-2 matching is a process that acts on this \"locked\" level-1 graph.\nTables that undergo level-2 matching are allowed to form up to only one edge per node (record) to connect themselves to the level-1 graph.\nThe effect of this limitation is that level-2 matching does not allow for identifier groups to change.\nBy effectively only adding <strong>leaf nodes</strong> to the graph, the subgraphs that make up each identifier group do not form new connections to each other, and thus remain fixed across the level-1 graph.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA record can form edges to multiple other records in level-1 matching, but a record can form only one edge in level-2 matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The justification for two levels of matching has its basis in data quality.\nIn TREE, Level-1 matching is typically reserved for reference data, where the combination of all level-1 data constitutes the entirety of the entities that get resolved.\nHowever, if tables of lesser quality and more loosely populated fields are introduced, it is possible (and often the case) that in order to effectively attach its records to other level-1 data, we establish edges too aggressively and inadvertently connect two separate clusters belonging to different entities together, resolving them to the same entity.\nBy restricting the number of edges formed to one in level-2, we prevent records from altering the connective structure of the entity groups.\nLevel-2 matching&#8217\\;s robustness against polluting the graph&#8217\\;s entities allows for looser matching criteria: by lowering the threshold for establishing an edge, a record is more likely to form a connection to another record, and we only choose the edge with the highest score.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is important to specify a table as level-1 if at all possible.\nThe graph produced by level-1 matching establishes the set of possible Tresata IDs: if an entity is not represented by at least one level-1 record, it will not receive a Tresata ID, and without at least one level-1 record to serve as an anchor for attaching level-2 records to the graph, level-2 records that represent this entity will not get resolved to a Tresata ID.\n<em>A table must be specified as level-1 in order for level-2 tables to form edges to its records</em>.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf an entity is not represented by any records from level-1 sources, it will not receive a Tresata ID.\nIt is important to include enough reference data in level-1 to establish the entities we wish to resolve.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>It is worth noting that establishing a single edge per record (\"targeted matching\") is not what distinguishes level-2 matching&#8212\\;&#8203\\;we allow this behavior in level-1 matching as well, on a source-by-source basis.\nWe restrict level-2 by enforcing that we only allow one edge per record, but in level-1 we can also specify that a source only produces single edges per record in the matching pipeline.\nWhat distinguishes level-1 from level-2 is that it <em>creates the graph that establishes the Tresata IDs</em>.\nOnce level-1 matching is complete, level-2 matching works by attaching records to the level-1 graph, absorbing the Tresata ID of the subgraph to which it connects.\nA table that matches using level-2 does not affect level-2 matching for other tables\\; every instance of level-2 matching operates independently by connecting records only to level-1 records.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the initial graph consisting of edges between level-1 records is created, it is resolved into subgraphs that form Tresata IDs using a <strong>graph clustering</strong> algorithm that combines <strong>graph cuts</strong> with <strong>connected components</strong>.\nAfter graph clustering finishes, any two nodes that are connected by a path of edges, i.e. belong to the same connected component, are resolved to the same entity.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the level-1 graph is resolved into single-entity subgraphs, a Tresata ID is chosen to represent each group.\nThe Tresata ID is chosen by selecting a representative record (by default the record with the canonical primary key that is first in lexicographic order among the records in the subgraph) and by creating a hashed representation of this record&#8217\\;s primary key.\nThis results in a consistently chosen identifier that persists through reruns of Twig.\nHowever, any time a level-1 table is added or modified and matching is re-run, the Tresata IDs (as well as the entity groups themselves) are subject to change.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nAdding or modifying a level-1 table between runs of Twig can potentially alter the Tresata IDs.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>All level-1 records receive a Tresata ID, including those that do not form any connections to other records (which we refer to as <strong>singletons</strong>).\nTables that participate in level-2 matching assign records the Tresata ID of the level-1 record with which they form a connection (when multiple records satisfy the matching criteria, we choose the edge with the highest score).\nLevel-2 records that do not form any connections are not assigned a Tresata ID, and are referred to as <strong>orphan</strong> records.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"purpose\">1. Purpose of Twig and Record Linkage</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The aim of Twig is to provide a highly effective and automated approach to the <a href=\"https://en.wikipedia.org/wiki/Record_linkage\">Record Linkage</a> problem.\nTwig can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources.\nEntity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.</p>\n</div>\n<div class=\"paragraph\">\n<p>Twig contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"_configuration\">4. Configuration</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_yaml_canonical_dsl\">4.1. YAML Canonical DSL</h3>\n<div class=\"paragraph\">\n<p>The Canonical Configuration maps fields (columns) in input data into the canonical data model which will be used for record linkage.\nFor each data source it shows which fields from the canonical data model are present in that source, and which fields in the source data they map to.\nSo it&#8217\\;s a mapping from canonical fields to source-specific fields.\nOptionally, for each canonical field per source, scrubbers (data cleaning and/or data filtering functions) can be specified.</p>\n</div>\n<div class=\"paragraph\">\n<p>The YAML format for the Canonical Configuration is illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\ncanonical: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- key1\n- key2\n- other\nsources: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  source1:\n    pkey: id <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    mapping: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      key1:\n        field: id <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      other:\n        field: other\n        scrubbers: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n        - type: regexFilter\n          regex: ^\\w+$\n  source2:\n    pkey: key\n    mappings: <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      left: <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n        key1:\n          field: lfkey1\n        key2:\n          field: lfkey2\n      right:\n        key2:\n          field: rfkey2</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a list of all the fields that make up the canonical data model.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>All sources are mentioned by name in the sources mapping. In this case there are two sources (<code>source1</code> and <code>source2</code>).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>For <code>source1</code> the primary key is called <code>id</code>. Note that <code>pkey</code> is optional. If provided it must be a field for which the values are unique per row for the given source.  If <code>pkey</code> is not provided a primary key is automatically generated by hashing the contents of the row.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The mapping for <code>source1</code> from the canonical fields to the actual field within the data source. Note that not all canonical fields need to be present here.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The actual field in the <code>source1</code> data that the canonical field maps to.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can optionally be provided to clean and/or filter the data. They are applied in order. The full list of scrubbers is outside the scope of this document.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Data source <code>source2</code> has a <code>mappings</code> instead of a <code>mapping</code>, to indicate this data source has multiple mappings per row. This can be the case if a single row has fields for multiple entities (for example a transaction with an originator and beneficiary).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>If <code>mappings</code> is to provide multiple mappings for a source then these mappings needs to be named. Here they are called <code>left</code> and <code>right</code>.</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nNot providing <code>pkey</code> for the sources in the Canonical Configuration is acceptable for a snapshot (one-time) data asset. But if you plan to regularly update the data for your sources and recompute then the results will not be stable unless you provide primary keys.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf <code>pkey</code> is provided for a source then the primary keys must be unique per row for a given source. If they are not unique the software will not detect this and will produce incorrect results.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_canonical_transforms\">4.1.1. Canonical Transforms</h4>\n<div class=\"paragraph\">\n<p>Scrubbers only provide simple string-to-string transformations. To allow for more powerful and generic transformations use <code>transforms</code>, which is available per source. Below is an example of using <code>transforms</code>. This example will be continued in the section <a href=\"#_leveraging_transform_results_in_pipeline\">Leveraging Transform Results in Pipeline</a>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\ncanonical: [name, address, phone]\nsources:\n  default: <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    pkey: pkey\n    transforms: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n    - type: name <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n      from: name\n      to: [name] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    - type: address <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      from: address\n      to: [address] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n    - type: scrubber <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n      from: phone\n      to: phone\n      scrubber:\n        type: phone\n    - type: expression <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      to: name\n      expression: \"join(' ', $name.firstName, $name.lastName)\"\n    mapping:\n      name:\n        field: name\n      address:\n        field: address\n      phone:\n        field: phone</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This example has only one source called <code>default</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Transforms are provided per source just like <code>mapping</code>/<code>mappings</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>The <code>name</code> transform takes a single string and returns a name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The result of name parsing is written as a nested object to the <code>name</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The <code>address</code> transform takes a single string and returns an address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The result of address parsing is written as a nested object to the <code>address</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can be leveraged as transforms as well</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>A simple string manipulation expression can be used using values from existing fields as inputs</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in transforms are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Name parsing from single string to name object. Returns <code>firstName</code>, <code>firstInitial</code>, <code>middleName</code>, <code>middleInitial</code>, <code>lastName</code>, <code>gender</code>, <code>generation</code>. The <code>to</code> fields must be either size 1 for nested result or size 7 for top-level (un-nested) results. The minimum required parsing confidence can be set with <code>threshold</code>. Name parsing also inserts gender (if it can be derived from either title or first name) and generation (if it can be derived from a suffix such as <code>junior</code>).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Address parsing from a single string to an address object. Returns <code>houseNumber</code>, <code>road</code>, <code>level</code>, <code>unit</code>, <code>city</code>, <code>postcode</code>, <code>state</code>, <code>country</code>. Requires a single <code>from</code> field and multiple <code>to</code> fields. The <code>to</code> fields must be either size 1 for nested result or size 8 for top-level (un-nested) results.\ncompanyName: Company name parsing from a single string to a company name object. Returns <code>name</code>, <code>activity</code>, <code>region</code>, <code>structure</code> and <code>legal</code>. The <code>to</code> fields must be either size 1 for nested result or size 4 for top-level (un-nested)results.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncoalesce\n</td>\n<td class=\"hdlist2\">\n<p>Supports simple string coalesce (pick first non-null value). Requires multiple <code>from</code> fields and a single <code>to</code> field.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nscrubber\n</td>\n<td class=\"hdlist2\">\n<p>Re-use a scrubber as a transform. Requires a single <code>from</code> field, a single <code>to</code> field and a <code>scrubber</code> or <code>scrubbers</code>. If <code>scrubbers</code> are provided they are applied in order.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Supports creating a string expression using a small collection of string manipulation functions and references to existing fields.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_string_expressions\">4.1.2. String Expressions</h4>\n<div class=\"paragraph\">\n<p>In the <code>expression</code> transform we support a simple syntax for creating a string expression. Existing fields are referenced using dollar symbol. For example if you have a field <code>middleName</code> you can refer to it by <code>$middleName</code>. String literals can be given as either single quoted (<code>'this is a literal'</code>) or double quoted (<code>\"this is a literal\"</code>).\nSupported functions are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nlower\n</td>\n<td class=\"hdlist2\">\n<p>Lowercase the string. For example: <code>lower($somefield)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nupper\n</td>\n<td class=\"hdlist2\">\n<p>Uppercase the string. For example: <code>upper($somefield)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nconcat\n</td>\n<td class=\"hdlist2\">\n<p>Concatenate multiple strings. For example: <code>concat($a, $b, ' ', $c)</code>. All <code>null</code> inputs are dropped from the concatenation. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\njoin\n</td>\n<td class=\"hdlist2\">\n<p>Same as concat but also requires a delimiter which is provided first. For example using dash (<code>-</code>) as the delimiter: <code>join('-', $areacode, $phonenumber)</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncoalesce\n</td>\n<td class=\"hdlist2\">\n<p>Pick the first non-null value. For example: <code>coalesce($a, $b, $c)</code>. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsubstring\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring from a given position (zero based) until a given position (exclusive). For example: <code>substring($a, 2, 4)</code> will extract the third and fourth characters of the string (characters <code>[2, 4)</code> zero based). Negative indices can also be used and are converted into <code>len + 1 - i</code> where <code>len</code> is the length of the input string and <code>i</code> is the negative index. For example <code>substring($a, 0, -2)</code> drops the last character of the string.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntake\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring by taking a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>take($a, 3)</code> takes first 3 characters, and <code>take($a, -4)</code> drops the last 3 characters.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ndrop\n</td>\n<td class=\"hdlist2\">\n<p>Take a substring by dropping a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>drop($a, 3)</code> drops the first 3 characters, and <code>drop($a, -4)</code> keeps the last 3 characters .</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nscrub\n</td>\n<td class=\"hdlist2\">\n<p>Converts an empty string into a <code>null</code>. For example <code>scrub($somefield)</code></p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntrim\n</td>\n<td class=\"hdlist2\">\n<p>Trims white space on left and right. For example <code>scrub(trim($somefield))</code>.</p>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nAll functions that take a single string as input will return <code>null</code> if that input is <code>null</code>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_pipeline_dsl\">4.2. YAML Pipeline DSL</h3>\n<div class=\"paragraph\">\n<p>The pipeline details the steps that make up record linkage.\nEach step captures a particular relationship in your data and can involve one or more sources.\nRecords that participate in a step are either resolved (assigned to an entity), which is final, or they are not, in which case we call them <strong>singletons</strong>, which is not final, and they can still be resolved in a later step.\nBy this mechanism, a source can be resolved in multiple steps: in each later step only the singletons remaining for that source participate.</p>\n</div>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Record Linkage Pipeline Config.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\nsteps:\n- type: resolve <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  sources: [source1, source2] <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  config: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    joins:\n    - field: key1\n    - field: key2\n- type: against <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  sources: [source3]\n  targets: [source1, source2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  config:\n    joins:\n    - field: key1\n    - field: key2\n- type: relaxed <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  pipeline:\n    steps:\n    - type: resolve\n      sources: [source4]\n      config:\n        joins:\n        - field: key3\n    - type: against\n      sources: [source4]\n      targets: [source1]\n      external: true <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      config:\n        joins:\n        - field: key1</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a step of type <code>resolve</code>: all records within this step can link to any other records within the same step.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>For each step you have to specify the sources that participate in it.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Most steps require you to provide a record linkage config. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a> for details.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>This is a step of type <code>against</code>: it resolves some sources against other sources (that have already been resolved in a previous step). These records cannot link to each other.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>When a step is of type <code>against</code> you must provide the sources to resolve against (\"targets\").</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>This is a step of type <code>relaxed</code>, the 3rd and final type for steps. Type <code>relaxed</code> sits somewhere between a <code>resolve</code> and a <code>against</code> step in that it allows both internal linkages and linkages against targets. A full explanation of how <code>relaxed</code> works is outside the scope of this document. Note that <code>relaxed</code> requires you to provide a nested pipeline instead of a config.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Since a <code>relaxed</code> step can have nested <code>against</code> steps that are either internal to the <code>relaxed</code> step or external (meaning they resolve against records in previous steps), the setting <code>external</code> is used to indicate the latter.</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_leveraging_transform_results_in_pipeline\">4.2.1. Leveraging Transform Results in Pipeline</h4>\n<div class=\"paragraph\">\n<p>As mentioned in the <a href=\"#_canonical_transforms\">Canonical Transforms</a> section, more powerful transformations can be applied per source. This is a continuation of that section&#8217\\;s example to show how the results of these transformations can be leveraged in the record linkage pipeline.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: resolve\n  sources: [default]\n  config:\n    weighted:\n      components:\n        - field: name <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n          weight: 1.0\n          type: name\n          parsed: true\n        - field: address <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n          weight: 1.0\n          type: address\n          parsed: true\n        - field: phone\n          weight: 1.0\n          type: same\n        - field: name.gender <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n          weight: -100.0\n          type: same\n        - field: name.generation <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n          weight: -100.0\n          type: same\n        - field: address.country <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n          weight: -1.0\n          type: same\n      threshold: 1.9\n    efc: true</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>Name matcher uses already parsed name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Address matcher uses already parsed address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.gender</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.generation</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Penalty on nested <code>address.country</code> field.</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_relaxed_with_efc\">4.2.2. Relaxed with EFC</h4>\n<div class=\"paragraph\">\n<p>A relaxed nested pipeline supports EFC via a provided <code>breakup</code>.\nGiven the flexibility of relaxed (you can model different relations for different sources) and the power of EFC, it is generally recommended (if possible) to model problems as a single relaxed step with EFC.\nAn example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: relaxed <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  sources: [source1, source2, source3, source4] <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  pipeline: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n    steps:\n    - type: resolve\n      sources: [source1, source2]\n      config:\n        joins:\n        - field: key1\n    - type: resolve\n      sources: [source3]\n      config:\n        joins:\n        - field: key2\n    - type: against <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n      sources: [source3, source4]\n      targets: [source1]\n      config:\n        joins:\n        - field: key3\n        mode: manyToMany <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    breakup: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      weighted:\n        components:\n        - field: key1\n          weight: -1.0\n          type: same\n        - field: key3\n          weight: -1.0\n          type: same</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This is our single relaxed step</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>List all sources here</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Pipeline steps here do not run in order (one after the other). In a relaxed pipeline, the steps are independent of each other and run concurrently. The benefit is that we can model different kind of relationships for different sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The type <code>against</code> here is used to limit the relationships to those between records of <code>source3</code> and <code>source1</code>, or between records of <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Do not limit the relationships formed to only picking the best but allow all relations between <code>source3</code> and <code>source1</code> or <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>EFC is enabled for a relaxed pipeline by providing a <code>breakup</code> config with just penalties. The weights for components don&#8217\\;t matter here as long as they are negative. The penalties will be used for EFC.</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_config_dsl\">4.3. YAML Config DSL</h3>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Config object to express both joining and weighted matching rules.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\njoins: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: field1 <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n- fields: [field2, field3] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n- fields: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  - field4\n  - field5\nweighted: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  components: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - field: field6\n    weight: 2.0\n    type: same\n  - fields: [field7, field8]\n    weight: 1.0\n    type: overlap\n  - field: field9\n    weight: 1.0\n    type: fuzzy\n  - field: field10\n    weight: 1.0\n    type: typos\n  - weight: 2.0 <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    components:\n    - field: field11\n      weight: 1.0\n      type: same\n    - field: field12\n      weight: 1.0\n      type: same\n    threshold: 1.9\n    type: nested\n  - field: field13\n    weight: -1.0 <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    type: same\n  threshold: 2.999 <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\nmode: manyToMany <i class=\"conum\" data-value=\"11\"></i><b>(11)</b>\nefc: true <i class=\"conum\" data-value=\"12\"></i><b>(12)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>joins</code> element is optional. If defined, its value is an array of fields which can be used for joining. The array is in descending order of importance, meaning the first fields for joining that return a result win.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>A join on a single field. Note the key is <code>field</code> not <code>fields</code>. The key <code>field</code> expects a single string as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>A join on multiple fields. Note the key is <code>fields</code> not <code>field</code>. The key <code>fields</code> expects an array of strings as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Another join on multiple fields using an alternative syntax.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The <code>weighted</code> element is optional. If defined, it expresses probabilistic matching and it must contain <code>components</code> and <code>threshold</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>The <code>components</code> element has as its value an array of all possible rules that can contribute to probabilistic matching. Each component has a <code>field</code> or <code>fields</code> to indicate on which fields it operates. The syntax and rules for <code>field</code> or <code>fields</code> are the same as for <code>joins</code>. Each component also has a <code>weight</code> that indicates its relative importance and <code>type</code> to pick how a probabilistic comparison is performed for this component.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>The <code>nested</code> component type here is used to construct an OR rule for probabilistic matching: either one of the nested component rules on field11 and field12 needs to match. If both match that is OK too.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>A negative score means this component functions as a penalty when it does not match, and contributes nothing when it does match.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>The sum of the <code>weights</code> of all elements in the <code>components</code> array that were successful is called the score. If the score is at or above <code>threshold</code>, then it is considered a probabilistic match. If there are multiple probabilistic matches then the one with the highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"11\"></i><b>11</b></td>\n<td>The <code>mode</code> field is optional and only allowed for <code>tres-tik-relations</code>. It restricts how many relations are allowed to be formed from the perspective of the sources and targets. The default value is <code>manyToOne</code> and allowed values are <code>manyToOne</code>, <code>oneToMany</code> and <code>manyToMany</code>. For knowledge graph generation this should probably be set to <code>manyToMany</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"12\"></i><b>12</b></td>\n<td>Turns on Exclusive Facts Clustering (EFC), which is our methodology for keeping entities consistent (where an entity is defined as the collection of records that can reach each other via transitive/multi-hop relations). When EFC is turned on, a penalty that is larger than the matching threshold will automatically be enforced across transitive relations, resulting in entities where no 2 records within the entity would have this penalty when compared to each other. EFC is by default turned off.</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\njoins always win over weighted (probabilistic) matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in weighed (probabilistic) matching are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nexact\n</td>\n<td class=\"hdlist2\">\n<p>Exact comparison of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsame\n</td>\n<td class=\"hdlist2\">\n<p>Lower cases and trims strings before doing an exact comparison. Does not consider <code>null</code> to be the same unless a field is added to <code>nullable</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nfuzzy\n</td>\n<td class=\"hdlist2\">\n<p>Fuzzy string match allowing for a small difference between the strings. The fuzziness tolerance can be set with the numeric <code>threshold</code> (default 0.9).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntypos\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of strings with different sizes and a proportional amount of typos (0 typos for 0 to 3 characters, 1 typo for 4 to 10 characters, 2 typos for 11+ characters)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\noverlap\n</td>\n<td class=\"hdlist2\">\n<p>Treats both sides as sets of string values and considers them a match if the sets have any overlap (ignoring nulls). Will do lower-casing and trimming of strings just like <code>same</code>. If the input is multiple fields it is assumed each field contains string values and the set will be composed from the values from the multiple fields (ignoring nulls). Also supports splitting strings on a provided <code>delimiter</code>. If the input is a single field and <code>delimiter</code> is provided then the single field is assumed to be a string, otherwise the single field is assumed to be an array of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Uses an \"address\" signature field to do address matching. The input is assumed to be an address string that has to be parsed, unless <code>signatures</code> is <code>true</code>, in which case the input is assumed to be address signatures, or unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) parsed address object. Also supports a boolean <code>withoutUnit</code> switch to ignore unit numbers in address matching and a boolean <code>justCityOrZip</code> to match on just the city or zip/postal.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnested\n</td>\n<td class=\"hdlist2\">\n<p>Uses nested probabilistic rules in <code>components</code> and a <code>threshold</code> to set the minimum score of the nested components for this rule to be considered a probabilistic match. Note that the score of the <code>components</code> and the <code>threshold</code> are purely internal to this rule and only used to determine if this rule is considered a probabilistic match. The contribution of this rule to the overall score is solely set by its <code>weight</code>, just like any other rule. Nested can be used to construct AND or OR like constructs.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of people. Requires full names, so first and last. Other elements like middle name, middle initial, title and suffix are also supported. The input is assumed to be a name string to be parsed where <code>threshold</code> controls minimum confidence of parsing, unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) already-parsed name object. The parameters <code>nFirst</code> and <code>nLast</code> can be used to only match on <code>nFirst</code> letters of first name and <code>nLast</code> letters of last name. Also supports parsing of less usual name formats (like last, first) if <code>freeform</code> is <code>true</code>, in which case the input has to be a string (but this can lead to more false positives).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncompanyName\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of companies. Will remove legal (ltd., company, etc.) or structure (holding, enterprise, etc.) related elements before comparing. Also supports an <code>aggressive</code> switch to remove more from company name such as generic corporate activities (consulting, investments, operations, etc.) before performing the comparison.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_yaml_summary_dsl\">4.4. YAML Summary DSL</h3>\n<div class=\"paragraph\">\n<p>The Summary Configuration details the rules to pick the best values for canonical fields from all available values.\nThe rules are score-based, and the value with the best score wins.</p>\n</div>\n<div class=\"paragraph\">\n<p>The Summary Configuration is expressed in a simple YAML format.\nAn extensive example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\npicks: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: other <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  score: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    type: expression <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    value: other\n- fields: [key1, key2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  scores: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - type: prefer <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    field: source\n    values: [source1, source2, source3]\n  - type: expression\n    value: -date\n- field: key3\n  score:\n    type: count <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    field: key3\n- field: key4\n  score:\n    type: rank <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\n    field: key1\n    order: [a, b, c]</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>picks</code> field gives an array of rules to pick best values for canonical fields</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Each rule has to provide the canonical field (or fields) for which a best value is to be picked</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The score is used to pick the best value for a canonical field (or fields).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>A Spark SQL expression can be used to score. The highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>This is an example of multiple canonical fields being scored together. This can ensure consistency (e.g. address, city and zip should probably be picked together).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Multiple scores can be provided instead of a single score. The scores are applied in order. If the first score does not provide a winner the second score is used as a tie breaker, etc.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Type <code>prefer</code> simply means to prefer certain values in the given field (often used to favor one or more sources for records). It scores the preferred sources higher than any other source, but does not pick between the preferred sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>Type <code>count</code> uses an approximate value count to pick the most frequent value. In this example the most frequent value for <code>key3</code> is picked.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>Type <code>rank</code> uses a ranking where values in the array are ranked in order of preference (most preferred first). It assigns the same score to any values not present in the ranking. It is similar to the prefer type, but establishes an order between the preferred values.</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for score <code>type</code> are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\ncount\n</td>\n<td class=\"hdlist2\">\n<p>Pick based on the most frequent value. An approximate counting algorithm is used for this. Requires <code>field</code> for the canonical field to count and base the pick on (this field can be different from the canonical field we are actually picking for).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nrank\n</td>\n<td class=\"hdlist2\">\n<p>Rank values in order of preference. Requires <code>field</code> for the canonical field we are ranking and <code>order</code> with the array of values in order of preference (most preferred first).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nprefer\n</td>\n<td class=\"hdlist2\">\n<p>Prefer values from the array of preferred values above others. Requires <code>field</code> for the canonical field we are expressing a preference for and <code>values</code> with the array of preferred values.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Use a general Spark SQL Expression to score (highest wins). Requires <code>value</code> to provide the expression which can use any canonical fields (columns).</p>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2022-2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-04-25 21:25:09 +0530\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Would be interested in this setup, if possible.  Mainly, less dependence on the ec2 instance would be helpful"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"@hunterc  do you wanna share your comfig, this could be the potential issue."
"\n<h2 id=\"tools\">1. Tools</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>For any of the following TRUCK applications, an explanation of their arguments can be made available when running with the <code>--help</code> option.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-script\">1.1. tres-script</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-script</code> tool compiles and executes a local Spark script.\nThe script expects an <code>Array[String]</code> input that is converted into a Tresata <code>Job</code> class.\nThe arguments can be handled either with Twitter Scalding, or the newer mainargs syntax.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following is an example of a simple Scala script that acts on an input Source and parses arguments using mainargs.\nIt also specifies a Config case class containing any arguments used along with types and any defaults, and logs the inputs to standard output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">script.scala</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import org.slf4j.LoggerFactory\n\nimport com.tresata.scala.row.Field\nimport com.tresata.spark.sql.source.Source\nimport com.tresata.spark.sql.Job\nimport mainargs.{arg, ParserForClass}\n\n(args: Array[String]) =&gt\\;\n  new Job {\n    val log = LoggerFactory.getLogger(getClass)\n\n    case class Config(\n        @arg(doc = \"Source to input data into\")\n        input: Source,\n        @arg(doc = \"source to write output into\")\n        output: Source,\n        @arg(name = \"group-field\", doc = \"column to group by\")\n        groupField: Field = \"A\"\n    )\n\n    override def run(): Unit = {\n      val config: Config = ParserForClass[Config].parseArgs(args.toSeq)\n      log.info(s\"config ${pprint.apply(config)}\")\n      import config._\n\n      input.read.fapi\n        .groupBy(groupField)(_.size(\"size\"))\n        .write(output)\n    }\n  }</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The above script can be run using <code>tres-script</code> with a job call like the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-script script.scala --input bsv%testfile.bsv --output bsv%testoutput.bsv --group-field A</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>tres-script</code> tool also accepts Scalding syntax for specifying arguments.\nThe following snippet provides an alternative script definition to the above <code>script.scala</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.spark.sql.source.Source\nimport com.tresata.spark.sql.Job\nimport com.twitter.scalding.Args\n\n(args: Array[String]) =&gt\\;\n  new Job {\n    override def run(): Unit = {\n      val sargs = Args(args)\n\n      Source(sargs(\"input\")).read.fapi\n        .groupBy(sargs(\"group-field\"))(_.size(\"size\"))\n        .write(Source(sargs(\"output\")))\n    }\n  }</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-source-cat\">1.2. tres-source-cat</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-source-cat</code> tool reads from a data Source and prints bar-separated (by default) data with header to standard output.\nThe job runs locally, avoiding submitting a job to the cluster.\nThe number of lines to output can be limited using the <code>--limit &lt\\;int&gt\\;</code> argument.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-source-convert\">1.3. tres-source-convert</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-source-convert</code> tool reads from a data Source and writes to another data Source with a potentially different format type.\nSee the tool help (<code>--help</code>) for more available features including: generating UUIDs, inserting fields and/or values, hashing column values, renaming fields, sampling data, and more.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-count\">1.4. tres-count</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-count</code> tool counts the number of rows in a source and prints to standard output.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-check-overlap\">1.5. tres-check-overlap</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-check-overlap</code> tool checks the number of identical rows (i.e. the overlap) between two specified input datasets (<code>--left</code> and <code>--right</code>) on HDFS and logs that number to standard output.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-fixed-to-delimited\">1.6. tres-fixed-to-delimited</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-fixed-to-delimited</code> tool infers the format for fixed width files and writes to character-separated outputs.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nThis tool is experimental.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-grep\">1.7. tres-grep</h3>\n<div class=\"paragraph\">\n<p>Grep, but for HDFS!\nSearches input for records that match query pattern.\nRecords are matched on a per field basis, ignoring nulls.\nMatching strategy can be specified as an argument, with the default being <code>exact</code>:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">exact</dt>\n<dd>\n<p>matches against the fixed pattern</p>\n</dd>\n<dt class=\"hdlist1\">glob</dt>\n<dd>\n<p>matches using a glob</p>\n</dd>\n<dt class=\"hdlist1\">regex</dt>\n<dd>\n<p>matches using a java regex pattern</p>\n</dd>\n<dt class=\"hdlist1\">null</dt>\n<dd>\n<p>searches for null values among your columns</p>\n</dd>\n</dl>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-print-schema\">1.8. tres-print-schema</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-print-schema</code> tool prints the schema header for the given table to standard output.</p>\n</div>\n<div class=\"paragraph\">\n<p>By default, the output is printed using a tree-like Spark schema syntax:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code>root\n |-- a: byte (nullable = true)\n |-- b: short (nullable = true)\n |-- c: integer (nullable = true)\n |-- d: long (nullable = true)\n |-- e: float (nullable = true)\n |-- f: double (nullable = true)\n |-- g: decimal(10,0) (nullable = true)\n |-- h: string (nullable = true)\n |-- i: binary (nullable = true)\n |-- j: boolean (nullable = true)\n |-- k: timestamp (nullable = true)\n |-- l: date (nullable = true)</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\n<div class=\"title\">Acceptable Spark to OpenSearch Types</div>\n<div class=\"paragraph\">\n<p>Unfortunately, some type mappings are finicky from Spark to OpenSearch, as such we don&#8217\\;t support all available types.\nMost notable are <code>TimestampType</code> and <code>MapType</code>, but also many Hive types.\nBe sure to check warnings in logs for unsupported column types, as we can&#8217\\;t guarantee that those columns will be correctly written to OpenSearch (at least, in the way you would expect).</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-incremental-load\">1.9. tres-incremental-load</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-incremental-load</code> tool compares two datasets (usually a new version of one dataset to another) and outputs the delta to OpenSearch, i.e. it writes just those rows that differ in the new dataset.\nThis can improve performance by removing duplicates in Spark (where the operation is more optimized) beforehand.\nThis tool works by hashing either all rows or a subset specified using args, and comparing these hashes across datasets as a proxy for full row equality.\nThen, deletes and replaces are performed directly on the OpenSearch index.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nThe selected pkey field (<code>--pkey</code>) must also be the id field within the OpenSearch index\n</td>\n</tr>\n</table>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 1. <code>tres-incremental-load</code> Example</div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Assume you have two versions of the same table:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">/prev.bsv\n+---+---+\n| x | y |\n+---+---+\n| 0 | a |\n| 1 | b |\n| 2 | c |\n+---+---+</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">/curr.bsv\n+---+---+\n| x | y |\n+---+---+\n| 0 | a |\n| 1 | d |\n+---+---+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>We can see that the second column changed, and running the command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-incremental-load --pkey x --prev bsv%prev.bsv --input bsv%curr.bsv --output opensearch%$ES_BASE_URL/incremental/_doc</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>On this initial index:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ES_BASE_URL/incremental/_doc\n+---+---+\n| x | y |\n+---+---+\n| 0 | a |\n| 1 | b |\n| 2 | c |\n| 3 | e |\n+---+---+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Which will give you the following (which is only those columns that have changed), deleting id on <code>2</code> and indexes on id <code>1</code>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ES_BASE_URL/incremental/_doc\n+---+---+\n| x | y |\n+---+---+\n| 0 | a |\n| 1 | d |\n| 3 | e |\n+---+---+</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-hadoop-preflight\">1.10. tres-hadoop-preflight</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-hadoop-preflight</code> tool runs the preflight checklist on the Hadoop cluster, validating that settings are as expected.\nIt returns 0 if successful, else it returns a bitset of failure codes as following for each checklist item that is false (as well as printing the errors for invalid checklist item to standard output).</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Hadoop Preflight Error Codes</caption>\n<colgroup>\n<col style=\"width: 50%\\;\">\n<col style=\"width: 50%\\;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Bit Position</th>\n<th class=\"tableblock halign-left valign-top\">Checklist Item</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Map output compression is enabled</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">1</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">File output format compression is enabled</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">2</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Number of reduce tasks is above configured value (default: 2)</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">3</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Minimum yarn container size (in Mb) is above configured value (default: 1024)</p></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-delta\">1.11. tres-delta</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-delta</code> tool is a Swiss-army knife for dealing with delta tables.\nIf used with no arguments, prints out usage and available tools.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-init\">1.11.1. tres-delta init</h4>\n<div class=\"paragraph\">\n<p>Efficiently initializes parquet tables to delta tables.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nThis tool initializes the input delta table in-place, which means parquet files should not be accessed directly anymore afterwards and should only be used as a delta file (e.g. never using <code>parq%table</code> and only using <code>delta%table</code>).\n</td>\n</tr>\n</table>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 2. Using <code>tres-delta init</code></div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Say you have the following parquet tables:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">foo/risk\n+-----+---------+------+\n| tid | country | risk |\n+-----+---------+------+\n|   0 |      US | 2    |\n|   1 |      CN | 10   |\n+-----+---------+------+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Assuming that the above is partitioned by <code>tid</code> and <code>country</code>, you can convert the <code>risk</code> table to a delta table like so:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta init --table foo/risk --partition tid INT, country STRING</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>And, if the above is not partitioned, then more simply like so:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta init --table foo/risk</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-create\">1.11.2. tres-delta create</h4>\n<div class=\"paragraph\">\n<p>Creates an empty delta table with a given schema (and other optional settings).</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nFor more information on DDL schemas, please see the <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\">HIVE DDL documentation</a>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-compact\">1.11.3. tres-delta compact</h4>\n<div class=\"paragraph\">\n<p>Compacts (optimizes) the delta table by re-partitioning it.\nThis allows delta to more efficiently process future queries by consolidating data.</p>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 3. Using <code>tres-delta compact</code></div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Just like <a href=\"#tres-delta-rm\">tres-delta rm</a>, this command also uses the special partition syntax for delta. You more efficiently\ncompact over selected partitions (saving time and resources) by slightly editing the base command. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta compact --table \"foo/delta/date==2020-06-*\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will compact and repartition only those respective partitions dated under June 2020.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-delete\">1.11.4. tres-delta delete</h4>\n<div class=\"paragraph\">\n<p>Using keys found in column \"id\" of the <code>deletes</code> dataset, deletes records from input delta tables (specified by <code>table</code>) with identical key value (found in column specified by <code>key</code>).</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-important\" title=\"Important\"></i>\n</td>\n<td class=\"content\">\nWe assume that all records are indexed by the same set of keys.\nI.e. if in one table you have record keyed under <code>tresataID</code> of <code>3</code> and another table with record keyed under <code>UNIQUE_ID</code> of <code>3</code> and you specify both keys in your <code>tables</code> input, then both records will be deleted according to their defaults.\nSaid differently, you cannot use this tool to selectively ignore keys to delete based on individual key columns within the corresponding tables.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-manifest\">1.11.5. tres-delta manifest</h4>\n<div class=\"paragraph\">\n<p>Generates a manifest file for a delta table to allow for Athena/Presto integration.\nSee <a href=\"https://docs.delta.io/latest/presto-integration.html\">here</a> for configuration from the Athena/Presto side.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can enable this to be done automatically if you add the following query when you create the table using <code>tres-source-convert</code>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-source-convert\n  --input bsv%foo.bsv\n  --output delta%foo.delta?delta.compatibility.symlinkFormatManifest.enabled=true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For this table, any write operations on it will automatically update the manifest, though you should read the docs linked above to see corner cases for this setting.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-mask\">1.11.6. tres-delta mask</h4>\n<div class=\"paragraph\">\n<p>Using keys found in column \"id\" of the <code>identifiers</code> dataset, mask (i.e. replacing the value in a field) the key matched records from input delta tables (specified by <code>table</code>) with the values defined in the <code>mask</code> dataset.\nSee the example below for how the mask dataset should be created.</p>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 4. Using <code>tres-delta mask</code></div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Say you have the following delta tables:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">foo/pii_0\n+-----+----------------+-------------+\n| tid | name           | social      |\n+-----+----------------+-------------+\n|   0 | Frankie Fungus | 123-45-6789 |\n|   1 | Alice Algae    | 987-65-4321 |\n+-----+----------------+-------------+</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bar/pii_1\n+-----------+----------------+-----+\n| tresataID | name           | age |\n+-----------+----------------+-----+\n|         0 | Frankie Fungus |  23 |\n|         2 | Lucy Lichen    |  58 |\n+-----------+----------------+-----+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Given you want to mask Frankie and Alice from your dataset (e.g. if given a GDPR request).\nYou should then make three tables.\nThe masking table should have three columns (all of type <code>string</code>):</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p><code>table</code> specifies in what tables the value should be masked (note these must be unadorned paths to delta tables in hdfs)</p>\n</li>\n<li>\n<p><code>field</code> specifies in what field in a matched record should be masked</p>\n</li>\n<li>\n<p><code>value</code> which contains the value to mask over that specific field</p>\n</li>\n<li>\n<p><code>id</code> should be the records you want deleted from every table that contains a match</p>\n</li>\n<li>\n<p><code>key</code> should be the column in each table used to key lookup the relevant records</p>\n</li>\n</ol>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">mask.csv\n+-----------+--------+-------------+\n| table     | field  | value       |\n+-----------+--------+-------------+\n| foo/pii_0 | name   | [REDACTED]  |\n| foo/pii_0 | social | 999-99-9999 |\n| bar/pii_1 | age    | 0           |\n| bar/pii_1 | name   | [REDACTED]  |\n+-----------+--------+-------------+</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">identifiers.csv\n+-----+\n|  id |\n+-----+\n|   0 |\n|   1 |\n+-----+</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tables.csv\n+-----------+-----------+\n| table     | key       |\n+-----------+-----------+\n| foo/pii_0 | tid       |\n| bar/pii_1 | tresataID |\n+-----------+-----------+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After running the following snippet</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta mask\n  --tables csv%tables.csv\n  --identifiers csv%identifiers.csv\n  --mask csv%mask.csv</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The two prior tables should now be masked and will be:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">pii_0\n+-----+------------+-------------+\n| tid | name       | social      |\n+-----+------------+-------------+\n|   0 | [REDACTED] | 999-99-9999 |\n|   1 | [REDACTED] | 999-99-9999 |\n+-----+------------+-------------+</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">pii_1\n+-----------+-------------+-----+\n| tresataID | name        | age |\n+-----------+-------------+-----+\n|         0 | [REDACTED]  |   0 |\n|         2 | Lucy Lichen |  58 |\n+-----------+-------------+-----+</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-important\" title=\"Important\"></i>\n</td>\n<td class=\"content\">\nWe assume that all records are indexed by the same set of keys.\nI.e. if in one table you have record keyed under <code>tresataID</code> of <code>3</code> and another table with record keyed under <code>UNIQUE_ID</code> of <code>3</code> and you specify that key, <code>3</code>, in your <code>tables</code> input then both records will be masked according to their defaults.\nSaid differently, you cannot use this tool to selectively ignore keys to delete based on individual key columns within the corresponding tables.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-history\">1.11.7. tres-delta history</h4>\n<div class=\"paragraph\">\n<p>Prints to standard out the history of actions associated with a Delta table.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-rm\">1.11.8. tres-delta rm</h4>\n<div class=\"paragraph\">\n<p>Removes data from a table under the partition path.</p>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 5. Using <code>tres-delta rm</code></div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>This tool uses a special syntax from the source API.</p>\n</div>\n<div class=\"paragraph\">\n<p>Assuming an input table like below:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">/foo/delta\n+-----+---+---+\n|   x | y | z |\n+-----+---+---+\n| \"a\" | 0 | 1 |\n| \"b\" | 2 | 3 |\n| \"a\" | 4 | 5 |\n+-----+---+---+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You can run the command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta rm --table \"/foo/delta/y==0/x==a\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>or</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta rm --table \"/foo/delta/x==a/y==0\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>since it&#8217\\;s order independent!\nTo see what partitions will be removed, you can use the <code>--dry-run</code> flag:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta rm --table \"/foo/delta/y==0/x==a\" --dry-run</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>which will log to standard out:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">+---+---+\n|  x|  y|\n+---+---+\n|  a|  0|\n+---+---+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>to be deleted. After you run the command (not as a dry run) the table would look like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">/foo/delta\n+-----+---+---+\n|   x | y | z |\n+-----+---+---+\n| \"b\" | 2 | 3 |\n| \"c\" | 4 | 4 |\n+-----+---+---+</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The partition format will also accept globs. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-delta rm --table \"/foo/delta/date==2020-03-*\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will remove all data partitioned under the month of March (03) 2020.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-vacuum\">1.11.9. tres-delta vacuum</h4>\n<div class=\"paragraph\">\n<p>Deletes delta data files that are beyond the specified retention period for the table.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"tres-delta-alter\">1.11.10. tres-delta alter</h4>\n<div class=\"paragraph\">\n<p>Alters the delta table&#8217\\;s metadata configuration.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nthe settings values (<code>--settings</code>) are URL-encoded, so a space is represented by <code>%20</code>. E.g if you need to set <code>delta.logRetentionDuration</code> to 7 days you use: <code>--settings delta.logRetentionDuration=7%20days</code>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-hfs-compress\">1.12. tres-hfs-compress</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-hfs-compress</code> tool compresses input and writes to output.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-log-analysis\">1.13. tres-log-analysis</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-log-analysis</code> tool processes and organizes yarn executor logs for easier viewing.</p>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 6. Using <code>tres-log-analysis</code></div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>First, you need to get the application Id from either the resource manager page, or by using:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">yarn application -list</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Next, use that application id to generate the logs and write them to a local file by using:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">yarn logs -applicationId \"$ID\" &gt\\; \"$ID.log\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Note the <code>-applicationId</code> argument is mandatory for the <code>yarn logs</code> tool. You can then process the logs running:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-log-analysis --input \"ID.log\"</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-roll-alias\">1.14. tres-roll-alias</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-roll-alias</code> tool sets the alias to point to the most recent index in OpenSearch with the given prefix.\nOptionally deletes indices older than the specified number of most recent indices with the given alias prefix.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nThe chronology of indices is determined by the date suffix of the index name, in yyyy-MM-dd format, e.g. index1_2022-01-01.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-restore-snapshot\">1.15. tres-restore-snapshot</h3>\n<div class=\"paragraph\">\n<p>The <code>tres-restore-snapshot</code> tool restores an index/indices from a snapshot repository, and blocks on the result until all shards involved are online and ready.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tres-opensearch-offline-snapshot\">1.16. tres-opensearch-offline-snapshot</h3>\n<div class=\"paragraph\">\n<p>Creates a snapshot for OpenSearch in Spark (hence offline from OpenSearch perspective).\nIndexing into OpenSearch is a CPU and disk intensive task.\nThis job does the indexing in Spark instead and saves the results as an OpenSearch Snapshot that can be efficiently loaded into OpenSearch.\nDoing the indexing in Spark is faster and keeps the load off OpenSearch which is often serving data and needs to remain responsive.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nthe OpenSearch index will have <code>number_of_replicas</code> set to 0.\nThis should be changed to at least 1 after the index is loaded into OpenSearch from the snapshot.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nEach job run creates a single snapshot that holds a single index.\nThere is no support for multiple indices in the same snapshot.\nIf you need to create multiple indices you need to do so using multiple snapshots.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nMultiple jobs can write snapshots to the same repository (as determined by <code>--dir</code>), but these jobs must run sequentially.\nIt is not safe to write snapshots to the same repository concurrently.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nDo not write the same index name multiple times to the same repository.\nInstead use naming patterns like <code>&lt\\;name&gt\\;_&lt\\;yyyy-MM-dd&gt\\;</code> to avoid duplicate index names and then use aliases in OpenSearch to make the data available as <code>&lt\\;name&gt\\;</code>.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf a repository is populated with snapshots created offline using this job, then do not also create snapshots in the same repository by other means such as from OpenSearch.\nThis job assumes all snapshots in the repository have been created using this job.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nThe number of shards for OpenSearch (as set with --num-shards) also determines parallelism in spark.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
"<iframe src=\"https://tresata.com/dune-beta-free-trial/\" width=\"720\" height=\"5100\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\">Loading…</iframe>"
"\n<h2 id=\"endpoints\">2. API endpoints</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Tresata supports basic filesystem exploration and extraction of file schemas.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"ls\">2.1. ls</h3>\n<div class=\"paragraph\">\n<p>The <code>/ls</code> endpoint returns a non-recursive file and directory listing for the given path.\nThe <code>path</code> parameter is required and specifies the directory from which to extract a list of contents.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=hdfs:///path/to/directory\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:21:29 GMT\n\n{\n    \"files\": [\n        {\n            \"isDir\": true,\n            \"path\": \"hdfs://dev/path/to/directory/dir1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file1\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file2\"\n        },\n        {\n            \"isDir\": false,\n            \"path\": \"hdfs://dev/path/to/directory/file3\"\n        }\n    ]\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=invalid/path\nHTTP/1.1 404 Not Found\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:24:49 GMT\n\npath: invalid/path does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /ls on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get localhost:8080/ls?path=file:/restricted/path\nHTTP/1.1 403 Forbidden\nContent-Length: 44\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:31:05 GMT\n\naccess denied for path file:/restricted/path</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"schema\">2.2. schema</h3>\n<div class=\"paragraph\">\n<p>The <code>/schema</code> endpoint returns a JSON detailing the schema extracted from the given path using the given format.\nAvi is testing this updating ...Dune initializes a spark session that it uses to extract the schema.\nPaths using the local filesystem are not allowed.\nThe format can be any format prefix supported by Tresata Source (e.g., <code>csv</code>, <code>parq</code>, <code>delta</code>, <code>es</code>, etc.)</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=bsv\"\nHTTP/1.1 200 OK\nContent-Length: 279\nContent-Type: application/json\nDate: Mon, 19 Apr 2023 14:47:29 GMT\n\n{\n    \"fields\": [\n        {\n            \"metadata\": {},\n            \"name\": \"a\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        },\n        {\n            \"metadata\": {},\n            \"name\": \"b\",\n            \"nullable\": true,\n            \"type\": \"string\"\n        }\n    ],\n    \"type\": \"struct\"\n}</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on a nonexistent path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=invalid&amp\\;format=bsv\"\nHTTP/1.1 404 Not Found\nContent-Length: 27\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 14:50:25 GMT\n\npath invalid does not exist</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema on an inaccessible path</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=restricted&amp\\;format=bsv\"\nHTTP/1.1 403 Forbidden\nContent-Length: 33\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:19:01 GMT\n\naccess denied for path restricted</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an unsupported format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/table&amp\\;format=xyz\"\nHTTP/1.1 400 Bad Request\nContent-Length: 22\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:28:49 GMT\n\nunsupported format xyz</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample request and response for /schema with an incorrect format</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ http --verify=no -A bearer -a $ID_TOKEN get \"localhost:8080/schema?path=some/bsv&amp\\;format=parq\"\nHTTP/1.1 400 Bad Request\nContent-Length: 32\nContent-Type: text/plain\\; charset=UTF-8\nDate: Wed, 19 Apr 2023 15:38:46 GMT\n\ninvalid format parq for some/bsv</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-04-23 23:05:09 -0400\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
"\n<h2 id=\"_invocation\">3. Invocation</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_typical\">3.1. Typical</h3>\n<div class=\"paragraph\">\n<p>A typical run of Twig could look like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-load \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --state myProjectDir \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --canonical myCanonical.yaml \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --source source1 \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --input bsvu%data/inputs/source1 <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\ntres-load \\ <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --input bsvu%data/inputs/source2\ntres-resolve \\ <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  --state myProjectDir \\\n  --pipeline myPipeline.yaml <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n  --output bsvu%data/outputs/canonical <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\ntres-output \\ <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source1 \\\n  --output bsvu%data/outputs/source1 <i class=\"conum\" data-value=\"11\"></i><b>(11)</b>\ntres-output \\ <i class=\"conum\" data-value=\"12\"></i><b>(12)</b>\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --output bsvu%data/outputs/source2\ntres-summary \\ <i class=\"conum\" data-value=\"13\"></i><b>(13)</b>\n  --state myProjectDir \\\n  --summary mySummary.yaml \\ <i class=\"conum\" data-value=\"14\"></i><b>(14)</b>\n  --output bsvu%data/summary</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td><code>tres-load</code> is used to load data into an application of Twig. It takes care of data cleaning and parsing, the mapping of columns into the Twig canonical data model, and the conversion into data formats used internally by Twig. It is assumed that more complex data transformations are done as needed before loading data into Twig.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Twig maintains state across commands. All of its state is maintained inside this directory. This directory should be unique to a particular application of Twig and not be shared between applications.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>This configuration file in YAML format describes the mappings of columns for each source into the canonical data model that is used internally by Twig, and any cleaning or other transformations needed to get the data ready for record linkage. See the section <a href=\"#_yaml_canonical_dsl\">YAML Canonical DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Name for this source. If the same source is loaded twice the second time it overwrites the data for this source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The actual location to load data from in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format. The data must have a unique primary key per row.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>A second invocation of <code>tres-load</code> for a different source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td><code>tres-resolve</code> runs record linkage (also known as entity resolution). This can be a multi-step process (a record linkage pipeline) that captures the specific relations between data sources and restricts unwanted linkages. Twig does not support hierarchical record linkage (parent-child relationships) at this point.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>The record linkage process is expressed as steps in a pipeline using a configuration file in YAML format. Deterministic and probabilistic record linkage are supported. See the section <a href=\"#_yaml_pipeline_dsl\">YAML Pipeline DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>The output of <code>tres-resolve</code> is data in the canonical data model with the results of record linkage in a column called <code>tresataId</code>. Each Tresata ID represents a unique entity and as such it presents a simple join key to find all records for an entity.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td><code>tres-output</code> produces the output per source after record linkage completes. The output is the same as the input but with new columns added called Tresata IDs that show the result of the record linkage. Each Tresata ID represents a unique entity. Note that <code>tres-output</code> expects the original input data for the source (as it was provided to <code>tres-input</code> using the <code>--input</code> flag) to still be present and unchanged as it will read it again.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"11\"></i><b>11</b></td>\n<td>The actual location to write the data to in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"12\"></i><b>12</b></td>\n<td>A second invocation of <code>tres-output</code> for the second source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"13\"></i><b>13</b></td>\n<td><code>tres-summary</code> provides summaries (also known as best values or golden records) per entity (and so per Tresata ID). This is rule-based, allowing you to specify for each canonical field what value to pick using a scoring mechanism.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"14\"></i><b>14</b></td>\n<td>The summary process is configured using scoring rules in YAML format. See the section <a href=\"#_yaml_summary_dsl\">YAML Summary DSL</a>.</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_houdini\">3.2. Houdini</h3>\n<div class=\"paragraph\">\n<p>If Twig is to be used for Houdini relations generation, then a typical run could look like this:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">tres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source1 \\\n  --input bsvu%data/inputs/source1\ntres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source2 \\\n  --input bsvu%data/inputs/source2\ntres-load \\\n  --state myProjectDir \\\n  --canonical myCanonical.yaml \\\n  --source source3 \\\n  --input bsvu%data/inputs/source3\ntres-relations \\ <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --state myProjectDir \\\n  --config myConfig.yaml \\ <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  --sources source3 \\ <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  --targets source1,source2 \\ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  --output bsv%data/relations <i class=\"conum\" data-value=\"5\"></i><b>(5)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td><code>tres-relations</code> generates houdini relations (possible/soft edges) between records (entities).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This configuration file in YAML format descibes how the relations should be established. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>The sources (comma-separated) for which we are establishing relations.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The sources (comma-separated) for which we are establishing relations against. This argument is optional. If it is not specified, the relations are generated internally within <code>sources</code>. If <code>targets</code> is specified, also pay attention to the <code>mode</code> argument in the config. The mode is expressed from <code>sources</code> to <code>targets</code>. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The actual location to write the data to in Tresata Source format (<code>&lt\\;format&gt\\;%&lt\\;location&gt\\;</code>). See Tresata TIK Documentation for details on the Source format.</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
"\n<h2 id=\"purpose\">1. Purpose of Twig and Record Linkage</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The aim of Twig is to provide a highly effective and automated approach to the <a href=\"https://en.wikipedia.org/wiki/Record_linkage\">Record Linkage</a> problem.\nTwig can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources.\nEntity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.</p>\n</div>\n<div class=\"paragraph\">\n<p>Twig contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
"by the way, i would recommend running companyName scrubber with aggressive=true,\nso:\n\n```\nscrubbers:\n- type: companyName\n  aggressive: true\n```"
"this aplies the scrubbers in order, so first trimLower and then companyName \n```\n    ---\n    canonical:\n    - name\n    sources:\n      sec__edgar_company_info:\n        pkey: company_cik_key\n        transforms:\n        - type: scrubber\n          from: company_name\n          to: company_name\n          scrubbers:\n          - type: trimLower\n          - type: companyName\n        mapping:\n         name:\n           field: company_name\n```\n\nnote that in this case since you only use scrubbers you can also do:\n```\n    ---\n    canonical:\n    - name\n    sources:\n      sec__edgar_company_info:\n        pkey: company_cik_key\n        mapping:\n          name:\n            field: company_name\n            scrubbers:\n            - type: trimLower\n            - type: companyName\n```"
"somewhat tangentially related... it is possible to run the software on ec2 but store the data and all the configurations on s3. this setup gives you less dependency on your ec2 instance (if it fails you can simply create a new one and resume). it also makes it easier to investigate the data using other tools (e.g. athena with jdbc/odbc and your favorite bi tool). if this is of interesting to you or anyone else we could document this setup. the ec2 instance could even be ephemeral."
"|SCRUBBER||DESCRIPTION|\n|---|---|---|\n||||\n|floatScrubber||Scrubs all valid decimal numbers, returns none if it is not decimal|\n||||\n|intScrubber||Scrubs only valid integers values|\n||||\n|symbolScrubber||Removes punctuations and non alpha-numeric symbols|\n||||\n|addressScrubber||Accepts only address line without city, state, country in it and normalises the string inputted|\n||||\n|atinScrubber||Accepts only valid adoption Taxpayer Identification number|\n||||\n|ccScrubber||Accepts valid credit card number of digits 13-19|\n||||\n|companyNameScrubber||Scrubs only company name from the given string|\n||||\n|countryLatLongScrubber||Provides Latitude and Longitude measurements to any country the user provides|\n||||\n|countryScrubber||Scrubs the country name and normalizes to three digit country name|\n||||\n|domainScrubber||Scrubs the domain name from the url provided|\n||||\n|einScrubber||Validates the correct EIN number|\n||||\n|emailScrubber||accepts and validates the email Id given by the user|\n||||\n|firstNameGenderScrubber||Determines the gender of the person based on the name given|\n||||\n|ipv4Scrubber||Validates if the given IP address is valid or not|\n||||\n|itinScrubber||Validates the correct ITIN number|\n||||\n|nicknameExpander||Prints possible nicknames for name given by the user|\n||||\n|nullScrubber||Removes the empty values|\n||||\n|phoneScrubber||Validates the phone number according to the country code|\n||||\n|ptinScrubber||Validates if the given number is valid PTIN number or not |\n||||\n|regexReplaceScrubber||Regex replace will replace the string according to the regex defined|\n||||\n|regexFilterScrubber||Regex replace will replace the string according to the regex defined|\n||||\n|setFilterNotScrubber||Returns those string that are not given in the filter set|\n||||\n|setFilterScrubber||Filters the string present in the set of strings given by the user|\n||||\n|ssnScrubber||Validates the correct ssn number|\n||||\n|tinScrubber||Validates the correct TIN number|\n||||\n|trimLower||Changes all uppercase characters into lowercase|\n||||\n|urlScrubber||Returns the whole string if the given URL is valid|\n||||\n|zip4Scrubber||Accept valid 9 digit zip code scrubbing for the last 4 digits|\n||||\n|zip5Scrubber||Accepts 9 digit zip code, scrubbing for first 5 digits only|\n||||\n|javaTimeDateScrubber||Validates if the given string is a valid date or not|\n||||\n|mailableAddressScrubber||Accepts only the address line without city state or country factors|\n||||\n|mapScrubber||According to the mapping done by the user, mapscrubber is going to replace the strings accordingly|\n||||\n|andScrubber||When the user wants to use two or more scrubber to one filed, this cleaner can be used. It returns after applying all scrubbers in the sequence|\n||||\n|orScrubber||When multiple scrubbers are given by the user, returns the first scrubber that is satisfied|\n||||\n|or||When multiple scrubbers are given by the user, returns the first scrubber that is satisfied|\n||||\n|nameParser||Parses the name into first-name, first initial, middle name middle initial, last name,gender and genderation|\n||||\n|nameSuffixGenerationScrubber||When user inputs suffix, this scrubber will find which generation they belong to|\n||||\n|nameTitleGenderScrubber||Based on the prefix the user provides, this scrubber will detect  the gender|\n||||\n|stateScrubber||Deetcts valid two digit us states|\n||||\n|passThroughScrubber||Passes all values along|\n||||\n|substringScrubber||To exact the substring by specifying number of characters to keep or drop from either end|\n||||\n|upcScrubber||Accepts valid GITIN format product codes from 2-13 characters without check-digits or 14 charcater product codes with check-digits. This scrubber returns a 13 digit check-digit free product code|\n||||\n|first||Applies the scrubbers in left to right order|\n||||\n|chain||This is used to chain the scrubbers together|\n||||\n|dateRangeScrubber||Displays the range from start and end date|\n||||\n|dateScrubber||Validates the correct date from the user input|"
"Does this work for us"
"[quote=\"gstvolvr, post:1, topic:271\"]\n```\n---\ncanonical:\n- name\nsources:\n  sec__edgar_company_info:\n    pkey: company_cik_key\n    transforms:\n    - type: scrubber\n      from: company_name\n      to: company_name\n      scrubber:\n        type: trimLower\n      scrubber:\n        type: companyName\n    mapping:\n     name:\n       field: company_name\n```\n[/quote]\n\nHello, \n\nIn this case, I would recommend using the \"and\" scrubber.\n\n```\n    transforms:\n    - type: scrubber\n      from: company_name\n      to: company_name\n      scrubbers:\n      - type: and\n        list:\n        - type: trimLower\n        - type: companyName\n```"
"Hi, \n\nIs this the correct format in order to apply multiple scrubbers toa single field? In this case applying `trimLower` and `companyName`. \n\n```\n---\ncanonical:\n- name\nsources:\n  sec__edgar_company_info:\n    pkey: company_cik_key\n    transforms:\n    - type: scrubber\n      from: company_name\n      to: company_name\n      scrubber:\n        type: trimLower\n      scrubber:\n        type: companyName\n    mapping:\n     name:\n       field: company_name\n```"
"Thanks @varun.community.mngr :)"
"If you're having issues connecting your Linux machine to a MS SQL Server, we recommend the following steps -\n\n**1. Confirm connection to the SQL Server**\n\nYou can ping the server by running a `telnet <host-ip> <port>` command. This will quickly confirm your connection. If it is unsuccessful, there might be a network connectivity issue, a firewall blocking the connection, or the SQL Server may not be configured to accept remote connections on the specific port. In this case, you should contact your system admin to check your network configuration, firewall settings, and SQL Server configuration to troubleshoot the issue.\n\n**2. Install JDBC Driver**\n\nIn order to read/write to JDBC connections using Tresata software, it requires a JDBC driver jar to be added to your Spark jars. To find the right jar, we first need to check which Java version we're using - `java -version`.\n\nUsing that information, we can use [this table](https://learn.microsoft.com/en-us/sql/connect/jdbc/microsoft-jdbc-driver-for-sql-server-support-matrix?view=sql-server-ver16#java-and-jdbc-specification-support) to find the compatible JDBC driver jar version. Click on the compatible version, and you will be directed to its download page. Download the driver and move it to your VM.\n\n**3. Open Spark Shell**\n\nNext, navigate to the Tresata Spark package. In order to prevent being redirected to another Spark version, run `unset SPARK_HOME`. \n\nBefore launching the Spark shell, we need to add the JDBC driver jar. To do so, you can add it to your Spark driver class path and launch the Spark shell - `./bin/spark-shell --conf \"spark.driver.extraClassPath=<path-to-jar>`. You can find which JDBC jar to include [here](https://learn.microsoft.com/en-us/sql/connect/jdbc/using-the-jdbc-driver?view=sql-server-ver16).\n\n**4. Test Connection to DB**\n\nIn the Spark shell, we can now test the connection to the DB. Here is some code to do that -\n```\nimport java.util.Properties\n\nval table = <table-name>\nval server = <server-name>\nval database = <database-name>\nval user = <username>\nval password = <password>\n\nval connectionProp = new Properties()\nconnectionProp.put(\"user\", user)\nconnectionProp.put(\"password\", password)\n\n\nval input = spark.read.jdbc(s\"jdbc:sqlserver://${server}:1433;database=${database}\", table, connectionProp)\ninput.show()\n```\n\nIf you get an error that the driver could not establish a secure connection to SQL Server by using Secure Sockets Layer (SSL) encryption, try adding the following to the end of the url - `;encrypt=true;trustServerCertificate=true;`\n\nThe last command should preview the first 20 rows of the table, confirming that we can access the DB.\n\n**5. Using Tresata Software with JDBC**\n\nNow, using Tresata software, we can read/write to JDBC connections using the following syntax - `jdbc%jdbc:mysql://somehost:1234/somedb/sometable?user=user1&pass=pass1`. \n\nWe still need to pass the JDBC jar and can do so with the environment variable `ADD_SPARK_OPTS=\"--conf \"spark.driver.extraClassPath=<path-to-jar>\"`. This variable is usually set in `conf/site.sh` file or on the command line.\n\nYou can find more information about the invocation of our software [here](https://community.tresata.com/t/steps-involved-in-a-dune-beta-run/164).\n\nHope this helps! Please let us know if you have any additional questions or issues."
"Loved the summary on the harmony between both teams @Shantanubose"
"**Introduction**\n\nIn the ever-evolving world of technology, different roles and methodologies have emerged to address various challenges and needs. Two of the most prominent disciplines in recent years are DevOps and Data Engineering. While both play essential roles in the tech industry, they have distinct functions and responsibilities.. As a tech enthusiast who favors DevOps, let's explore their main differences and responsibilities.\n\n![DevOps vs Data Engineering Core Differences & responsibilities|500x500](upload://yJOGV6HD2vbGZMt2FXCUhod46hC.jpeg)\n\n\n**DevOps: Improving Software Development**\n\nDevOps combines development and operations, aiming to make software releases faster and more reliable. It helps companies stay competitive by making sure their software runs smoothly.\n\nMain DevOps Engineer tasks:\n\n1. Boost collaboration between development and operations teams.\n2. Use automation tools to improve the software development process.\n3. Monitor applications for performance and security.\n4. Manage infrastructure and deploy code.\n5. Adapt processes based on feedback.\n\n**Data Engineering: Creating Strong Data Pipelines**\n\nData Engineering, on the other hand, focuses on the design, construction, and maintenance of data pipelines that transport and process data from various sources to a unified destination. Data Engineers work with Big Data technologies and handle large-scale data storage, processing, and analysis. Their primary goal is to ensure that data is accessible, reliable, and scalable for data-driven decision-making..\n\n**Main Data Engineer tasks:**\n\n1. Build and maintain data pipelines for large-scale data processing.\n2. Manage data storage systems and optimize performance.\n3. Implement data processing algorithms.\n4. Work with Data Scientists to provide clean data for analysis.\n5. Ensure data privacy and regulatory compliance.\n\n**Comparing DevOps and Data Engineering**\n\n1. Focus: DevOps streamlines software development, while Data Engineering builds reliable data pipelines, connections for analytics.\n2. Tools: DevOps uses tools like Docker and Kubernetes, while Data Engineering uses technologies like Hadoop,Spark and SQL.\n3. Scalability: DevOps ensures applications can handle high/low demand, while Data Engineering manages growing data volumes around the world.\n\nWhy DevOps Has an Edge\n\nDevOps holds an edge over Data Engineers as it focuses on infrastructure development, enabling seamless collaboration and faster software delivery. Data Engineers work within the infrastructure created by DevOps, using data to derive insights. Although DevOps plays a foundational role, both disciplines are integral parts of the system, each with individual responsibilities. Comparing them may not be worthwhile, as they complement each other and contribute to a well-functioning, data-driven organisation."
"A new branch feature-subtractor has been recorded\n\nhttps://api.github.com/repos/mukulkemla/Community-test/commits/bcea7fa4a7e136e563b8674d899e57665529192a"
"A new commit to branch main has been recorded\nmukulkemla: Adding new libraries to support xyz functions"
"## About the main branch.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident"
"testing for the demo"
"zap test to check time"
"zap to test 1.3 with no author and topic"
"zap test to git.jira"
"Create Zapier testmukulkemla"
"Integrated api to test jira/git"
"testing if new topics raise a ticket"
"https://github.com/Iniyan-mani/Iniyan"
"Testing zap 1.3, should raise a ticket"
"Did it raise a ticket"
"Testing JIRA ZAP, raise ticket"
"Add files via uploadmukulkemla"
"I am able to see the commit, trying to comment on it"
"https://github.com/Iniyan-mani/Iniyan"
"I am able to see the page"
"https://github.com/mukulkemla/Community-test\n\nhttps://github.com/mukulkemla/Community-test/commit/0647a39178d238b65d3533d9f7c670b5423c991b"
"Hi Chafe,\n\nYes, confirming that the ec2 instance I was using had issues even though I was able to connect previously.  I was connecting from ssh which gave me a connection timeout error.   When trying through instance connect it said the instance was unavailable.   I've since terminated the instance, but the error was something like this:\n\nNetwork error: Connection timed out or Error connecting to [instance], reason: Connection timed out"
"In Scala, a case class is a special type of class that is designed to be used in pattern matching. Case classes are immutable by default and can be instantiated without using the 'new' keyword.\n\nHere is an example of a case class in Scala:\n```\ncase class Person(name: String, age: Int)\n```\nIn this example, we have defined a case class called 'Person' with two fields: 'name' and 'age'. We can create an instance of this case class as follows:\n```\nval person = Person(\"John\", 30)\n```\nNote that we did not need to use the 'new' keyword to create an instance of the case class. We simply invoked the case class name followed by the field values in parentheses.\n\nHere are some key features of case classes:\n\n* Fields of a case class are public and immutable by default.\n* Case classes automatically provide implementations for common methods such as 'toString', 'equals', and 'hashCode'.\n* Case classes can be used in pattern matching, which is a powerful feature of Scala. When we use a case class in pattern matching, the compiler generates code to extract the fields of the case class automatically.\n\nOverall, case classes are a useful feature of Scala that can simplify code and make it more expressive."
"Good morning, \n\nTo confirm you're saying that your ec2 instance no longer works and wont let you connect after using dune last week? Is this not working from ssh or instance connect? if it is through ssh can you send the error you received? \n\nThank you"
"Related to getting this medical dataset test up and running:\n\nI was able to download DUNE and run the demo on my ec2 instance earlier last week.   Yesterday, I tried to reconnect to the instance and was unable to connect.  There seemed to have been an issue with the ec2 instance even though nothing was running on the instance and it was in a running state in the AWS console. \n\nBefore standing up a new ec2 instance and re-requesting a download for DUNE, is there any advice on how to prevent this in the future?  Also open to using the pre-configured cloud-hosted version if that's more reliable."
"I've found an interesting dataset for the DUNE test.  The use case would be resolving patients across multiple health system sites.\n\nThere is a publicly available dataset called [Synthetic Denver](https://synthea.mitre.org/downloads) which is realistic synthetic patient data mocked to test entity resolution at a Denver area healthcare provider.  It includes the resolved ids as well for testing.\n\nThe data is stored in [FHIR](https://build.fhir.org/datatypes.html) format and can be converted to CSV using this repo: https://github.com/smart-on-fhir/bulk-data-tools"
"There are several benefits of using Scala over Java:\n\n1. **Concise and expressive syntax**: Scala has a concise and expressive syntax that allows developers to write code in a more concise and readable manner. This can lead to reduced development time and easier maintenance of code.\n2. **Functional programming**: Scala has strong support for functional programming, which makes it easier to write code that is both concise and maintainable. It allows developers to write code that is more modular and easier to test.\n3. **Type inference**: Scala has a powerful type inference system that reduces the amount of boilerplate code required for type declarations. This can help to reduce the size of code and increase productivity.\n4. **Interoperability with Java**: Scala is fully interoperable with Java, which means that developers can use Scala code in Java projects and vice versa. This makes it easy to integrate Scala into existing Java projects and take advantage of the benefits of both languages.\n5. **High-performance computing**: Scala has strong support for parallel programming, which makes it well-suited for high-performance computing. It allows developers to write code that takes advantage of multi-core processors and distributed computing frameworks.\n6. **Scalability**: Scala is designed to scale to large projects and applications, with features such as lazy evaluation, tail recursion, and immutability that help to manage complexity and reduce bugs in large code-bases.\n\nOverall, Scala provides many benefits over Java, including a more concise and expressive syntax, support for functional programming, strong type inference, interoperability with Java, and strong support for parallel and distributed computing."
"Users might discuss best practices for data cleaning and data visualization, share tips and tricks for working with different software tools and programming languages, or seek advice on how to approach a particular data analysis challenge."
"Share your ideas and suggestions for new features or improvements:\n\nDo you have a brilliant idea, you would like to see in the Tresata product? \n\nPost it here and it will be taken ahead! Whether it's a small tweak or a major addition, your input can help us shape the future of the product.\n\nPlease provide as much detail as possible about your idea, including any relevant information that will help us evaluate its potential."
"Functional programming is a programming paradigm that emphasises the use of functions to perform computations. In Scala, functional programming is an important aspect of the language and can be used to create powerful, expressive and maintainable code. In this post, we will explore some of the key concepts of functional programming in Scala.\n\n**1. Immutable Data Structures**\nIn functional programming, immutability is a fundamental concept. Immutable data structures are those that cannot be modified once they have been created. In Scala, immutable data structures can be created using case classes, which are a type of class that automatically generate immutable objects\n\n**2. Higher-Order Functions**\nIn Scala, functions are first-class values, which means that they can be passed as arguments to other functions, returned as values from functions, and stored in variables or data structures. This is a key aspect of functional programming and allows for the creation of higher-order functions, which are functions that take one or more functions as arguments or return a function as their result.\n\n**3. Pattern Matching**\nPattern matching is a powerful feature of Scala that allows for more concise and expressive code. It allows you to match a value against a pattern and execute code based on the match.\n\n**4. Recursion**\nRecursion is a fundamental concept in functional programming and involves calling a function from within itself. In Scala, recursion is used to process data structures recursively, such as lists or trees.\n\nFunctional programming is a powerful paradigm that can help you write code that is more modular, reusable, and easier to reason about. Scala provides a powerful set of functional programming tools that allow you to write expressive and maintainable code. By leveraging these tools, you can create robust and scalable applications."
"Functional programming is a programming paradigm that emphasises the use of functions to perform computations. In Scala, functional programming is an important aspect of the language and can be used to create powerful, expressive and maintainable code. In this post, we will explore some of the key concepts of functional programming in Scala.\n\n**1. Immutable Data Structures**\nIn functional programming, immutability is a fundamental concept. Immutable data structures are those that cannot be modified once they have been created. In Scala, immutable data structures can be created using case classes, which are a type of class that automatically generate immutable objects\n\n**2. Higher-Order Functions**\nIn Scala, functions are first-class values, which means that they can be passed as arguments to other functions, returned as values from functions, and stored in variables or data structures. This is a key aspect of functional programming and allows for the creation of higher-order functions, which are functions that take one or more functions as arguments or return a function as their result.\n\n**3. Pattern Matching**\nPattern matching is a powerful feature of Scala that allows for more concise and expressive code. It allows you to match a value against a pattern and execute code based on the match.\n\n**4. Recursion**\nRecursion is a fundamental concept in functional programming and involves calling a function from within itself. In Scala, recursion is used to process data structures recursively, such as lists or trees.\n\nFunctional programming is a powerful paradigm that can help you write code that is more modular, reusable, and easier to reason about. Scala provides a powerful set of functional programming tools that allow you to write expressive and maintainable code. By leveraging these tools, you can create robust and scalable applications."
"Scala is a general-purpose programming language that blends object-oriented and functional programming concepts. It is a general-purpose language that combines functional programming concepts with object-oriented programming constructs.\n\nScala has a concise syntax that makes it easier to read and write code. It also supports type inference, which means that the compiler can deduce the type of a variable based on its usage, reducing the need for explicit type annotations. Scala is also a statically-typed language, which means that type checking is done at compile-time rather than runtime. This helps to catch errors early in the development process.It runs on the Java Virtual Machine (JVM), making it fully interoperable with Java and allowing developers to easily integrate with existing Java code and libraries.\n\nOne of the key features of Scala is its support for functional programming constructs. Functions in Scala are first-class objects, which means they can be passed around and manipulated just like any other value. Scala also supports immutable data structures and encourages the use of pure functions, which can make code more robust and easier to reason about.\n\nScala also supports a feature called pattern matching, which allows you to match complex data structures and extract values from them. This is particularly useful when working with collections of data, such as lists or maps.\n\nScala also supports object-oriented programming, allowing developers to use familiar concepts such as classes, inheritance, and polymorphism. However, Scala takes a slightly different approach to object-oriented programming than languages like Java. In Scala, everything is an object, including functions and basic types like integers and strings. This allows for more concise and expressive code.\n\nAnother key feature of Scala is its support for concurrency and parallelism. Scala provides a powerful actor-based concurrency model that makes it easy to write concurrent and distributed applications. Additionally, Scala provides libraries for parallel collections and parallel execution, making it easy to take advantage of multicore processors.\n\nIn summary, Scala is a modern programming language that offers a range of features that make it a powerful tool for developing scalable and efficient applications. It is a powerful and flexible language that combines the best of both functional and object-oriented programming. Its support for concurrency and parallelism, along with its interoperability with Java, make it a popular choice for developing high-performance, scalable applications."
"<a name=\"civilized\"></a>\n\n## [This is a Civilized Place for Public Discussion](#civilized)\n\nPlease treat this discussion forum with the same respect you would a public park. We, too, are a shared community resource &mdash; a place to share skills, knowledge and interests through ongoing conversation.\n\nThese are not hard and fast rules. They are guidelines to aid the human judgment of our community and keep this a kind, friendly place for civilized public discourse.\n\n<a name=\"improve\"></a>\n\n## [Improve the Discussion](#improve)\n\nHelp us make this a great place for discussion by always adding something positive to the discussion, however small. If you are not sure your post adds to the conversation, think over what you want to say and try again later.\n\nOne way to improve the discussion is by discovering ones that are already happening. Spend time browsing the topics here before replying or starting your own, and you’ll have a better chance of meeting others who share your interests.\n\nThe topics discussed here matter to us, and we want you to act as if they matter to you, too. Be respectful of the topics and the people discussing them, even if you disagree with some of what is being said.\n\n<a name=\"agreeable\"></a>\n\n## [Be Agreeable, Even When You Disagree](#agreeable)\n\nYou may wish to respond by disagreeing. That’s fine. But remember to _criticize ideas, not people_. Please avoid:\n\n* Name-calling\n* Ad hominem attacks\n* Responding to a post’s tone instead of its actual content\n* Knee-jerk contradiction\n\nInstead, provide thoughtful insights that improve the conversation.\n\n<a name=\"participate\"></a>\n\n## [Your Participation Counts](#participate)\n\nThe conversations we have here set the tone for every new arrival. Help us influence the future of this community by choosing to engage in discussions that make this forum an interesting place to be &mdash; and avoiding those that do not.\n\nDiscourse provides tools that enable the community to collectively identify the best (and worst) contributions: bookmarks, likes, flags, replies, edits, watching, muting and so forth. Use these tools to improve your own experience, and everyone else’s, too.\n\nLet’s leave our community better than we found it.\n\n<a name=\"flag-problems\"></a>\n\n## [If You See a Problem, Flag It](#flag-problems)\n\nModerators have special authority; they are responsible for this forum. But so are you. With your help, moderators can be community facilitators, not just janitors or police.\n\nWhen you see bad behavior, don’t reply. Replying encourages bad behavior by acknowledging it, consumes your energy, and wastes everyone’s time. _Just flag it_. If enough flags accrue, action will be taken, either automatically or by moderator intervention.\n\nIn order to maintain our community, moderators reserve the right to remove any content and any user account for any reason at any time. Moderators do not preview new posts; the moderators and site operators take no responsibility for any content posted by the community.\n\n<a name=\"be-civil\"></a>\n\n## [Always Be Civil](#be-civil)\n\nNothing sabotages a healthy conversation like rudeness:\n\n* Be civil. Don’t post anything that a reasonable person would consider offensive, abusive, or hate speech.\n* Keep it clean. Don’t post anything obscene or sexually explicit.\n* Respect each other. Don’t harass or grief anyone, impersonate people, or expose their private information.\n* Respect our forum. Don’t post spam or otherwise vandalize the forum.\n\nThese are not concrete terms with precise definitions &mdash; avoid even the _appearance_ of any of these things. If you’re unsure, ask yourself how you would feel if your post was featured on the front page of a major news site.\n\nThis is a public forum, and search engines index these discussions. Keep the language, links, and images safe for family and friends.\n\n<a name=\"keep-tidy\"></a>\n\n## [Keep It Tidy](#keep-tidy)\n\nMake the effort to put things in the right place, so that we can spend more time discussing and less cleaning up. So:\n\n* Don’t start a topic in the wrong category; please read the category definitions.\n* Don’t cross-post the same thing in multiple topics.\n* Don’t post no-content replies.\n* Don’t divert a topic by changing it midstream.\n* Don’t sign your posts &mdash; every post has your profile information attached to it.\n\nRather than posting “+1” or “Agreed”, use the Like button. Rather than taking an existing topic in a radically different direction, use Reply as a Linked Topic.\n\n<a name=\"stealing\"></a>\n\n## [Post Only Your Own Stuff](#stealing)\n\nYou may not post anything digital that belongs to someone else without permission. You may not post descriptions of, links to, or methods for stealing someone’s intellectual property (software, video, audio, images), or for breaking any other law.\n\n<a name=\"power\"></a>\n\n## [Powered by You](#power)\n\nThis site is operated by your [friendly local staff](/about) and *you*, the community. If you have any further questions about how things should work here, open a new topic in the [site feedback category](/c/site-feedback) and let’s discuss! If there’s a critical or urgent issue that can’t be handled by a meta topic or flag, contact us via the [staff page](/about).\n\n<a name=\"tos\"></a>\n\n## [Terms of Service](#tos)\n\nYes, legalese is boring, but we must protect ourselves &ndash; and by extension, you and your data &ndash; against unfriendly folks. We have a [Terms of Service](/tos) describing your (and our) behavior and rights related to content, privacy, and laws. To use this service, you must agree to abide by our [TOS](/tos)."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Community is the heart of Tresata. Community guidelines are a crucial aspect of any online platform or community.  To ensure a respectful and safe environment for all Tresata Community Members, we have set the following guidelines:\n\n* Engage respectfully, professionally, and with integrity at all times\n* Describe the situation & context, not specific details\n* Never share sensitive or revealing information (related to people, products, or clients)\n* Keep informal conversations outside of the community\n* Use Tresata Community for all technical topics/ threads\n* When responding to a post, attempt to solve or progress the conversation productively\n* Search for duplicates before posting\n* Use proper grammar and spelling\n* Facts > opinions, post thoughtfully\n* If you think something contributes to the conversation or is the right answer, upvote it and vice-versa\n* Zero-tolerance for inappropriate, hurtful, or negative content\n* [Reminder] - This is an open community so these guidelines apply to both internal & external user engagement, act accordingly"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Welcome and congratulations on becoming a member of the **Tresata community!** \n\nTresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. \n\nYou will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community. \n\nKnow about using the portal here:\n![image|690x329](upload://n72U8QITc9VRqqzBs68ZRcAgwT5.png)\n\n\n\n\n\n\nYou will see personalised cards displayed on the [landing page](https://community.tresata.com/) covering multiple topic categories with relevant posts, specifically for you. The **GET STARTED** section will help you use the downloaded product easily.\n\nYou can leverage the robust **Search** functionality to find topic you are interested and leave comments, vote for the topic, or even create a new topic if needed.\n\nFor any support required, do reach out to support@tresata.com"
"\nWelcome and congratulations on becoming a member of the **Tresata community!** \n\nTresata Community aims to provide a robust platform, where you can find everything about Tresata products, Industry Best Practices and a common forum to Exchange Ideas and Information. This interactive platform provides a self-serve engagement forum where you can go through various informative topics, provide comments, communicate with Tresata experts using Personal Chat options or Emails, etc. \n\nYou will find like minded, Technology Enthusiasts, Data Champions, Tresata Product Experts, and Subject Matter Experts on this Tresata Community. As one of the first members of the Elite group of community users,  you also get to participate in the **30-day test** for a new release of Tresata software.\n\nKnow about using the portal here:\n![image|690x329](upload://n72U8QITc9VRqqzBs68ZRcAgwT5.png)\n\n\n\n\n\n\nYou will see personalised cards displayed on the [landing page](https://community.tresata.com/) covering multiple topic categories with relevant posts, specifically for you. The **GET STARTED** section will help you use the downloaded product easily.\n\nYou can leverage the robust **Search** functionality to find topic you are interested and leave comments, vote for the topic, or even create a new topic if needed.\n\nFor any support required, do reach out to us at support@tresata.com"
"in twig's canonical pkey is no longer required.\nnote that if pkey is not provided twig's load job will create pkey by hashing the row of data. this is stable if the exact same data source is read from twice. but its not stable for any even minor data changes. so this solution for dropping pkeys is only useful for one-time snapshot data asset. it should not be used for any data asset that will be updated."
"# Configuring Tresata\nOnce the package is downloaded and installed on the system we should be able to see the following directories.\n\n`bin  conf  demo  lib  libexec  LICENSE  notices`\n\nThe `demo` directory comprises of the configuration and input data to run a demo use case. \n```\ndemo\n|-- config\n|   |-- canonical.yaml\n|   |-- pipeline.yaml\n|   `-- summary.yaml\n`-- data\n    |-- crock_clients.bsv\n    |-- crock_transactions.bsv\n    |-- mdm_clients.bsv\n    |-- vacuum_clients.bsv\n    `-- vacuum_transactions.bsv\n```\nTo learn more about the config files check out the links below.\n1.https://community.tresata.com/t/configuring-yaml-canonical-dsl/168\n2.https://community.tresata.com/t/configuring-yaml-canonical-dsl/169\n3.https://community.tresata.com/t/configuring-yaml-canonical-dsl/170\n4.https://community.tresata.com/t/configuring-yaml-canonical-dsl/171\n\nTo run your data create and configure the `canonical.yaml` `pipeline.yaml` `summary.yaml` files.\n\n# Defining the paths to the data and config\n\nThe bin directory comprises of the `run-demo.sh` file which contains all the necessary paths to the configs and data. \n\n\n```\nbin\n|-- run-demo.sh\n|-- TRESATA-LICENSE.txt\n|-- tres-load\n|-- tres-output\n|-- tres-relations\n|-- tres-resolve\n`-- tres-summary\n```\n\nCopy this file and update it to match your paths as shown in the code snippet below:\n\n- Change the variable `OUT` to match your project title.\n- `--canonical` holds the path to the canonical.yaml file\n- `--pipeline` holds the path to the pipeline.yaml file\n- `--summary` holds the path to the summary.yaml file\n- `--input and --output` to state the path to the input data and the output data.\nNote- The format of the data is provided alongside the path as format%path.\nExample: `bsvu%${OUT}/output/mdm_clients`\n\nRename your project file, e.g. `run-newproject.sh` \n```\n#!/bin/bash -e\n\nSCRIPT=`readlink -f ${0}`\nSCRIPT_DIR=`dirname ${SCRIPT}`\nAPP_DIR=${SCRIPT_DIR}/..\n\nexport HADOOP_CONF_DIR=${APP_DIR}/conf\nexport ADD_SPARK_OPTS=\"--master local --conf spark.eventLog.enabled=false --conf spark.ui.enabled=false $ADD_SPARK_OPTS\"\n\nOUT=DUNE Beta-demo-out\nSTATE=$OUT/state\n\n# load\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source vacuum_clients --input bsvu%${APP_DIR}/demo/data/vacuum_clients.bsv\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source vacuum_transactions --input bsvu%${APP_DIR}/demo/data/vacuum_transactions.bsv\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source crock_clients --input bsvu%${APP_DIR}/demo/data/crock_clients.bsv\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source crock_transactions --input bsvu%${APP_DIR}/demo/data/crock_transactions.bsv\n${APP_DIR}/bin/tres-load --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source mdm_clients --input bsvu%${APP_DIR}/demo/data/mdm_clients.bsv\n\n# resolve\n${APP_DIR}/bin/tres-resolve --state ${STATE} --pipeline ${APP_DIR}/demo/config/pipeline.yaml --output bsv%${OUT}/prepwithids\n\n# output\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source vacuum_clients --output bsvu%${OUT}/output/vacuum_clients\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source vacuum_transactions --output bsvu%${OUT}/output/vacuum_transactions\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source crock_clients --output bsvu%${OUT}/output/crock_clients\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source crock_transactions --output bsvu%${OUT}/output/crock_transactions\n${APP_DIR}/bin/tres-output --state ${STATE} --canonical ${APP_DIR}/demo/config/canonical.yaml --source mdm_clients --output bsvu%${OUT}/output/mdm_clients\n\n# summary\n${APP_DIR}/bin/tres-summary --state ${STATE} --summary ${APP_DIR}/demo/config/summary.yaml --output bsvu%${OUT}/summary\n\n```\nThe above configurations are set to use the data provided with the package.\n```\n`-- data\n    |-- crock_clients.bsv\n    |-- crock_transactions.bsv\n    |-- mdm_clients.bsv\n    |-- vacuum_clients.bsv\n    `-- vacuum_transactions.bsv\n```\n\nTo work with your data, Update the data files as above or provide the path to you your data files in above configurations.\n\n# Run the application\nOnce all the configurations are defined run the application using `./run-newproject.sh` from the bin directory.\n\n\n\n# Know  the output\nThe output files are located in the twig-demo-out/output directory. These data assets can be used to power any visualisation tools or applications such as Tableau.\n\n# Troubleshooting\n- For any support required reach out to dunebeta.support@tresata.com or post your questions here."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"![TWIG_Install_Demo (2)|video](upload://fMDg104Nv220zXWJFMpuoVUDdYO.mp4)"
"![TWIG_Output_Demo|video](upload://3Qx4lpUYR6MrymE1ZeW18cCQZEU.mp4)"
"![Demo Summary Recording  (2023-03-09 11_39 GMT)|video](upload://mkl1BVFAFR2sdNeBV5scSRVjIJc.mp4)"
"![Demo Vid. Creation (2023-03-08 18_24 GMT)|video](upload://khLdRcNuyPXt0dvzYZIW9yQSrDB.mp4)"
"![TWIG_Pipeline_Walkthrough|video](upload://noLRYJiuFOhSUBMHXaBYCoUe9K7.mp4)"
"\n<h3 id=\"_yaml_summary_dsl\">4. YAML Summary DSL</h3>\n\nhttps://community.tresata.com/t/summary-yaml-configuration-walkthrough/180\n<div class=\"paragraph\">\n<p>The Summary Configuration details the rules to pick the best values for canonical fields from all available values.\nThe rules are score-based, and the value with the best score wins.</p>\n</div>\n<div class=\"paragraph\">\n<p>The Summary Configuration is expressed in a simple YAML format.\nAn extensive example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\npicks: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: other <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  score: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    type: expression <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    value: other\n- fields: [key1, key2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  scores: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - type: prefer <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    field: source\n    values: [source1, source2, source3]\n  - type: expression\n    value: -date\n- field: key3\n  score:\n    type: count <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    field: key3\n- field: key4\n  score:\n    type: rank <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\n    field: key1\n    order: [a, b, c]</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>picks</code> field gives an array of rules to pick best values for canonical fields</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Each rule has to provide the canonical field (or fields) for which a best value is to be picked</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The score is used to pick the best value for a canonical field (or fields).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>A Spark SQL expression can be used to score. The highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>This is an example of multiple canonical fields being scored together. This can ensure consistency (e.g. address, city and zip should probably be picked together).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Multiple scores can be provided instead of a single score. The scores are applied in order. If the first score does not provide a winner the second score is used as a tie breaker, etc.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Type <code>prefer</code> simply means to prefer certain values in the given field (often used to favor one or more sources for records). It scores the preferred sources higher than any other source, but does not pick between the preferred sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>Type <code>count</code> uses an approximate value count to pick the most frequent value. In this example the most frequent value for <code>key3</code> is picked.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>Type <code>rank</code> uses a ranking where values in the array are ranked in order of preference (most preferred first). It assigns the same score to any values not present in the ranking. It is similar to the prefer type, but establishes an order between the preferred values.</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for score <code>type</code> are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\ncount\n</td>\n<td class=\"hdlist2\">\n<p>Pick based on the most frequent value. An approximate counting algorithm is used for this. Requires <code>field</code> for the canonical field to count and base the pick on (this field can be different from the canonical field we are actually picking for).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nrank\n</td>\n<td class=\"hdlist2\">\n<p>Rank values in order of preference. Requires <code>field</code> for the canonical field we are ranking and <code>order</code> with the array of values in order of preference (most preferred first).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nprefer\n</td>\n<td class=\"hdlist2\">\n<p>Prefer values from the array of preferred values above others. Requires <code>field</code> for the canonical field we are expressing a preference for and <code>values</code> with the array of preferred values.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Use a general Spark SQL Expression to score (highest wins). Requires <code>value</code> to provide the expression which can use any canonical fields (columns).</p>\n</td>\n</tr>\n</table>"
"<h3 id=\"_yaml_config_dsl\">3. YAML Config DSL</h3>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Config object to express both joining and weighted matching rules.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\njoins: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- field: field1 <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n- fields: [field2, field3] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n- fields: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  - field4\n  - field5\nweighted: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  components: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  - field: field6\n    weight: 2.0\n    type: same\n  - fields: [field7, field8]\n    weight: 1.0\n    type: overlap\n  - field: field9\n    weight: 1.0\n    type: fuzzy\n  - field: field10\n    weight: 1.0\n    type: typos\n  - weight: 2.0 <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n    components:\n    - field: field11\n      weight: 1.0\n      type: same\n    - field: field12\n      weight: 1.0\n      type: same\n    threshold: 1.9\n    type: nested\n  - field: field13\n    weight: -1.0 <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n    type: same\n  threshold: 2.999 <i class=\"conum\" data-value=\"10\"></i><b>(10)</b>\nmode: manyToMany <i class=\"conum\" data-value=\"11\"></i><b>(11)</b>\nefc: true <i class=\"conum\" data-value=\"12\"></i><b>(12)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>The <code>joins</code> element is optional. If defined, its value is an array of fields which can be used for joining. The array is in descending order of importance, meaning the first fields for joining that return a result win.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>A join on a single field. Note the key is <code>field</code> not <code>fields</code>. The key <code>field</code> expects a single string as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>A join on multiple fields. Note the key is <code>fields</code> not <code>field</code>. The key <code>fields</code> expects an array of strings as the value.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Another join on multiple fields using an alternative syntax.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The <code>weighted</code> element is optional. If defined, it expresses probabilistic matching and it must contain <code>components</code> and <code>threshold</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>The <code>components</code> element has as its value an array of all possible rules that can contribute to probabilistic matching. Each component has a <code>field</code> or <code>fields</code> to indicate on which fields it operates. The syntax and rules for <code>field</code> or <code>fields</code> are the same as for <code>joins</code>. Each component also has a <code>weight</code> that indicates its relative importance and <code>type</code> to pick how a probabilistic comparison is performed for this component.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>The <code>nested</code> component type here is used to construct an OR rule for probabilistic matching: either one of the nested component rules on field11 and field12 needs to match. If both match that is OK too.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>A negative score means this component functions as a penalty when it does not match, and contributes nothing when it does match.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"10\"></i><b>10</b></td>\n<td>The sum of the <code>weights</code> of all elements in the <code>components</code> array that were successful is called the score. If the score is at or above <code>threshold</code>, then it is considered a probabilistic match. If there are multiple probabilistic matches then the one with the highest score wins.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"11\"></i><b>11</b></td>\n<td>The <code>mode</code> field is optional and only allowed for <code>tres-tik-relations</code>. It restricts how many relations are allowed to be formed from the perspective of the sources and targets. The default value is <code>manyToOne</code> and allowed values are <code>manyToOne</code>, <code>oneToMany</code> and <code>manyToMany</code>. For knowledge graph generation this should probably be set to <code>manyToMany</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"12\"></i><b>12</b></td>\n<td>Turns on Exclusive Facts Clustering (EFC), which is our methodology for keeping entities consistent (where an entity is defined as the collection of records that can reach each other via transitive/multi-hop relations). When EFC is turned on, a penalty that is larger than the matching threshold will automatically be enforced across transitive relations, resulting in entities where no 2 records within the entity would have this penalty when compared to each other. EFC is by default turned off.</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\njoins always win over weighted (probabilistic) matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in weighed (probabilistic) matching are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nexact\n</td>\n<td class=\"hdlist2\">\n<p>Exact comparison of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsame\n</td>\n<td class=\"hdlist2\">\n<p>Lower cases and trims strings before doing an exact comparison. Does not consider <code>null</code> to be the same unless a field is added to <code>nullable</code>.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nfuzzy\n</td>\n<td class=\"hdlist2\">\n<p>Fuzzy string match allowing for a small difference between the strings. The fuzziness tolerance can be set with the numeric <code>threshold</code> (default 0.9).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntypos\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of strings with different sizes and a proportional amount of typos (0 typos for 0 to 3 characters, 1 typo for 4 to 10 characters, 2 typos for 11+ characters)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\noverlap\n</td>\n<td class=\"hdlist2\">\n<p>Treats both sides as sets of string values and considers them a match if the sets have any overlap (ignoring nulls). Will do lower-casing and trimming of strings just like <code>same</code>. If the input is multiple fields it is assumed each field contains string values and the set will be composed from the values from the multiple fields (ignoring nulls). Also supports splitting strings on a provided <code>delimiter</code>. If the input is a single field and <code>delimiter</code> is provided then the single field is assumed to be a string, otherwise the single field is assumed to be an array of strings.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Uses an \"address\" signature field to do address matching. The input is assumed to be an address string that has to be parsed, unless <code>signatures</code> is <code>true</code>, in which case the input is assumed to be address signatures, or unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) parsed address object. Also supports a boolean <code>withoutUnit</code> switch to ignore unit numbers in address matching and a boolean <code>justCityOrZip</code> to match on just the city or zip/postal.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnested\n</td>\n<td class=\"hdlist2\">\n<p>Uses nested probabilistic rules in <code>components</code> and a <code>threshold</code> to set the minimum score of the nested components for this rule to be considered a probabilistic match. Note that the score of the <code>components</code> and the <code>threshold</code> are purely internal to this rule and only used to determine if this rule is considered a probabilistic match. The contribution of this rule to the overall score is solely set by its <code>weight</code>, just like any other rule. Nested can be used to construct AND or OR like constructs.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of people. Requires full names, so first and last. Other elements like middle name, middle initial, title and suffix are also supported. The input is assumed to be a name string to be parsed where <code>threshold</code> controls minimum confidence of parsing, unless <code>parsed</code> is <code>true</code>, in which case the input is assumed to be a (nested) already-parsed name object. The parameters <code>nFirst</code> and <code>nLast</code> can be used to only match on <code>nFirst</code> letters of first name and <code>nLast</code> letters of last name. Also supports parsing of less usual name formats (like last, first) if <code>freeform</code> is <code>true</code>, in which case the input has to be a string (but this can lead to more false positives).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncompanyName\n</td>\n<td class=\"hdlist2\">\n<p>Comparison of names of companies. Will remove legal (ltd., company, etc.) or structure (holding, enterprise, etc.) related elements before comparing. Also supports an <code>aggressive</code> switch to remove more from company name such as generic corporate activities (consulting, investments, operations, etc.) before performing the comparison.</p>\n</td>\n</tr>\n</table>"
"<h3 id=\"_yaml_pipeline_dsl\">2. YAML Pipeline DSL</h3>\n\nhttps://community.tresata.com/t/how-to-configure-the-pipeline-yaml/173\n\n<div class=\"paragraph\">\n<p>The pipeline details the steps that make up record linkage.\nEach step captures a particular relationship in your data and can involve one or more sources.\nRecords that participate in a step are either resolved (assigned to an entity), which is final, or they are not, in which case we call them <strong>singletons</strong>, which is not final, and they can still be resolved in a later step.\nBy this mechanism, a source can be resolved in multiple steps: in each later step only the singletons remaining for that source participate.</p>\n</div>\n<div class=\"paragraph\">\n<p>We designed a simple YAML format for the Record Linkage Pipeline Config.\nIt is best illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\nsteps:\n- type: resolve <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  sources: [source1, source2] <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  config: <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    joins:\n    - field: key1\n    - field: key2\n- type: against <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  sources: [source3]\n  targets: [source1, source2] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  config:\n    joins:\n    - field: key1\n    - field: key2\n- type: relaxed <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  pipeline:\n    steps:\n    - type: resolve\n      sources: [source4]\n      config:\n        joins:\n        - field: key3\n    - type: against\n      sources: [source4]\n      targets: [source1]\n      external: true <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      config:\n        joins:\n        - field: key1</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a step of type <code>resolve</code>: all records within this step can link to any other records within the same step.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>For each step you have to specify the sources that participate in it.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Most steps require you to provide a record linkage config. See the section <a href=\"#_yaml_config_dsl\">YAML Config DSL</a> for details.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>This is a step of type <code>against</code>: it resolves some sources against other sources (that have already been resolved in a previous step). These records cannot link to each other.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>When a step is of type <code>against</code> you must provide the sources to resolve against (\"targets\").</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>This is a step of type <code>relaxed</code>, the 3rd and final type for steps. Type <code>relaxed</code> sits somewhere between a <code>resolve</code> and a <code>against</code> step in that it allows both internal linkages and linkages against targets. A full explanation of how <code>relaxed</code> works is outside the scope of this document. Note that <code>relaxed</code> requires you to provide a nested pipeline instead of a config.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Since a <code>relaxed</code> step can have nested <code>against</code> steps that are either internal to the <code>relaxed</code> step or external (meaning they resolve against records in previous steps), the setting <code>external</code> is used to indicate the latter.</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_leveraging_transform_results_in_pipeline\">2.1. Leveraging Transform Results in Pipeline</h4>\n<div class=\"paragraph\">\n<p>As mentioned in the <a href=\"#_canonical_transforms\">Canonical Transforms</a> section, more powerful transformations can be applied per source. This is a continuation of that section&#8217;s example to show how the results of these transformations can be leveraged in the record linkage pipeline.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: resolve\n  sources: [default]\n  config:\n    weighted:\n      components:\n        - field: name <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n          weight: 1.0\n          type: name\n          parsed: true\n        - field: address <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n          weight: 1.0\n          type: address\n          parsed: true\n        - field: phone\n          weight: 1.0\n          type: same\n        - field: name.gender <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n          weight: -100.0\n          type: same\n        - field: name.generation <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n          weight: -100.0\n          type: same\n        - field: address.country <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n          weight: -1.0\n          type: same\n      threshold: 1.9\n    efc: true</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>Name matcher uses already parsed name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Address matcher uses already parsed address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.gender</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Penalty (that will also be used for EFC) on nested <code>name.generation</code> field.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Penalty on nested <code>address.country</code> field.</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_relaxed_with_efc\">2.2. Relaxed with EFC</h4>\n<div class=\"paragraph\">\n<p>A relaxed nested pipeline supports EFC via a provided <code>breakup</code>.\nGiven the flexibility of relaxed (you can model different relations for different sources) and the power of EFC, it is generally recommended (if possible) to model problems as a single relaxed step with EFC.\nAn example follows.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\nsteps:\n- type: relaxed <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  sources: [source1, source2, source3, source4] <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  pipeline: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n    steps:\n    - type: resolve\n      sources: [source1, source2]\n      config:\n        joins:\n        - field: key1\n    - type: resolve\n      sources: [source3]\n      config:\n        joins:\n        - field: key2\n    - type: against <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n      sources: [source3, source4]\n      targets: [source1]\n      config:\n        joins:\n        - field: key3\n        mode: manyToMany <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n    breakup: <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      weighted:\n        components:\n        - field: key1\n          weight: -1.0\n          type: same\n        - field: key3\n          weight: -1.0\n          type: same</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This is our single relaxed step</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>List all sources here</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Pipeline steps here do not run in order (one after the other). In a relaxed pipeline, the steps are independent of each other and run concurrently. The benefit is that we can model different kind of relationships for different sources.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The type <code>against</code> here is used to limit the relationships to those between records of <code>source3</code> and <code>source1</code>, or between records of <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>Do not limit the relationships formed to only picking the best but allow all relations between <code>source3</code> and <code>source1</code> or <code>source4</code> and <code>source1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>EFC is enabled for a relaxed pipeline by providing a <code>breakup</code> config with just penalties. The weights for components don&#8217;t matter here as long as they are negative. The penalties will be used for EFC.</td>\n</tr>\n</table>"
"<h3 id=\"_yaml_canonical_dsl\">1. YAML Canonical DSL</h3>\n\nhttps://community.tresata.com/t/how-to-configure-the-canonical-yaml/174\n<div class=\"paragraph\">\n<p>The Canonical Configuration maps fields (columns) in input data into the canonical data model which will be used for record linkage.\nFor each data source it shows which fields from the canonical data model are present in that source, and which fields in the source data they map to.\nSo it&#8217;s a mapping from canonical fields to source-specific fields.\nOptionally, for each canonical field per source, scrubbers (data cleaning and/or data filtering functions) can be specified.</p>\n</div>\n<div class=\"paragraph\">\n<p>The YAML format for the Canonical Configuration is illustrated with an extensive example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---  <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\ncanonical: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n- key1\n- key2\n- other\nsources: <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  source1:\n    pkey: id <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    mapping: <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      key1:\n        field: id <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n      other:\n        field: other\n        scrubbers: <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n        - type: regexFilter\n          regex: ^\\w+$\n  source2:\n    pkey: key\n    mappings: <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      left: <i class=\"conum\" data-value=\"9\"></i><b>(9)</b>\n        key1:\n          field: lfkey1\n        key2:\n          field: lfkey2\n      right:\n        key2:\n          field: rfkey2</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>A YAML file must start with a line with three dashes</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This is a list of all the fields that make up the canonical data model.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>All sources are mentioned by name in the sources mapping. In this case there are two sources (<code>source1</code> and <code>source2</code>).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>For <code>source1</code> the primary key is called <code>id</code>. Each source must have a primary key column that is unique for each row within that source.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The mapping for <code>source1</code> from the canonical fields to the actual field within the data source. Note that not all canonical fields need to be present here.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The actual field in the <code>source1</code> data that the canonical field maps to.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can optionally be provided to clean and/or filter the data. They are applied in order. The full list of scrubbers is outside the scope of this document.\n\n https://community.tresata.com/t/list-of-scrubbers-to-use-with-dune-beta/275\n\n\n</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>Data source <code>source2</code> has a <code>mappings</code> instead of a <code>mapping</code>, to indicate this data source has multiple mappings per row. This can be the case if a single row has fields for multiple entities (for example a transaction with an originator and beneficiary).</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"9\"></i><b>9</b></td>\n<td>If <code>mappings</code> is to provide multiple mappings for a source then these mappings needs to be named. Here they are called <code>left</code> and <code>right</code>.</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_canonical_transforms\">1.1 Canonical Transforms</h4>\n<div class=\"paragraph\">\n<p>Scrubbers only provide simple string-to-string transformations. To allow for more powerful and generic transformations use <code>transforms</code>, which is available per source. Below is an example of using <code>transforms</code>. This example will be continued in the section <a href=\"#_leveraging_transform_results_in_pipeline\">Leveraging Transform Results in Pipeline</a>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"yaml\">---\ncanonical: [name, address, phone]\nsources:\n  default: <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    pkey: pkey\n    transforms: <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n    - type: name <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n      from: name\n      to: [name] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n    - type: address <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n      from: address\n      to: [address] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n    - type: scrubber <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n      from: phone\n      to: phone\n      scrubber:\n        type: phone\n    - type: expression <i class=\"conum\" data-value=\"8\"></i><b>(8)</b>\n      to: name\n      expression: \"join(' ', $name.firstName, $name.lastName)\"\n    mapping:\n      name:\n        field: name\n      address:\n        field: address\n      phone:\n        field: phone</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This example has only one source called <code>default</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Transforms are provided per source just like <code>mapping</code>/<code>mappings</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>The <code>name</code> transform takes a single string and returns a name object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>The result of name parsing is written as a nested object to the <code>name</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>The <code>address</code> transform takes a single string and returns an address object</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>The result of address parsing is written as a nested object to the <code>address</code> field</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>Scrubbers can be leveraged as transforms as well</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>A simple string manipulation expression can be used using values from existing fields as inputs</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The possible values for <code>type</code> in transforms are:</p>\n</div>\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\nname\n</td>\n<td class=\"hdlist2\">\n<p>Name parsing from single string to name object. Returns <code>firstName</code>, <code>firstInitial</code>, <code>middleName</code>, <code>middleInitial</code>, <code>lastName</code>, <code>gender</code>, <code>generation</code>. The <code>to</code> fields must be either size 1 for nested result or size 7 for top-level (un-nested) results. The minumum required parsing confidence can be set with <code>threshold</code>. Name parsing also inserts gender (if it can be derived from either title or first name) and generation (if it can be derived from a suffix such as <code>junior</code>).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress\n</td>\n<td class=\"hdlist2\">\n<p>Address parsing from a single string to an address object. Returns <code>houseNumber</code>, <code>road</code>, <code>level</code>, <code>unit</code>, <code>city</code>, <code>postcode</code>, <code>state</code>, <code>country</code>. Requires a single <code>from</code> field and multiple <code>to</code> fields. The <code>to</code> fields must be either size 1 for nested result or size 8 for top-level (un-nested) results.\ncompanyName: Company name parsing from a single string to a company name object. Returns <code>name</code>, <code>activity</code>, <code>region</code>, <code>structure</code> and <code>legal</code>. The <code>to</code> fields must be either size 1 for nested result or size 4 for top-level (un-nested)results.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncoalesce\n</td>\n<td class=\"hdlist2\">\n<p>Supports simple string coalesce (pick first non-null value). Requires multiple <code>from</code> fields and a single <code>to</code> field.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nscrubber\n</td>\n<td class=\"hdlist2\">\n<p>Re-use a scrubber as a transform. Requires a single <code>from</code> field, a single <code>to</code> field and a <code>scrubber</code> or <code>scrubbers</code>. If <code>scrubbers</code> are provided they are applied in order.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nexpression\n</td>\n<td class=\"hdlist2\">\n<p>Supports creating a string expression using a small collection of string manipulation functions and references to existing fields.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_string_expressions\">1.2 String Expressions</h4>\n<div class=\"paragraph\">\n<p>In the <code>expression</code> transform we support a simple syntax for creating a string expression. Existing fields are referenced using dollar symbol. For example if you have a field <code>middleName</code> you can refer to it by <code>$middleName</code>. String literals can be given as either single qouted (<code>'this is a literal'</code>) or double quoted (<code>\"this is a literal\"</code>).\nSupported functions are:</p>\n</div>\n<div class=\"dlist horizonal\">\n<dl>\n<dt>lower</dt>\n<dd>\n<p>Lowercase the string. For example: <code>lower($somefield)</code>.</p>\n</dd>\n<dt>upper</dt>\n<dd>\n<p>Uppercase the string. For example: <code>upper($somefield)</code>.</p>\n</dd>\n<dt>concat</dt>\n<dd>\n<p>Concatenate multiple strings. For example: <code>concat($a, $b, ' ', $c)</code>. All <code>null</code> inputs are dropped from the concatenation. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</dd>\n<dt>join</dt>\n<dd>\n<p>Same as concat but also requires a delimiter which is provided first. For example using dash (<code>-</code>) as the delimiter: <code>join('-', $areacode, $phonenumber)</code>.</p>\n</dd>\n<dt>coalesce</dt>\n<dd>\n<p>Pick the first non-null value. For example: <code>concat($a, $b, $c)</code>. If all inputs are <code>null</code> the output is also <code>null</code>.</p>\n</dd>\n<dt>substring</dt>\n<dd>\n<p>Take a substring from a given position (zero based) until a given position (exclusive). For example: <code>substring($a, 2, 4)</code> will extract the third and fourth characters of the string (characters <code>[2, 4)</code> zero based). Negative indices can also be used and are converted into <code>len + 1 - i</code> where <code>len</code> is the length of the input string and <code>i</code> is the negative index. For example <code>substring($a, 0, -2)</code> drops the last character of the string.</p>\n</dd>\n<dt>take</dt>\n<dd>\n<p>Take a substring by taking a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>take($a, 3)</code> takes first 3 characters, and <code>take($a, -4)</code> drops the last 3 characters.</p>\n</dd>\n<dt>drop</dt>\n<dd>\n<p>Take a substring by dropping a number of characters from left. Supports negative indices like <code>substring</code>. For example <code>drop($a, 3)</code> drops the first 3 characters, and <code>drop($a, -4)</code> keeps the last 3 characters .</p>\n</dd>\n<dt>scrub</dt>\n<dd>\n<p>Converts an empty string into a <code>null</code>. For example <code>scrub($somefield)</code></p>\n</dd>\n<dt>trim</dt>\n<dd>\n<p>Trims whitespace on left and right. For example <code>scrub(trim($somefield))</code>.</p>\n</dd>\n</dl>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nAll functions that take a single string as input will return <code>null</code> if that input is <code>null</code>.\n</td>\n</tr>\n</table>"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"## Invocation\n\n### 1. Typical\n\nA typical run of Tresata Beta could look like this:\n\ntres-load \\ (1)\n\n--state myProjectDir \\ (2)\n\n--canonical myCanonical.yaml \\ (3)\n\n--source source1 \\ (4)\n\n--input bsvu%data/inputs/source1 (5)\n\ntres-load \\ (6)\n\n--state myProjectDir \\\n\n--canonical myCanonical.yaml \\\n\n--source source2 \\\n\n--input bsvu%data/inputs/source2\n\ntres-resolve \\ (7)\n\n--state myProjectDir \\\n\n--pipeline myPipeline.yaml (8)\n\n--output bsvu%data/outputs/canonical (9)\n\ntres-output \\ (10)\n\n--state myProjectDir \\\n\n--canonical myCanonical.yaml \\\n\n--source source1 \\\n\n--output bsvu%data/outputs/source1 (11)\n\ntres-output \\ (12)\n\n--state myProjectDir \\\n\n--canonical myCanonical.yaml \\\n\n--source source2 \\\n\n--output bsvu%data/outputs/source2\n\ntres-summary \\ (13)\n\n--state myProjectDir \\\n\n--summary mySummary.yaml \\ (14)\n\n--output bsvu%data/summary\n\n|1|tres-load is used to load data into an application of Tresata Beta. It takes care of data cleaning and parsing, the mapping of columns into the Tresata Beta canonical data model, and the conversion into data formats used internally by Tresata Beta. It is assumed that more complex data transformations are done as needed before loading data into Tresata Beta.|\n| --- | --- |\n|2|Tresata Beta maintains state across commands. All of its state is maintained inside this directory. This directory should be unique to a particular application of Tresata Beta and not be shared between applications.|\n|3|This configuration file in YAML format describes the mappings of columns for each source into the canonical data model that is used internally by Tresata Beta, and any cleaning or other transformations needed to get the data ready for record linkage. See the section [YAML Canonical DSL](https://community.tresata.com/t/steps-involved-in-tresata-run/164#_yaml_canonical_dsl).|\n|4|Name for this source. If the same source is loaded twice the second time it overwrites the data for this source.|\n|5|The actual location to load data from in Tresata Source format (<format>%<location>). See Tresata TIK Documentation for details on the Source format. The data must have a unique primary key per row.|\n|6|A second invocation of tres-load for a different source.|\n|7|tres-resolve runs record linkage (also known as entity resolution). This can be a multi-step process (a record linkage pipeline) that captures the specific relations between data sources and restricts unwanted linkages. Tresata Beta does not support hierarchical record linkage (parent-child relationships) at this point.|\n|8|The record linkage process is expressed as steps in a pipeline using a configuration file in YAML format. Deterministic and probabilistic record linkage are supported. See the section [YAML Pipeline DSL](https://community.tresata.com/t/steps-involved-in-tresata-run/164#_yaml_pipeline_dsl).|\n|9|The output of tres-resolve is data in the canonical data model with the results of record linkage in a column called tresataId. Each Tresata ID represents a unique entity and as such it presents a simple join key to find all records for an entity.|\n|10|tres-output produces the output per source after record linkage completes. The output is the same as the input but with new columns added called Tresata IDs that show the result of the record linkage. Each Tresata ID represents a unique entity. Note that tres-output expects the original input data for the source (as it was provided to tres-input using the --input flag) to still be present and unchanged as it will read it again.|\n|11|The actual location to write the data to in Tresata Source format (<format>%<location>). See Tresata TIK Documentation for details on the Source format.|\n|12|A second invocation of tres-output for the second source.|\n|13|tres-summary provides summaries (also known as best values or golden records) per entity (and so per Tresata ID). This is rule-based, allowing you to specify for each canonical field what value to pick using a scoring mechanism.|\n|14|The summary process is configured using scoring rules in YAML format. See the section [YAML Summary DSL](https://community.tresata.com/t/steps-involved-in-tresata-run/164#_yaml_summary_dsl).|\n\n### 2. Format Prefixes\n\nA format prefix can be specified for each input or output using a format prefix, placed before the URL, separated by a % sign. This specifies the format to read from or write to.\n\nAvailable Format Prefixes\n\n|parq|Parquet format; Spark’s go-to columnar format for performance; supports complex nested types and predicate pushdown|\n| --- | --- |\n|csv|Comma-separated values format with header; supports quoting and type inference|\n|csvu|Same as csv%, but without type inference|\n|csvh|Same as csv% but with separate header file at <path>.header|\n|csvhu|Same as csvu% but with separate header file at <path>.header|\n|bsv|Bar-separated values format with header; supports type inference but not quoting|\n|bsvu|Same as bsv%, but without type inference|\n|bsvh|Same as bsv% but with separate header file at <path>.header|\n|bsvhu|Same as bsvu% but with separate header file at <path>.header|\n|tldsv|Tilde (~) separated values format with header; supports type inference but not quoting|\n|tldsvu|Same as tldsv%, but without type inference|\n|casv|Control-A separated values format with header; supports type inference but not quoting|\n|casvu|Same as casv%, but without type inference|\n|avro|Avro format; supports complex nested types; one file is chosen to extract the schema|\n|hive|Read from or write to Hive tables; only works if the Spark version is compiled with Hive support|\n|json|JSON-per-line format; supports type inference; useful for debugging complex types at small scale|\n|bin|Binary format; allows files to be read in binary format (only read in supported at this moment)|\n|opensearch|OpenSearch; supports complex types (also available as os% or es%)|\n|opensearchjson|Similar to OpenSearch; reads and writes each OpenSearch document as a single json object string (also available as osjson% or esjson%)|\n|libsvm|LIBSVM format, which is popular in certain machine learning communities; does not support writing|\n|orc|Optimized Row Columnar format, a competitor to Parquet from Hive community; only works if the Spark version is compiled with Hive support|\n|kafka|Read from or write to Kafka streams|\n|console|Write output to the console, does not support batch, really only useful for debugging streaming jobs on small test data|\n|text|Read or write plain text lines. Reads in a single column called value with the lines of text. Writes out the first column.|\n|jdbc|Read from or write to jdbc connections. Requires jdbc driver jar to be added to spark jars. Query parameters user and pass are treated special in that they set properties of the jdbc connection instead of general options.|\n|delta|Parquet-backed storage layer that supports transactions, schema verification, updates and deletes.|\n\nExamples:\n\n--input bsv%/data will read in read bar-separated input from /data (either a single file or a directory with files).\n\n--input jdbc%jdbc:mysql://somehost:1234/somedb/sometable?user=user1&pass=pass1 will read from a mysql table without concurrency.\n\nSource supports multiple inputs: they will be merged by column names, with column names that are missing in any of the inputs dropped. For example: --input avro%/some/file bsv%/some/other/file\n\nSource supports multiple outputs: the data will be persisted and then written to all outputs sequentially.\n\nSource also supports an alternate syntax where the format is specified as a query parameter type. For example, to read from a bsv file, use: --input /some/file?type=bsv"
"we are still learning about how to make deployment on k8s as easy as possible. we recently introduced a kubernetes config-map called `tresata-conf` that allows us to have deployment specific settings in our pods. basically this config-map is mounted at `/opt/tresata/common/conf` which gives us `/opt/tresata/common/conf/site.sh` and  `/opt/tresata/common/conf/license_key`.\n\nthe idea is that every pod now has a license key `/opt/tresata/common/conf/license_key` (so no more need for `TRESATA_LICENSE_PATH` environment variable).\n\neach pod also has a `/opt/tresata/common/conf/site.sh` file that is specific to the deployment. this is what it looks like for pelargir:\n```\n$ kubectl exec --stdin --tty proxy -- /bin/bash\ntresata@proxy:~$ more /opt/tresata/common/conf/site.sh \nSPARK_SUBMIT=/opt/spark/bin/spark-submit\nexport TRESATA_MODELS_PATH=s3a://tresata-models\nADD_SPARK_OPTS=\"\n--executor-memory 2G \\\n--executor-cores 1 \\\n--conf spark.executor.memoryOverhead=1024 \\\n--conf spark.hadoop.fs.defaultFS=s3a://tresata-pelargir \\\n--conf spark.kubernetes.namespace=default \\\n$ADD_SPARK_OPTS\"\n```\nthis means no more need for `TRESATA_MODELS_PATH` or `DEFAULT_FS` environment variables.\n\nit does mean we now all have to mount this config-map in every pod. examples of how to do that can be found in `git@server02:user/koert/k8s`, see `aws/proxy.yaml`. also check out the twig workflows in `git@server02:user/koert/argo` in the directory `twig`.\n\nargoscript has already been updated to produce workflows that leverage this new config-map.\n\nalso note that because we now set `fs.defaultFS` (via `spark.hadoop.fs.defaultFS`) you can write your jobs and workflows to be bucket agnostic. basically use `s3a:///some/dir` instead of `s3a://some-bucket/some/dir` in your code and it will automatically use the correct bucket per deployment. see argo twig workflow again for examples of how to do this.\n```"
"# Tresata - **Connect**\n\n## Purpose of Tresata's Connect and Record Linkage\nThe aim of Tresata's Connect is to provide a highly effective and automated approach to the [Record Linkage](https://en.wikipedia.org/wiki/Record_linkage) problem. Tresata can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources. Entity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.\n\nTresata contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources."
"### Domino’s data avalanche\nWith almost 14,000 locations, Domino’s was already the largest pizza company in the world by 2015. But when the company launched its AnyWare ordering system, it was suddenly faced with an avalanche of data. Users could now place orders through virtually any type of device or app, including smart watches, TVs, car entertainment systems, and social media platforms.\n\nThat meant Domino’s had data coming at it from all sides. By putting reliable data profiling to work, Domino’s now collects and analyzes data from all of the company’s point of sales systems in order to streamline analysis and improve data quality. As a result, Domino’s has gained deeper insights into its customer base, enhanced its fraud detection processes, boosted operational efficiency, and increased sales."
"# What is Record Linkage?\nRecord Linkage is the process in which records or units from different data sources are joined together into a single file using non-unique identifiers, such as names, date of birth, addresses and other characteristics. It is also known as data matching, data linkage, entity resolution and many other terms depending on the fields it’s been used.\nRecord linkage is used in creating a sampling frame, removing duplicates from files, providing extra information to assist data processing, or combining files so that relationships on two or more data elements from separate files can be studied.\n\nhttps://community.tresata.com/t/using-tresatas-connect-for-record-linkage/158"
"# WHAT IS DATA CLEANING ?\nData cleaning is an important step in data analysis that involves identifying and correcting or removing errors and inconsistencies in a dataset. The main goal of data cleaning is to improve the quality and accuracy of the data by removing any errors or inconsistencies that might affect the analysis or interpretation of the data.\n\nThe process of data cleaning involves several steps, including:\n* **Data auditing**: This involves assessing the quality of the data, identifying any issues, and determining the best course of action to correct them.\n\n\n* **Data cleaning**: This involves correcting errors, removing inconsistencies, and filling in missing data where possible. Techniques such as outlier removal, data imputation, and data normalization may be used to clean the data.\n\n* **Data validation**: This involves checking that the data meets specific quality standards, such as being accurate, complete, and consistent.\n\n* **Data transformation**: This involves converting the data into a format that can be used for analysis. For example, transforming text data into numerical data or converting data from one type to another.\n\n* **Documentation**: This involves documenting the changes made to the data, including any corrections or transformations, to ensure that the data is transparent and reproducible.\n\nData cleaning is an iterative process that may involve multiple rounds of auditing, cleaning, and validation. The process can be time-consuming and complex, especially when dealing with large datasets or data from multiple sources. However, it is essential to ensure that the data is reliable and accurate and that any insights or conclusions drawn from the data are valid and meaningful.\n\n# IMPORTANCE OF DATA CLEANING\n\nCertainly! The importance of data cleaning cannot be overstated in the world of data analysis. Here are some key reasons why data cleaning is so important:\n\n* **Accurate analysis**: Data cleaning helps ensure that the data being analyzed is accurate and free of errors, which can help to produce more accurate insights and conclusions.\n\n* **Reliable results**: By removing inconsistencies and errors in the data, data cleaning helps ensure that the results of the analysis are reliable and can be replicated.\n\n* **Better decision-making**: Data cleaning can help to improve the quality and accuracy of the data, which can lead to better-informed decision-making based on the insights and conclusions drawn from the data.\n\n* **Improved efficiency**: Data cleaning can help to streamline the data analysis process by reducing the need for manual checking and correcting of errors in the data.\n\n* **Cost-effective**: By identifying and correcting errors in the data early on, data cleaning can help to avoid costly mistakes that might result from inaccurate or incomplete data.\n\n* **Compliance**: In some industries, such as healthcare and finance, compliance regulations require that data be accurate and complete, making data cleaning an essential step to ensure compliance.\n\nIn summary, data cleaning is crucial for ensuring that data is accurate, reliable, and consistent. It helps to produce more accurate insights and conclusions, improves decision-making, and can help to save time and money by avoiding costly mistakes resulting from inaccurate or incomplete data."
"The objective of understanding is to gain general insights about the data that will potentially be helpful for the further steps in the data analysis process, but data understanding should not be driven exclusively by the goals and methods to be applied in later steps. Although these requirements should be kept in mind during data understanding, one should approach the data from a neutral point of view. Decisions to process the data should be taken after subjecting it through certain plausibility checks. This is would include identifying the size of data we are going to process, what kind of business is it associated with, the potential tables and fields that we need to consider to derive insights."
"# TREK Overview\n\nTREK is Tresata’s Data Inventory Engine. TREK is designed to rapidly profile and inventory as-is data stored in Hadoop across all rows and columns to create an informed view of all valuable enterprise data feeds stored in HDFS. With TREK, you’ll be able to inventory large volumes of data and assess the quality of a data asset.\n### Data Profiling\n\nBefore you can turn data into knowledge, you must first know what data you have. With TREK's data profiling capabilities, you can discover \"what\" and \"how much\" usable information is actually in your at-scale data assets.\n![trek1|690x330](upload://aFsCsWNaLhCB1h3Nac2E4rOy6WX.png)\n\n### Data Quality\n\nData Analytics is only as valuable as the quality of the data itself. TREK's field statistics, project timelines, and **Predictive Data Ontologies** are just a few tools produced by TREK to help you determine the value of your data.\n![trek2|690x368](upload://uRgRh7wbrGLznXLmSJoAh3wtzhG.png)\n\n### Data Inventory\n\nTREK's hierarchical structure (projects, partitions, and fields) provides you with a full inventory of all your enterprise data assets in an organized, intuitive UI.\n![trek3|690x206](upload://jsdMwqmUGHuVfR7uUIhffp2c1wJ.png)"
"# Basics of data profiling\n\nData profiling is the process of examining, analyzing, and creating useful summaries of data. The process yields a high-level overview which aids in the discovery of data quality issues, risks, and overall trends. Data profiling produces critical insights into data that companies can then leverage to their advantage.\n\nMore specifically, data profiling sifts through data to determine its legitimacy and quality. Analytical algorithms detect dataset characteristics such as mean, minimum, maximum, percentile, and frequency to examine data in minute detail. It then performs analyses to uncover metadata, including frequency distributions, key relationships, foreign key candidates, and functional dependencies. Finally, it uses all of this information to expose how those factors align with your business’s standards and goals.\n\nData profiling can eliminate costly errors that are common in customer databases. These errors include null values (unknown or missing values), values that shouldn’t be included, values with unusually high or low frequency, values that don’t follow expected patterns, and values outside the normal range.\n# How do we evaluate data quality and inventory?\nTo tackle these tasks, our tool kit offers Tresata's Record Exploration Kit or TREK. It provides a visual representation of populated and unpopulated fields, data types, and field names across all your datasets. It lets you tag, categorize, and view all of your datasets from one place.  Using TREK to organize and take stock of your raw datasets, it's easy to identify gaps, overlap, and a multitude of other qualities easily from the same interactive dashboard."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
" \n<h2 id=\"tfidf\">Appendix C: Tf-Idf Matching</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>TREE has the ability to perform matching over text fields using <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">term frequency–inverse document frequency</a>, or <strong>tf-idf</strong>, scores.\nA tf-idf score is assigned to each word proportional to the number of times that word appears in a document, and is offset by the frequency of the word in the corpus.\nIn the context of matching across columns in TREE, a tf-idf score for a word in a field measures its significance in matching, and a word is weighted less heavily if it appears more often in the column in general.\nA common application for TREE might be for use in matching entries of an address field: the address \"175 varick street\" would assign values to each of the three words, with the value for \"street\" being lower due to its prevalence in the address column.</p>\n</div>\n<div class=\"paragraph\">\n<p>TREE provides matchers that take advantage of the tf-idf score for each word, and will find a match between two values where conventional edit distance-based matchers, such as the Jaccard matcher, will find too large a difference between values.\nThe values \"175 varick st\" and \"175 varick street\" may differ by too much to meet the threshold for a Jaccard matcher, but if tf-idf scores are taken into account, the common tokens \"street\" and \"st\" will not have as much an effect on the similarity score between them.</p>\n</div>\n<div class=\"paragraph\">\n<p>TREE needs to first calculate the prevalence of common tokens in the data.\nBefore values can be mapped to term-weight vectors (which map each token to a decimal score), the data needs to be used to establish the counts for all the common tokens.\nThe training is typically done on a single table from the staging directory, over a single column that will be used as the corpus for calculating document frequency scores for each word.\nTREE provides two approaches to pre-computing token counts, each with their own advantages and disadvantages.\nThe first method involves running a job that computes the document frequencies for each word over a field from a staging table, and then storing the results in an auxiliary file.\nThe file is then loaded into a custom cleaning routine during the loading phase for any table that will need tf-idf scores for tokens over a field.\nThe second method runs training for the document frequencies on-the-fly during the matching phase, and is configured in the <code>Config</code> itself.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nThe file type for prep tables should be left to the default, avro, which can store complex data types including the term-weight vectors that assign scores to each token.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_method_1_cleaning_phase\">C.1. Method 1: Cleaning Phase</h3>\n<div class=\"paragraph\">\n<p>TREE includes a script for running a job that calculates document frequencies for each word that is encountered in a field.\nThe <code>bin/tres-df</code> script is available to run the job on a staging table and save the results into an auxiliary file for later use in the custom cleaners.\n<code>bin/tres-df</code> also takes in an argument for the field to extract tokens from for training, as shown below.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-df \\\n  --input bsv%${STAGING_PATH}/source=testSrc/table=testTbl \\\n  --field address \\\n  --output avro%${TREE_PATH}/auxiliary/address_cms</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This call will create an avro file <code>address_cms</code> in a folder called auxiliary that stores token counts for words encountered in the address field.\nThe output file is stored using a probabilistic data structure (a count-min sketch) that will keep approximate counts, with higher precision for the more common tokens.\n<code>bin/tres-df</code> supports additional arguments for customizing the CMS structure:</p>\n</div>\n<div class=\"dlist\">\n<div class=\"title\">tres-df additional options</div>\n<dl>\n<dt class=\"hdlist1\">--width</dt>\n<dd>\n<p>size of domain for hash functions used for storing token counts (default: 4096)</p>\n</dd>\n<dt class=\"hdlist1\">--depth</dt>\n<dd>\n<p>number of pairwise-independent hash functions used for storing token counts (default: 4)</p>\n</dd>\n<dt class=\"hdlist1\">--remove-stop-words</dt>\n<dd>\n<p>remove tokens matching common stop words (default: false)</p>\n</dd>\n<dt class=\"hdlist1\">--ngram-size</dt>\n<dd>\n<p>split on character n-grams, or words if set to zero (default: 0)</p>\n</dd>\n<dt class=\"hdlist1\">--shingle-size</dt>\n<dd>\n<p>size of subsequences of tokens, or single tokens if set to zero (default: 0)</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p>The <code>bin/tres-df</code> script uses a Lucene tokenizer to process text which will ignore case and can remove stop words (\"the\", \"and\", etc.).\nIf <code>ngram-size</code> is set to zero, the String is split on words, otherwise it is split on character n-grams of the specified size.\nIf <code>shingle-size</code> is set to a non-zero value, the resulting tokens are shingled into subsequences of the given size.\nOnly one of <code>ngram-size</code> or <code>shingle-size</code> can be set to a non-zero value.\nSee the Scaladocs for the <code>com.tresata.matching.core.token</code> package for more information on customizing the Lucene tokenizer.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once a count-min sketch file has been created using the desired corpus data, it can be loaded into a custom cleaner file for use in mapping strings to term-weight vectors.\nThe following file reads in the CMS file that resulted from the call to <code>bin/tres-df</code>, specifies the use of the same Lucene tokenizer, and creates term-weight vectors inside the <code>address</code> field.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import org.apache.spark.sql.DataFrame\nimport org.apache.spark.SparkContext._\nimport com.tresata.spark.ml.ir.tfIdfTermVecs\nimport com.tresata.spark.sql.Dsl._\nimport com.tresata.spark.sql.source.Source\nimport com.tresata.matching.core.token\n\n(df: DataFrame) =&gt\\; {\n  implicit val spark = df.sparkSession\n  import spark.implicits._\n\n  val cmsPath = \"tree/auxiliary/address_cms\"\n  val cms = Source(s\"json%${cmsPath}\").read.as[Seq[Long]].collect.toSeq\n  val tokenizer = token.lucene(removeStopwords = true, ngramSize = 0, shingleSize = 0)\n\n  df.fapi\n    .map(\"address\" -&gt\\; \"address\")(tfIdfTermVecs(cms, tokenizer))\n    .df\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The final step is to make use of the term-weight vectors in a <code>Config</code> by using a matcher that can assess similarity of term-weight vectors.\nThe following is an example of a <code>Config</code> in a pipeline file that uses the <code>Instance</code> matcher to establish matches between term-weight vectors that have a similarity that meets a threshold of 0.85.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">val mc = (Config.init\n  ...\n  matchOn \"address\" score 1.0 using integrated.Instance[String](thres = 0.85)\n)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information on the usage of the <code>Instance</code> matcher and its arguments, consult the Scaladocs for the <code>com.tresata.matching.core.integrated</code> package.</p>\n</div>\n<div class=\"paragraph\">\n<p>This approach for computing document frequencies has the advantage of only creating the count-min sketch object once: during the call to <code>bin/tres-df</code>.\nThis count-min sketch file can then be used across different <code>Config</code> objects.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_method_2_matching_phase\">C.2. Method 2: Matching Phase</h3>\n<div class=\"paragraph\">\n<p>Another method for computing document frequencies of tokens and creating term-weight vectors for use in a <code>Config</code> involves specifying the term-weight transformation within the <code>Config</code> itself.\nTREE provides <code>Config</code> objects with the <code>transform</code> syntax for creating document frequency counts on-the-fly:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.matching.core.dsl.transform\n\nval mc = (Config.init\n  transform \"address\" -&gt\\; \"address\" using transform.TfIdf()\n  matchOn \"address\" score 1.0 using integrated.Instance[String](thres = 0.85)\n  ...\n)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This approach has the advantage of convenience\\; the call to <code>transform.TfIdf()</code> is the only place where the creation of term-weight vectors is specified.\nThere is no need for custom cleaner files or persisting a count-min sketch file to disk.\nThe effect is the same as the first approach, but the creation of a count-min sketch structure and the subsequent transformations are done at matching-time, so prep will contain the un-transformed values instead of term-weight vectors.\nThis also has the disadvantage of being re-run each time the matching pipeline is executed, which may or may not have an appreciable run-time, depending on the size of the input data.\nThe data used to establish the document frequencies includes all the sources involved if the <code>Config</code> is used in a level-1 pipeline.\nIf the <code>Config</code> is used in a level-2 pipeline step, the sources resolved <em>against</em> (i.e. the level-1 tables) are used to build up the document frequencies.</p>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169\\; 2011-2023 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2023-02-20 08:34:57 -0500\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>"
" \n<h2 id=\"address-processing\">Appendix B: Address Processing</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_address_parsing\">B.1. Address Parsing</h3>\n<div class=\"paragraph\">\n<p>Included with TREE is a utility to parse addresses into its component parts. You can use it with the following arguments.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-parse-address\n  --input &lt\\;source&gt\\; <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  --output &lt\\;source&gt\\; <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  [--no-flat] <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n  [--no-sign] <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n  [--complex] <i class=\"conum\" data-value=\"5\"></i><b>(5)</b>\n  [--address &lt\\;field&gt\\;] <i class=\"conum\" data-value=\"6\"></i><b>(6)</b>\n  [--parsed &lt\\;field&gt\\;] <i class=\"conum\" data-value=\"7\"></i><b>(7)</b>\n  [--signature &lt\\;field&gt\\;] <i class=\"conum\" data-value=\"8\"></i><b>(8)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>input source containing addresses to parse</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>output source with parsed addresses added</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>don&#8217\\;t flatten nested data structures (default is <code>false</code>)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>disable writing signatures for address (default is <code>false</code>)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"5\"></i><b>5</b></td>\n<td>write out all address fields instead of just basic fields (default is <code>false</code>)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"6\"></i><b>6</b></td>\n<td>input field for addresses to parse (default is <code>address</code>)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"7\"></i><b>7</b></td>\n<td>output prefix or field for parsed address (default is <code>parsedAddress</code>)</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"8\"></i><b>8</b></td>\n<td>output field for address signatures (default is <code>addressSignature</code>)</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>For example, for the following table,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">addresses.bsv</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"csv\">+------------------------------------------+\n| address                                  |\n+------------------------------------------+\n| 1616 Camden Rd #300, Charlotte, NC 28203 |\n+------------------------------------------+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>running the tool with the command <code>bin/tres-parse-address --input bsv%addresses.bsv --output json%parsed-addresses.json</code> would yield the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">parsed-addresses.json</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{\"address\":\"1616 Camden Rd #300, Charlotte, NC 28203\",\"addressSignature\":\"auct~camden road~1616~300~charlotte&amp\\;aupc~camden road~1616~300~28203\",\"parsedAddressPostcode\":\"28203\",\"parsedAddressCity\":\"charlotte\",\"parsedAddressHouseNumber\":\"1616\",\"parsedAddressUnit\":\"#300\",\"parsedAddressState\":\"nc\",\"parsedAddressRoad\":\"camden rd\"}</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"efc\">Appendix A: Exclusive Facts Clustering</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Tree offers a more advanced version of graph clustering for establishing identities that performs graph cuts on connected components using penalties.\nDefined at the level of a matching Config, Exclusive Facts Clustering, or <strong>EFC</strong>, ensures that any two records with \"conflicting\" information appear in separate Tresata ID groups.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, consider a cluster of records that gets resolved together via a combination of rules on name and address.\nBut now consider that we are also penalizing matches on social security numbers (ssns) and gender, to differentiate between people who share the same name and address.\nThis might prevent any two records with conflicting information from developing a relation between the two of them, but what if the penalized field is sparsely populated?\nWhile two records with a defined and different social security number might not develop a relation, they might both connect to another record where one is not defined.\nUnfortunately, unless a penalized field is fully populated, it&#8217\\;s often the case that an entire group remains connected, even if multiple nodes present conflicting information with one another.</p>\n</div>\n<div class=\"paragraph\">\n<p>This is the motivation for EFC, so that once a Tresata ID group is created, we can use the information we do have to partition the nodes within the subgraph further into separate entities.\nEFC relies on the idea of a \"killer penalty\" to dictate how clusters of records are broken up.\nWhen EFC is enabled in a Config using <code>efc true</code>, any <em>penalties with an absolute score higher then the difference between the maximum possible score and the matching threshold</em> participate in the breakup process.\nAssuming they meet this threshold value, their relative score has no meaning in EFC.</p>\n</div>\n<div class=\"paragraph\">\n<p>EFC will use these penalties to ensure that records with conflicting information appear in different subgroups by breaking apart a subset of the edges.\nAdditionally, EFC will optimize the partitioning such that relations with stronger scores are maintained over weaker edges.</p>\n</div>\n<div class=\"paragraph\">\n<p>In <em>relaxed</em> mode pipelines, EFC is enabled a bit differently.\nRelaxed steps enable EFC for their entire pipeline, as opposed to an individual Config (relegated to a step of the pipeline).\nBecause of this, EFC is not enabled through an <code>efc</code> switch in a Config.\nInstead, to enable EFC in a relaxed step, you must call the <code>breakup</code> method on the pipeline and pass in a Config containing penalties.\nFor example, see the following relaxed step from a pipeline:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">relaxed (Pipeline.init\n  resolve \"i\" using config1\n  resolve \"j\" using config2\n  resolve \"j\" against \"i\" using config2\n  breakup (Config.init\n    penalizeOn \"ssn\" penalty 1.0 using integrated.Same[String]()\n    penalizeOn \"gender\" penalty 1.0 using integrated.Same[String]()\n  )\n)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example, the call to <code>breakup</code> enables EFC, and the penalties on \"ssn\" and \"gender\" will be used to ensure their are no conflicting values within the partitioned Tresata ID groups.\nTheir scores (<code>1.0</code> and <code>1.0</code>) are ignored\\; all penalties passed into <code>breakup</code> are used by EFC.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"master\">12. Customizing Master Output</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The Master component of TREE affords the user the flexibility to create master record tables for the original tables in staging, allowing for the insertion of scrubbed values, roll-up values, and the application of a few basic field operations.\nAfter the output tables are generated (original staging tables with Tresata ID fields joined in), the user can call the <code>bin/tres-master</code> script, and TREE will read a <em>master configuration</em> (for the given source) from the canonical config file.\nThe master table can contain any subset of fields from the staging, prep, and roll-up tables.\nIn addition, the fields can be manipulated to fit the format preferable to the user.</p>\n</div>\n<div class=\"paragraph\">\n<p>In the example below, a \"best\" first name field (<code>best_firstName</code>) is created by coalescing the first name values from each of the roll-up tables.\nThe prep and roll-up fields are also renamed to use the \"clean_\" and \"best_\" prefixes, and the Tresata ID fields for the two hierarchies are renamed to \"child_id\" and \"parent_id\".</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{\n  \"canonical\": [ \"firstName\", \"lastName\", \"dob\", \"ssn\", \"address\" ],\n  \"sources\": {\n    \"testSrc\": {\n      \"pkey\": \"pkey\",\n      \"master\": {\n        \"stagingFields\": [ \"full_name\", \"dob\" ], <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n        \"canonicalFields\": [ \"lastName\", \"ssn\", \"address\" ], <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n        \"rollUpFields\": { <i class=\"conum\" data-value=\"3\"></i><b>(3)</b>\n          \"l1\": [ \"firstName\" ],\n          \"l1a\": [ \"firstName\" ]\n        },\n        \"operations\": [ <i class=\"conum\" data-value=\"4\"></i><b>(4)</b>\n          { \"type\": \"rename\", \"from\": \"ref_lastName\", \"to\": \"clean_lastName\" },\n          { \"type\": \"rename\", \"from\": \"ref_ssn\", \"to\": \"clean_ssn\" },\n          { \"type\": \"rename\", \"from\": \"ref_address\", \"to\": \"clean_address\" },\n          { \"type\": \"coalesce\",\n            \"from\": [ \"rollUp_l1_ref_firstName\", \"rollUp_l1a_ref_firstName\" ],\n            \"to\": \"best_firstName\"\n          },\n          { \"type\": \"rename\", \"from\": \"tresataId_ref_l1\", \"to\": \"child_id\" },\n          { \"type\": \"rename\", \"from\": \"tresataId_ref_l1a\", \"to\": \"parent_id\" },\n          { \"type\": \"discard\", \"fields\": [ \"rollUp_l1_ref_b\", \"rollUp_l1a_ref_b\" ] }\n        ]\n      },\n      \"entities\": {\n        \"ref\": {\n          ...\n        }\n      }\n    }\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>Fields from the original table in staging</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>Fields from the prep table</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"3\"></i><b>3</b></td>\n<td>Fields from the roll-up tables</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"4\"></i><b>4</b></td>\n<td>Series of post-processing operations (supported operations include <code>coalesce</code>, <code>rename</code>, <code>discard</code>, and <code>project</code>)</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Note that the canonical fields gathered from the prep and roll-up tables are given a set of unique prefixes to prevent name clashes.\nFields from prep are prefixed with the entity name (<code>ref_</code> in this example), and roll-up fields are prefixed with <code>rollUp</code> followed by the hierarchy and entity name.\nSimilarly, Tresata ID fields use a suffix containing the entity and hierarchy name.</p>\n</div>\n<div class=\"paragraph\">\n<p>To generate a master record table, the canonical config must contain a master configuration for the source, and the <code>bin/run</code> script should contain a line</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-master --source &lt\\;source&gt\\; [--table &lt\\;glob-syntax&gt\\;]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>after <code>bin/tres-output</code> has been called for the given source/tables.\nAs with <code>bin/tres-link-2</code> and <code>bin/tres-output</code>, <code>bin/tres-master</code> supports optional globbing on tables.\nThe master output is the final step in processing a table through TREE.\nFor more examples of master configurations, see the canonical config in the demo.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"roll-up-configs\">11. Roll-up Rules In-depth</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The Roll-up step of TREE establishes representative values for canonical fields amongst the level-1 records that belong to an individual Tresata ID.\nThis set of values for each Tresata ID can be used in the next hierarchy of matching, or just as informative data to supplement the original records in the <a href=\"#master\">master output</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before the representative values are chosen, TREE reads in a list of rules from a file or class which specifies how the \"best\" values are to be chosen within each Tresata ID group.\nBelow is an example of a very basic roll-up configuration that chooses a single best \"name\" by scoring the values with the number of times the name appears in each Tresata ID group.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import org.apache.spark.sql.functions._\nimport com.tresata.matching.core.dsl.RollUpRules\nimport com.tresata.matching.core.dsl.rules._\n\nRollUpRules.init\n  score \"name\" -&gt\\; \"best_name\" using countOf \"name\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>TREE will run a roll-up job to generate a table containing a primary key (the Tresata ID) and a <code>best_name</code> field.\nThe values of the <code>best_name</code> field will be the most common (non-null) entries from the <code>name</code> column within each Tresata ID.\nIf there is a matching configuration for level-1a, the input fields for matching are the output fields of roll-up (roll-up essentially becomes \"prep\" for hierarchical matching).</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA roll-up phase is required for running multiple hierarchies of matching, i.e. level-1 roll-up becomes the \"prep\" for level-1a matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>There is also the option to generate a set of values instead of picking one representative for the Tresata ID group.\nTREE can run matching using rules that compare sets of values (for example, looking for overlaps in a first name field).\nWe can modify the above example to take the top 3 most common names for roll-up by using the <code>collect ... top ... using ...</code> syntax:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">RollUpRules.init\n  collect \"name\" -&gt\\; \"best_names\" top 3 using countOf \"name\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>We can also forgo the use of a scoring function altogether and keep all the values in the given field.\nDoing so requires setting a maximum value to limit the size of the set of values.\nWe can choose to either take the first N values we encounter and discard the rest, or discard groups that surpass the maximum altogether.\nDiscarding entire groups that surpass this threshold is useful in instances where too many differing entries is indicative of an issue with the input data\\; if we see a user has more than 30 different first names, it is possible they are not the same entity, and it may be harmful to allow it to go through matching with all 30 names.\nThe threshold for the number of values to allow is set with the <code>keep</code> or <code>abortIfGreaterThan</code> parameters:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">RollUpRules.init\n  collect \"name\" -&gt\\; \"names\" abortIfGreaterThan 5\n  collect \"address\" -&gt\\; \"addresses\" keep 5</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In addition to just using the count of occurrences in determining the roll-up value, the user can specify one of a set of other available rules, any custom spark-sql user-defined function (UDF), or a combination of rules that are applied in sequence (where a tie in the score from the first function is settled by the score of the next function, and so on).\nA rule ranks the value in the input field by scoring the record that contains it.</p>\n</div>\n<div class=\"paragraph\">\n<p><code>prefer</code> takes in a field along with a set of preferred values, which will rank a record with one of the given values in that field higher than any records where the value does not belong to the set.</p>\n</div>\n<div class=\"paragraph\">\n<p><code>prioritize</code> takes in a field with a sequence of values that are in order of most preferred to least.\nRecords containing a value in that field with higher priority are ranked higher than records with a value of lower priority, which will be ranked higher than records with a value not in the sequence.</p>\n</div>\n<div class=\"paragraph\">\n<p>The example below shows an example of a fully formed (though partly contrived) roll-up rules file.\nNote that multiple input fields for a <code>score</code> or <code>collect</code> rule are allowed.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import org.apache.spark.sql.functions._\nimport com.tresata.matching.core.dsl.RollUpRules\nimport com.tresata.matching.core.dsl.rules._\n\nRollUpRules.init\n  score \"name\" -&gt\\; \"name\" using (\n    prefer(\"source\", (\"A\", \"B\", \"C\")),\n    prioritize(\"source\", (\"B\", \"A\")),\n    countOf(\"name\")\n  )\n  score \"name\" -&gt\\; \"name1\"\n    filter \"source == 'A'\"\n    filter \"length(name) &gt\\; 3\" <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n    using countOf(\"name\")\n  score \"zip\" -&gt\\; \"zip\" using (\n    prefer(\"source\", \"A\"),\n    countOf(\"zip\")\n  )\n  score \"last_name\" -&gt\\; \"last_name\" using \"timestamp\" <i class=\"conum\" data-value=\"2\"></i><b>(2)</b>\n  collect \"name\" -&gt\\; \"aliases\" abortIfGreaterThan 5\n  collect \"login\" -&gt\\; \"logins\" keep 10\n  collect (\"name\", \"age\") -&gt\\; \"agedName\" top 5 using udf{\n    (name: String, age: Int) =&gt\\; if (name != null) age.toDouble else 0.0\n  }.apply(col(\"name\"), col(\"age\"))\n  collect \"address\" -&gt\\; \"known_addresses\" top 3 using countOf(\"address\")</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>Note that this is identical to the single filter expression <code>filter \"source == 'A' AND length(name) &gt\\; 3\"</code></td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td><code>\"timestamp\"</code> is a short form for a udf expression, and effectively takes the most recent record (largest value)</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>If roll-up is desired, it should be invoked in the <code>bin/run</code> script after level-1 matching has completed (a call to <code>bin/tres-link-1</code>), using the <code>bin/tres-roll-up-1</code> command (no arguments).\nFor further hierarchies, there are scripts for the later tiers: <code>bin/tres-roll-up-1a</code>, <code>bin/tres-roll-up-1b</code>, and <code>bin/tres-roll-up-1c</code>.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"singleton-roll-up\">11.1. Singleton Configuration</h3>\n<div class=\"paragraph\">\n<p>Beginning with version 6.5.1, TREE provides the user with the ability to specify separate rules for singleton records, i.e., Tresata IDs that comprise of a single record.\nThis can be a useful optimization when there are a large number of singletons, since it avoids creating and maintaining an expensive probabilistic data structure for storing counts of occurrences for column values over every Tresata ID.</p>\n</div>\n<div class=\"paragraph\">\n<p>We can provide a separate set of roll-up rules to handle singletons, allowing these records to bypass the roll-up scoring process.\nSince these records will only contain one record, the rules will be much simpler, filling in the roll-up field with the single value (or a set containing the single value, for fields containing sets of values).</p>\n</div>\n<div class=\"paragraph\">\n<p>Consider the following example of a roll-up rules configuration:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">RollUpRules.init\n  score (\"name\", \"address\") -&gt\\; (\"name\", \"address\") using countOf(\"name\")\n  collect \"name\" -&gt\\; \"aliases\" abortIfGreaterThan 5\n  collect \"address\" -&gt\\; \"known_addresses\" top 3 using countOf(\"address\")</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>These three rules result in a roll-up table containing four output columns: <code>name</code>, <code>address</code>, <code>aliases</code>, and <code>known_addresses</code>.\nThe <code>name</code> and <code>address</code> columns contain a single value, the <code>aliases</code> field stores up to five values in a set of Strings, and <code>known_addresses</code> contains a set with up to three Strings.\nIn order to specify roll-up rules for singletons, all four of these fields must be defined using a singleton rule with the <code>forSingleton</code> syntax:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">RollUpRules.init\n  score (\"name\", \"address\") -&gt\\; (\"name\", \"address\") using countOf(\"name\")\n  collect \"name\" -&gt\\; \"aliases\" abortIfGreaterThan 5\n  collect \"address\" -&gt\\; \"known_addresses\" top 3 using countOf(\"address\")\n  forSingleton (\"name\", \"address\") use (\"name\", \"address\")\n  forSingleton \"aliases\" use fill(\"name\")\n  forSingleton \"known_addresses\" use fill(\"address\")</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In the above roll-up rules config, the final three rules use the <code>forSingleton ... use ...</code> syntax to specify how to fill the roll-up output fields using the record itself.\nThe <code>use</code> keyword accepts either an input field/fields or a call to <code>fill</code>, which populates the column with a set containing a single value.\nIt&#8217\\;s required that if any singleton rules are present, all output fields have a singleton rule, where the <code>fill</code> function is used on fields containing sets of values.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"matching-configs\">10. Matching Configurations In-depth</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The matching configuration defines the logic for scoring and creating edges in TREE.\nIt is responsible for defining how to form absolute edges, or <strong>piggybacks</strong>, as well as edges between records that surpass a threshold score, using a series of <strong>scored matching</strong> rules.\nIt is configured and integrated into a pipeline in a matching pipeline file.</p>\n</div>\n<div class=\"paragraph\">\n<p>A matching configuration is defined using the <code>Config</code> domain-specific language (DSL), which declares matching rules and scores, as well as other settings pertinent to the matching process.\nThe following is an example of a matching config containing both piggybacks and scored matching rules.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  piggybackOn \"id\" score 100.0\n  matchOn \"first\" score 1.0 using integrated.Jaccard(tok = token.ngrams(size = 2), thres = 0.6)\n  matchOn \"last\" score 1.0 using integrated.Jaccard(tok = token.ngrams(size = 2), thres = 0.6)\n  matchOn \"ssn\" score 1.0 using integrated.Same[String]()\n  matchIfScore 1.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After initializing the matching config with <code>Config.init</code>, any number of rules or settings can be chained onto the result.\nIn this example we declare one field, the <code>id</code> field, to be sufficient evidence of a match between any two records with the rule <code>piggybackOn \"id\" score 100.0</code>.\nThis indicates that any two records with a matching, non-null value in the <code>id</code> field form an edge that will be written to the relations table.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to the piggyback rule, the matching config defines three rules with a score, using the <code>matchOn ... score ... using ...</code> syntax.\nEach of these rules will contribute either 0.0 or the stated score value to the total score between two records.\nIf the sum of these scores is greater than 1.9, the threshold defined in the <code>matchIfScore</code> call, two records are considered to represent the same entity, and an edge between them is added to the relations table.\nThe piece that follows the <code>using</code> keyword is the <strong>matcher</strong>.\nIn the case of the <code>ssn</code> field, the values are compared as is using the <code>integrated.Same[String]()</code> matcher, and are given a score of 1.0 if they are equivalent non-null Strings.\nFor <code>first</code> and <code>last</code>, the values are converted to a set of <a href=\"https://en.wikipedia.org/wiki/Bigram\">bi-grams</a>, and if the <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Jaccard index</a> between two sets of bi-grams is at least 0.6, a score of 1.0 is given to the pair.</p>\n</div>\n<div class=\"paragraph\">\n<p>Note that not all pairs are compared and scored, as this would increase the running time of TREE quadratically with the size of the data.\nInstead, TREE automatically groups the data several times into smaller groups by generating signatures based on the matchers used, and does this automatically without any input from the user.\nThese groups contain sets of records that have an appreciable probability of generating a match.\nPiggybacks work differently and in parallel with this process\\; any two records that have a matching value in a piggybacked field are guaranteed to form a match and be connected via a path of edges.\nMore information about the matchers, as well as the rules that can be used, is available in the Scaladocs for <code>Config</code> and <code>integrated</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A <code>Config</code> can be re-used throughout multiple steps of a <code>Pipeline</code>.\nIn the following example of a matching pipeline, we use one matching config to resolve sources <code>a</code> and <code>b</code> together, and then another for sources <code>c</code>, <code>d</code>, <code>e</code>, and <code>f</code> through a series of targeted matching steps.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">val config1 = Config.init // ...\nval config2 = Config.init // ...\nval pipeline = (Pipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve (\"c\", \"d\") against (\"a\", \"b\") using config2\n  resolve (\"e\", \"f\") against (\"c\", \"d\") using config2\n)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>TREE also allows the user to define <strong>penalties</strong>, rules which subtract from the score between two records.\nPenalties are often useful if equivalent values do not add much information, but differing values are indicative of a mismatch.\nFor example, take the following matching rule that defines a penalty for mismatches in the <code>gender</code> field, a field containing either an \"M\" or \"F\" string:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">penalizeOn \"gender\" penalty 1.0 using integrated.Same[String]()</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Here we tell TREE that a score of 1.0 should be subtracted from the net score between two records, potentially disallowing a relation, in the case of a mismatch in the <code>gender</code> field.\nThis could be useful if for example the gender column was only marginally populated, and we don&#8217\\;t want to rely on the presence of a gender field for a match, so we allow a sum of rules that doesn&#8217\\;t include gender to total above the threshold.\nThis way only when the value in the <code>gender</code> field for two records is present and different do we allow it to impact the total score.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to <code>matchOn</code> rules, we can also specify <code>scoreOn</code> rules, which in most applications is used as a \"boost\" to the score for a pair of records, which will still need to reach the matching threshold through <code>matchOn</code> rules alone.\nSuch a boost can be useful if we are scoring level-2 matches, where only the highest scored edge is used to resolve the record&#8217\\;s identity.\nTake the following example of a matching configuration:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"ssn\" score 1.0 using integrated.Same[String]()\n  matchOn \"firstName\" score 0.2 using integrated.Same[String]()\n  scoreOn \"address\" score 0.2 ifComparisonAtLeast 0.8 using comparison.jaroWinkler\n  matchIfScore 1.1</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>We can still only reach the matching threshold of 1.1 if two records form a match on <code>ssn</code> and <code>firstName</code>.\nThe <code>comparison.jaroWinkler</code> call is an example of a <strong>Compare</strong> function, which takes two values and outputs a number from 0.0 to 1.0, where 1.0 is a perfect match.\nBy declaring <code>score 0.2 ifComparisonAtLeast 0.8</code>, we are converting this into a binary rule: either the Compare function returns a value below 0.8, and the pair is not given a score boost, or the function returns at least 0.8, and a boost of 0.2 is given to the pair&#8212\\;&#8203\\;for a maximum possible score of 1.4.\nIf we leave out the score and threshold, the output of the Compare function is used.</p>\n</div>\n<div class=\"paragraph\">\n<p>Note that unlike <code>matchOn</code> rules which employ a Matcher function, <code>scoreOn</code> rules can accept a Compare function as well.\nMatcher classes contain <code>Compare</code> functions themselves, but also define a <code>Derive</code> function, that will push some of the preprocessing work out of the comparison function, if possible, so that they are calculated only once per record.\nIf a Matcher is available over an analagous comparison function, using it instead of the comparison function will result in a minor efficiency boost.\nMore information about the built-in Compare functions can be found in the Scaladocs for the <code>com.tresata.matching.core.comparison</code> package.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nIf there exists a Matcher that implements the comparison function you want to use in a Config rule, use it in place of the Compare function.\nIn particular, it&#8217\\;s preferred to use <code>integrated.Same[String]()</code> over <code>comparison.same[String]</code> to avoid having to do any preprocessing (like trimming and lowercasing) more than once per record.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>More settings, particularly those that optimize the formation of groups of signatures based on the scored matching rules, are available in the <code>Config</code> DSL.\nConfiguring these settings may become necessary to run TREE optimally in cases where the matching logic is more complex and the number of rules is increased.\nThese settings are not covered here but can be found in the matching config guide or the <code>Config</code> Scaladocs.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"manual-edges\">9. Manual Edges</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Once the matching logic has been configured correctly and TREE has generated the level-1 graph to the user&#8217\\;s liking, there may remain portions of the graph that have not been resolved quite as well as the user would like.\nIt may be the case that no more modifications can be made to the matching config rules without further damaging the structure of the graph.\nIn these instances TREE allows for the user to make manual edits to the edges of the graph.</p>\n</div>\n<div class=\"paragraph\">\n<p>Sometimes there is only so far we can get in matching given the data quality of the input tables.\nSparsely populated data in fields that are integral for forming relations using the matching logic can lead to the segmentation of an entity into two or more components in the graph.\nIn these cases, adding more matching rules may not be feasible, and overly aggressive matching can lead to separate entities being incorrectly resolved together.\nUsing a <strong>resolution</strong> table, we can manually add edges to connect separate components.</p>\n</div>\n<div class=\"paragraph\">\n<p>Just as matching can result in the segmentation of a graph, we may also find that some portions of the graph suffer from over-connectedness.\nTREE might resolve two or more entities together into the same Tresata ID.\nIf the subgraphs representing two entities are loosely connected by one or more edges, we can add a <strong>bad edge</strong> table to subtract the incorrect edges.</p>\n</div>\n<div class=\"paragraph\">\n<p>Note that the process of finding and adding bad edges is trickier than adding resolutions.\nWe only need to choose two records from different entities to create a resolution that combines two subgraphs.\nWhen we wish to split apart an entity, we must find all the edges that connect the subgraphs by digging through the relations table and running iterations of level-1 matching until we get the separation of entities we desire.\nBreaking up entities is much more difficult than connecting them together, which is why it is better to opt for a more conservative matching logic in level-1.</p>\n</div>\n<div class=\"paragraph\">\n<p>We can define resolution and bad edge tables for any individual tier of matching.\nNote however that the effects of adding a resolution will persist through later hierarchies, but a subtracted edge may need to be utilized in each subsequent tier if necessary.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"resolutions\">9.1. Adding Record Links (Resolutions)</h3>\n<div class=\"paragraph\">\n<p>To add resolutions, we need to create a relations table, insert it into the location for the correct tier of matching, and enable resolutions in the <code>conf/settings.sh</code> file.</p>\n</div>\n<div class=\"paragraph\">\n<p>Below is an example of a resolution table from the demo.\nThere are three required columns that are necessary, <code>pkey1</code>, <code>pkey2</code>, and <code>score</code>.\n<code>pkey1</code> and <code>pkey2</code> contain the pairs of canonical pkeys we wish to connect, and <code>score</code> contains the (positive) weight value to assign the edge.\nA score of <code>1000.0</code> is acceptable to enforce an edge.\nThis table is read alongside the relations that TREE creates automatically to form the edges of the graph.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">pkey1|pkey2|score\nB/table1/ref/XX04|B/table1/ref/XX05|1000.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This table will result in adding one edge to the graph, which TREE will treat the same as edges that are created automatically using the matching logic when constructing the graph.\nThe table goes into a subdirectory of the <code>var</code> path, where intermediate files are stored.\nIf we wish to add this resolution to the primary tier of level-1 matching, it should go into the path <code>var/resolutions/tier=l1/</code>.\nThe table itself can have any name, and can be split across separate files.\nSimilarly, level-1a resolutions should go into <code>var/resolutions/tier=l1a/</code> (and <code>tier=l1b/</code> for level-1b, and so on), and level-2 resolutions should go into <code>var/resolutions/tier=l2/</code>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nFor bad edges and level-1/1a/etc. resolutions, the order of primary keys does not matter.\nFor level-2 resolutions, however, <code>pkey1</code> and <code>pkey2</code> should correspond to the level-2 (source) and level-1 (target) records, respectively.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Finally, if we have at least one resolution table, we need to enable resolutions by setting the following variable in the <code>conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export ENABLE_RESOLUTIONS=true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This is a global setting: once we enable it, each tier of matching will filter for only the relevant resolutions.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"bad-edges\">9.2. Subtracting Edges</h3>\n<div class=\"paragraph\">\n<p>The process for adding and enabling bad edge tables is similar to that of resolutions.\nTo add bad edges, we need to create a relations table, insert it into the location for the correct tier of matching, and enable bad edges in the <code>conf/settings.sh</code> file.</p>\n</div>\n<div class=\"paragraph\">\n<p>Below is an example of a bad edge table from the demo.\nUnlike with resolution tables, only two columns are used, <code>pkey1</code> and <code>pkey2</code>, containing the pairs of canonical pkeys we wish to break apart.\nThis table is used as a filter on the relations that TREE uses to form the edges of the graph.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">pkey1|pkey2\nB/table1/ref/XX03|B/table1/ref/XX04</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The edges represented by this table will be filtered from matching if an edge was formed between them.\nThe table goes into a subdirectory of the <code>var</code> path, where intermediate files are stored.\nIf we want the edges in this table to be filtered out of the primary tier of level-1 matching, it should go into the path <code>var/badEdges/tier=l1/</code>.\nThe table itself can have any name, and can be split across separate files.\nSimilarly, level-1a bad edges should go into <code>var/badEdges/tier=l1a/</code> (and <code>tier=l1b/</code> for level-1b, and so on), and level-2 bad edges should go into <code>var/badEdges/tier=l2/</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If there is at least one bad edge table present, we need to enable bad edges by setting the following variable in the <code>conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export ENABLE_BAD_EDGES=true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This is a global setting: once we enable it, each tier of matching will only filter edges from the relevant subdirectory of the <code>var/badEdges</code> folder.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"hierarchies\">8. Matching Hierarchies</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Sometimes there are multiple notions of \"identity\" that can exist simultaneously in a dataset.\nFor example, a database of customers may resolve naturally into individual people per entity, each person getting assigned their own Tresata ID.\nIt may also be useful, however, to construct an identifier that represents an entity at the family level.\nAnother example one might encounter is the resolution of subsidiaries into parent companies in a corporate transaction database.</p>\n</div>\n<div class=\"paragraph\">\n<p>TREE provides for the possibility that there are multiple levels of entities by supporting additional <strong>hierarchies</strong> of matching, with each tier supplying a level-1 record with an additional Tresata ID.\nThe lowest tier of matching, level-1, which partitions the graph into the most atomic level of entities, runs normally using the familiar <code>bin/tres-link-1</code> script.\nWe refer to this as <strong>tier 1</strong> of matching.\nThe next layer of matching, instead of linking records together to form a graph, forms relations between the Tresata IDs themselves.\nThis is accomplished by using the roll-up table as the prep for the next hierarchy of matching, <strong>tier 1a</strong>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nHierarchical matching consists of one or more tiers of matching above the typical level-1 graph.\nAfter the usual level-1 graph is resolved (tier 1), we can further establish edges between the entities themselves in tier 1a (and again in tier 1b, and so on).\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Implementing an effective tier of matching above the primary tier requires running <a href=\"#roll-up\">roll-up</a> on the results of level-1 matching.\nLevel-1a matching requires a whole new matching pipeline with new matching logic.\nA roll-up config for level-1 should, at the minimum, provide the values for fields that are used in the matching logic for the next tier.\nSince level-1a will connect records in level-1 roll-up together instead of records from prep, <em>matching will fail if we use fields in the matching pipeline that are not created in the previous tier&#8217\\;s roll-up phase</em>.</p>\n</div>\n<div class=\"paragraph\">\n<p>After an additional tier of matching is enabled and matching is complete, output tables will automatically contain multiple Tresata IDs for each record from the input tables.\nIf, for example, each record in an input table receives an additional field containing the Tresata IDs called <code>\"tresataId_ref_l1\"</code>, adding a secondary tier of matching will result in an additional column called <code>\"tresataId_ref_l1a\"</code>, containing the level-1a Tresata IDs.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"level-1a-matching\">8.1. Level-1a Matching</h3>\n<div class=\"paragraph\">\n<p>Once roll-up is configured and run for level-1, we are able to run another tier of matching (level-1a).</p>\n</div>\n<div class=\"paragraph\">\n<p>There is no limit to the number of additional tiers of matching we can run, though typically no more than two additional tiers are used (levels 1a and 1b).\nTREE supplies scripts for level-1a through level-1c matching.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each additional tier of matching requires a separate pipeline file to describe the matching logic.\nThe pipeline for level-1a matching is set in either the file <code>config/l1a_pipeline.scala</code>, or a compiled class <code>$CONFIG_PACKAGE.PipelineL1a</code> available on the classpath (recall that the <code>CONFIG_PACKAGE</code> variable is defined in the <code>conf/settings.sh</code> file).\nHere is the level-1a pipeline for the demo:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">package com.tresata.tree.demo\n\nimport com.tresata.scala.env.Env\n\nimport com.tresata.matching.core.dsl.{Config, Pipeline}\nimport com.tresata.matching.core.integrated\n\nclass PipelineL1a extends Function1[Env, Pipeline] {\n  def apply(env: Env): Pipeline = {\n    val config1 = (Config.init\n      matchOn \"y\" score 2.0 using integrated.Same[String]()\n      matchIfScore 0.99\n      explain true\n    )\n\n    (Pipeline.init\n      resolve (\"A\", \"B\") using config1\n    )\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Since the roll-up configuration for level-1 created the <code>y</code> field, it is available here for use in a matching config in the level-1a pipeline.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nWhen we resolve a source in a pipeline step for a higher tier of matching, we are resolving all Tresata IDs that derive from a record in that source.\nThe Tresata ID may in fact contain records that belong to other sources as well.\nThis is important to keep in mind if you are attempting to resolve sources that were connected in a previous tier separately in the current tier.\nIt&#8217\\;s atypical to have higher-tier pipelines resolve sources in separate steps.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Once a level-1a pipeline is defined and level-1 roll-up has completed, we can run level-1a matching with the command</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-link-1a</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now when output tables are created, two tiers of Tresata IDs will be included, one for level-1 and one for level-1a.\nAny table that undergoes level-2 matching will run against the primary tier (level-1) graph, and records will receive both a level-1 and level-1a Tresata ID.</p>\n</div>\n<div class=\"paragraph\">\n<p>The process for running level-1b matching (and so on) is similar.\nWe specify a level-1a roll-up configuration and level-1b matching pipeline, run <code>bin/tres-roll-up-1a</code> for level-1a roll-up, and run <code>bin/tres-link-1b</code> to complete level-1b matching.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"roll-up\">7. Roll-up Phase</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The roll-up phase of TREE establishes a set of representative values for each Tresata ID defined in level-1 matching.\nA roll-up configuration file specifies how to select these values from the set of values present in the canonical fields over each Tresata ID.\nSee the section titled <a href=\"#roll-up-configs\">Roll-up Rules In-depth</a> for further details on how to customize the roll-up rules in a config file.\nThe (level-1) roll-up rules are defined either in a file <code>config/l1_roll_up_rules.scala</code>, or a compiled class <code>$CONFIG_PACKAGE.RollUpRulesL1</code> available on the classpath (recall that the <code>CONFIG_PACKAGE</code> variable is defined in the <code>conf/settings.sh</code> file).\nHere is an example of a compiled level-1 roll-up config used in the demo:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">package com.tresata.tree.demo\n\nimport com.tresata.scala.env.Env\n\nimport org.apache.spark.sql.functions._\nimport com.tresata.matching.core.dsl.RollUpRules\nimport com.tresata.matching.core.dsl.rules._\n\nclass RollUpRulesL1 extends Function1[Env, RollUpRules] {\n  def apply(env: Env): RollUpRules =\n    (RollUpRules.init\n      score \"name\" -&gt\\; \"name\" using countOf(\"name\")\n      score \"x\" -&gt\\; \"x\" using countOf(\"x\")\n      score \"y\" -&gt\\; \"y\" using countOf(\"y\")\n    )\n}</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nThe <code>Env</code> object is a construct introduced in TREE 7.0.0 that provides the <code>FileAccess</code>, a typeclass to abstract away accessing files on the local filesystem, Hadoop filesystem, or other filesystems, for example in providing model files for the Address matcher.\nIn most cases it can be ignored as just boilerplate code.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>This roll-up config file specifies that roll-up will contain three fields (in addition to pkey): <code>name</code>, <code>x</code>, and <code>y</code>.\nThe value of each is determined by the counts of occurrences of all values in the respective column of prep, per Tresata ID&#8212\\;&#8203\\;the value with the highest count of occurrences for a Tresata ID will be chosen.\nIf ten records from level-1 represent a Tresata ID, and seven of those have the name \"bob\" and three have the name \"bobby\", there will be a record in the roll-up table for that Tresata ID, and the <code>name</code> field will have the value \"bob\".\nThe <code>countOf</code> method is just one of many possible expressions for calculating roll-up values.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is also possible to store a set of multiple values, using the <code>collect</code> keyword.\nFor example, in the following <code>RollUpRules</code> object,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">RollUpRules.init\n  collect \"name\" -&gt\\; \"aliases\" abortIfGreaterThan 5\n  collect \"address\" -&gt\\; \"known_addresses\" top 3 using countOf(\"address\")</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>we store all available <code>name</code> values into <code>aliases</code> (and store none if more than five are found), and the top three most common values for <code>address</code> in <code>known_addresses</code>.\nThe output fields created in this config (<code>aliases</code> and <code>known_addresses</code>) are the fields available for use in the matching logic for the <a href=\"#hierarchies\">next tier of matching</a> (if one exists), as opposed to the original canonical fields.\nNote that there are set-based matchers that are equipped to handle fields consisting of several values.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nInstead of the canonical fields described in the canonical config, the matching logic defined in the <a href=\"#level-1a-matching\">level-1a pipeline</a> uses the fields created by level-1 roll-up.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To run level-1 roll-up, once level-1 matching has completed, run</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-roll-up-1</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The roll-up table serves two purposes in TREE:\n(1) it <em>enriches the Tresata IDs with representative data</em>, which can be joined in later in the <a href=\"#master\">master output</a>, and\n(2) it <em>provides the nodes of the graph for the next tier of matching</em>.\nThe clusters of the level-1 graph that resolve to a single entity in the primary tier become the nodes themselves when creating the relations for level-1a.\nInstead of edges connecting single records on primary keys, we have edges connecting entities on Tresata IDs.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"matching-process\">6. The Matching Process</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The crux of TREE&#8217\\;s entity matching lies in the construction of a graph representing the records from the input tables, and its resolution into clusters that represent singular entities.\nEach table that is processed by TREE connects itself to this graph so that these clusters can each be mapped to a single unique and universal Tresata ID.\nThere are two variations in how we allow tables to attach themselves to the graph: Level-1 and Level-2.\nA source can only participate in either level-1 or level-2 matching.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nIn Twig, only level-1 matching is supported.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The process of matching can be divided into two separate stages:\n(1) the creation of <strong>edges</strong> that link records together in the graph, representing an equivalence relation in identity, and\n(2) the resolution of the resulting graph into clusters that determine how we create <strong>identifiers</strong>, or Tresata IDs.\nLevel-1 and level-2 matching differ slightly in their approach, but both accomplish the resolution of entities through these two stages.</p>\n</div>\n<div class=\"paragraph\">\n<p>In a matching pipeline for level-1, each step describes either an <em>internal</em> or <em>targeted</em> matching step.\nAn <strong>internal</strong> matching step allows for any two records in a set of sources to establish a relation, or edge, between them given the rules in a matching config.\nA <strong>targeted</strong> matching step allows for any record in a set of sources to establish at most one edge to a record in a set of (already resolved) target sources using the rules in a matching config.\nA level-1 source can participate (be resolved) in a combination of internal and targeted matching steps inside a level-1 pipeline.\nLevel-2 sources can only participate in one targeted matching step inside a level-2 pipeline.</p>\n</div>\n<div class=\"paragraph\">\n<p>In level-1 matching, a graph is built from a set of inputs that are designated as being level-1 sources.\nAny two records from these sources that satisfy the matching criteria described in the matching pipeline form an <em>edge</em> in the graph.\nThe resulting graph is divided into clusters and these clusters form <em>identifier</em> groups that receive a unique Tresata ID.\nOnce all the level-1 sources are joined together in this graph, the graph is \"locked\" throughout level-2 matching.\nIn particular, the subgraphs that form the identifier clusters are fully resolved\\; they can no longer become further connected or modified in any way.\nStated differently, two nodes of the level-1 graph are path-connected after level-2 matching if and only if they were path-connected at the conclusion of level-1 matching.</p>\n</div>\n<div class=\"paragraph\">\n<p>Level-2 matching is a process that acts on this \"locked\" level-1 graph.\nTables that undergo level-2 matching are allowed to form up to only one edge per node (record) to connect themselves to the level-1 graph.\nThe effect of this limitation is that level-2 matching does not allow for identifier groups to change.\nBy effectively only adding <strong>leaf nodes</strong> to the graph, the subgraphs that make up each identifier group do not form new connections to each other, and thus remain fixed across the level-1 graph.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA record can form edges to multiple other records in level-1 matching, but a record can form only one edge in level-2 matching.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The justification for two levels of matching has its basis in data quality.\nIn TREE, Level-1 matching is typically reserved for reference data, where the combination of all level-1 data constitutes the entirety of the entities that get resolved.\nHowever, if tables of lesser quality and more loosely populated fields are introduced, it is possible (and often the case) that in order to effectively attach its records to other level-1 data, we establish edges too aggressively and inadvertently connect two separate clusters belonging to different entities together, resolving them to the same entity.\nBy restricting the number of edges formed to one in level-2, we prevent records from altering the connective structure of the entity groups.\nLevel-2 matching&#8217\\;s robustness against polluting the graph&#8217\\;s entities allows for looser matching criteria: by lowering the threshold for establishing an edge, a record is more likely to form a connection to another record, and we only choose the edge with the highest score.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is important to specify a table as level-1 if at all possible.\nThe graph produced by level-1 matching establishes the set of possible Tresata IDs: if an entity is not represented by at least one level-1 record, it will not receive a Tresata ID, and without at least one level-1 record to serve as an anchor for attaching level-2 records to the graph, level-2 records that represent this entity will not get resolved to a Tresata ID.\n<em>A table must be specified as level-1 in order for level-2 tables to form edges to its records</em>.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nIf an entity is not represented by any records from level-1 sources, it will not receive a Tresata ID.\nIt is important to include enough reference data in level-1 to establish the entities we wish to resolve.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>It is worth noting that establishing a single edge per record (\"targeted matching\") is not what distinguishes level-2 matching&#8212\\;&#8203\\;we allow this behavior in level-1 matching as well, on a source-by-source basis.\nWe restrict level-2 by enforcing that we only allow one edge per record, but in level-1 we can also specify that a source only produces single edges per record in the matching pipeline.\nWhat distinguishes level-1 from level-2 is that it <em>creates the graph that establishes the Tresata IDs</em>.\nOnce level-1 matching is complete, level-2 matching works by attaching records to the level-1 graph, absorbing the Tresata ID of the subgraph to which it connects.\nA table that matches using level-2 does not affect level-2 matching for other tables\\; every instance of level-2 matching operates independently by connecting records only to level-1 records.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the graph consisting of edges between level-1 records is created, it is resolved into subgraphs that form Tresata IDs using the <strong>connected components</strong> algorithm.\nAny two nodes that are connected by a path of edges, i.e. belong to the same connected component, are resolved to the same entity.\nThis algorithm has the advantage of being deterministic, but is susceptible to <em>bad edges</em>, or edges that connect two otherwise disconnected subgraphs belonging to different entities.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the level-1 graph is resolved into single-entity subgraphs, a Tresata ID is chosen to represent each group.\nThe Tresata ID is chosen by selecting the canonical primary key that is first in lexicographic order among the records in the subgraph.\nNote that the canonical primary key has the form <code>source/table/entity/pkey</code> (and typically appear hashed in the output).\nThis results in a consistently chosen identifier that persists through reruns of TREE.\nHowever, any time a level-1 table is added or modified and matching is re-run, the Tresata IDs (as well as the entity groups themselves) are subject to change.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nAdding or modifying a level-1 table between runs of TREE can potentially alter the Tresata IDs.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>All level-1 records receive a Tresata ID, including those that do not form any connections to other records (which we refer to as <strong>singletons</strong>).\nTables that participate in level-2 matching assign records the Tresata ID of the level-1 record with which they form a connection (when multiple records satisfy the matching criteria, we choose the edge with the highest score).\nLevel-2 records that do not form any connections are not assigned a Tresata ID, and are referred to as <strong>orphan</strong> records.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"level-1\">6.1. Level-1 Matching</h3>\n<div class=\"paragraph\">\n<p>Level-1 matching builds the graph of entities that determines how Tresata IDs are assigned to each record.\nA <strong>pipeline</strong> file details which sources can be matched together, along with the logic for how to score and form edges.\nA pipeline consists of a series of matching steps that are run through in order.\nThe pipeline for level-1 is defined either in a file <code>config/l1_pipeline.scala</code>, or a compiled class <code>$CONFIG_PACKAGE.PipelineL1</code> available on the classpath (recall that the <code>CONFIG_PACKAGE</code> variable is defined in the <code>conf/settings.sh</code> file).\nAn example of a compiled level-1 pipeline class is shown below.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">package com.tresata.tree.demo\n\nimport com.tresata.scala.env.Env\n\nimport com.tresata.matching.core.dsl.{Config, Pipeline}\nimport com.tresata.matching.core.integrated\n\nclass PipelineL1 extends Function1[Env, Pipeline] {\n  def apply(env: Env): Pipeline = {\n    val config1 = (Config.init\n      piggybackOn \"ssn\" score 100.0\n      matchOn \"first\" score 1.0 using integrated.Jaccard(thres = 0.8)\n      matchOn \"last\" score 1.0 using integrated.Jaccard(thres = 0.8)\n      matchOn \"address\" score 1.0 using integrated.Jaccard(thres = 0.8)\n      matchIfScore 1.9\n    )\n\n    (Pipeline.init\n      resolve (\"a\", \"b\") using config1\n    )\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nThe <code>Env</code> object is a construct introduced in TREE 7.0.0 that provides the <code>FileAccess</code>, a typeclass to abstract away accessing files on the local filesystem, Hadoop filesystem, or other filesystems, for example in providing model files for the Address matcher.\nIn most cases it can be ignored as just boilerplate code.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>In this example we define a single <em>internal</em> matching step (using the <code>resolve ... using ...</code> syntax) that states we wish to resolve the records in sources <code>a</code> and <code>b</code> together using the matching logic outlined in <code>config1</code>, a <code>Config</code> object defined in the file.\nAny pair of records from <code>a</code> and <code>b</code> can be matched together if they satisfy the matching criteria: a match on <code>ssn</code>, or a score of at least 1.9, which amounts to any two of the three Jaccard-based <code>matchOn</code> rules over the <code>first</code>, <code>last</code> and <code>address</code> fields.\nSee the section on <a href=\"#matching-configs\">matching configurations</a> for details on how to configure the matching config itself.</p>\n</div>\n<div class=\"paragraph\">\n<p>We may choose to add another level-1 source, <code>c</code>, where we wish to only form edges from <code>c</code> into records in <code>b</code>--a <em>targeted</em> matching step&#8212\\;&#8203\\;using a different set of rules.\nWe would modify the pipeline object as such:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve \"c\" against \"b\" using config2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Using the <code>resolve ... against ... using ...</code> syntax, we restrict the set of <em>target records</em> for source <code>c</code> to the tables belonging to source <code>b</code>.\nNote that when we use this syntax, the records we are resolving <em>only form one edge to records in the target sources</em>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Let&#8217\\;s say we have a fourth source, <code>d</code>, that we only wish to form edges against <code>c</code>.\nWe can chain another matching step like we did for <code>c</code>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve \"c\" against \"b\" using config2\n  resolve \"d\" against \"c\" using config3</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now, it may be the case that we didn&#8217\\;t want to match any records in <code>d</code> against records in <code>c</code> that hadn&#8217\\;t themselves matched into <code>b</code>.\nPhrased differently, we may want to adjust the resolution of records in <code>d</code> to only form edges against records that are not <em>singletons</em>.\nThis may be useful in cases where we want to ensure that records only connect to other records that have matched against a strong reference dataset.\nWe can adjust the targeted matching step on <code>d</code> by using the <code>withoutSingletonsIn</code> keyword:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve \"c\" against \"b\" using config2\n  resolve \"d\" against \"c\" withoutSingletonsIn \"c\" using config3</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The set of sources following the <code>withoutSingletonsIn</code> keyword must be a subset of the target sources, and it reduces the set of target records to only non-singleton records.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nA source may be resolved in multiple matching steps of a pipeline, but once a record has established at least one edge, it does not participate in future steps.\nI.e. if a source is resolved again, it will only operate on the <strong>singletons</strong> of previous steps in which the source was resolved.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>It&#8217\\;s worth mentioning that <code>Config</code> objects have a <code>standalone</code> setting that can be enabled, where any records that meet the matching criteria when compared against themselves will receive a <em>self-edge</em>.\nThis is useful in instances where we want to restrict the source dataset in a targeted matching step to only include records that weren&#8217\\;t \"matchable\" in previous steps.\nConsider the following Config:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve \"a\" using config1 // standalone mode enabled in config1\n  resolve \"a\" against \"a\" using config2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Enabling standalone mode for the first step has the effect of creating a self-edge for every record in <code>a</code> that meets the matching criteria when compared against itself.\nThen, the singleton records from <code>a</code> (records that didn&#8217\\;t form an edge to either themselves or other records) will participate in the second step, while the target records will include any records in <code>a</code> that formed an edge in the first step&#8212\\;&#8203\\;to either themselves or another record.</p>\n</div>\n<div class=\"paragraph\">\n<p>The example that follows is another situation where standalone mode can help:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve \"a\" using config1 // standalone mode enabled in config1\n  resolve \"b\" against \"a\" withoutSingletonsIn \"a\" using config2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now, instead of limiting the second step to only creating edges between records in \"b\" and records in \"a\" that formed edges to other records in step 1, records in \"b\" can establish edges to records in \"a\" that only formed edges to themselves.</p>\n</div>\n<div class=\"paragraph\">\n<p>It may be the case that we don&#8217\\;t want to allow any records in a set of sources to form any edges, while still allowing them to become targets for future pipeline steps.\nWe accomplish this using the <code>advance ...</code> syntax.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  advance (\"a\", \"b\")\n  resolve \"c\" against (\"a\", \"b\") using config1</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Here we don&#8217\\;t allow any of the records in <code>a</code> and <code>b</code> to match to each other.</p>\n</div>\n<div class=\"paragraph\">\n<p>The advance, internal, and targeted steps form the fundamental blocks of a level-1 pipeline.\nEach step in a pipeline results in the formation of relations between records, and the resolution of identifiers for records that underwent a match.</p>\n</div>\n<div class=\"paragraph\">\n<p>A series of steps can also participate in <strong>relaxed</strong> mode, where a block of matching steps (which themselves form a <em>nested</em> pipeline object) can establish their identifiers simultaneously, once all their relations have been determined individually.\nRelaxed mode allows for more control over how records can establish relations across multiple sources, without having to resort to adding additional tiers of matching.\nFor example, multiple groups of sources can participate in their own internal matching, then be linked together using targeted matching steps, all without having to participate in multiple iterations of matching over extra hierarchies.\nConsider the following example of a pipeline with a relaxed step:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve \"a\" using config1\n  relaxed (Pipeline.init\n    resolve \"i\" using config2\n    resolve \"j\" using config3\n    resolve \"j\" against \"i\" using config4\n    resolve (\"i\", \"j\") externallyAgainst \"a\" using config5\n  )</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In a typical pipeline, Resolving <code>j</code> against <code>i</code> after resolving <code>j</code> internally would mean only connecting the singletons from <code>j</code> into <code>i</code>, since each step in a pipeline assigns identifiers to the records that participate in matching (and the identifiers cannot be changed once the records are resolved).\nThe purpose of relaxed mode is to <em>delay the assignment of identifiers until all steps have completed finding relations</em>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Notice the <code>externallyAgainst</code> syntax in the last relaxed step.\nThis indicates an (optional) <strong>external</strong> resolution step, in which the sources resolved within the nested pipeline form edges against sources resolved outside of the relaxed block (<code>a</code>).\nThis is in contrast to the other steps that involve only sources that were not previously resolved (<code>i</code> and <code>j</code>).\nTREE will first run the non-external steps and find relations involving <code>i</code> and <code>j</code>, then resolve them into <em>temporary</em> identifier groups, then it will find relations for the external step, and finally resolve all the internal identifier groups against records in <code>a</code>.\nUnlike a typical targeted matching step where each record can connect to 0 or 1 records in the target sources, the external step only allows up to one edge per identifier group.\nIn this example, if the first three relaxed steps connected ten records from <code>i</code> and <code>j</code> together, and each record found an edge to a record in <code>a</code>, only one of these edges to <code>a</code> is kept (the highest-scoring edge), and all ten records receive the same Tresata ID connecting to a single record in <code>a</code>.\nThis ensures that all ten records from <code>i</code> and <code>j</code> receive the same Tresata ID.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to matching steps, level-1 pipelines can also specify how the Tresata IDs themselves are selected.\nBy default, the record with the primary key that is first in lexicographic order is used as the Tresata ID.\nIn the previous examples, if a Tresata ID includes records from both <code>a</code> and <code>b</code>, the ID itself will always be a primary key from a record in <code>a</code>.\nThis can be adjusted by specifying a <strong>priority list</strong> for selecting the Tresata ID.\nUsing the <code>prioritize</code> keyword, we can specify the ordering of sources for selecting the Tresata ID:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve \"c\" against \"b\" using config2\n  prioritize (\"b\", \"a\")</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This pipeline will always assign Tresata IDs a representative from the <code>b</code> source if possible, then from <code>a</code>.\nIf sources are not mentioned in the priority list, then they will be given a lesser priority and be resolved lexicographically as usual.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more control over how Tresata IDs are chosen, a series of rules can be specified using the <code>scoreTidUsing</code> syntax.\nThis can include Spark UDFs, as well as rules defined in the roll-up rules DSL:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.matching.core.dsl.rules._\nimport org.apache.spark.sql.functions._\n\nPipeline.init\n  resolve (\"a\", \"b\") using config1\n  resolve \"c\" against \"b\" using config2\n  scoreTidUsing ((\n    prioritize(\"source\", (\"b\", \"a\")),\n    udf{ name: String =&gt\\; if (name != null) 1.0 else 0.0 }.apply(col(\"name\"))\n  ))</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example, priority is first given to records in source \"b\", then source \"a\", then any other sources.\nAfter giving priority to the source, records are then scored using a Spark UDF that assigns one point to records with a defined \"name\" value, and zero otherwise.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once the pipeline file or class is configured, and the level-1 tables are all loaded using the <code>bin/tres-load</code> script (see the <code>bin/run</code> script in the template for an example), we can run level-1 matching using the <code>bin/tres-link-1</code> script.\nThe <code>bin/tres-link-1</code> script runs through all the steps in the level-1 pipeline file and generates Tresata IDs for all sources that are advanced or resolved in the pipeline.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you wish to see the relations that form the edges of the graph, you can enable persisting the relations to a table by adding the following setting to the <code>conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export WRITE_RELATIONS=true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The relations tables will be written to the <code>var</code> subdirectory of the TREE path, in <code>var/l1/relations</code>.\nOnce level-1 matching is complete, the graph that defines the Tresata IDs is locked, and level-2 can (optionally) be run.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_selecting_steps_from_a_pipeline_for_execution\">6.1.1. Selecting Steps from a Pipeline for Execution</h4>\n<div class=\"paragraph\">\n<p>Additionally, a <code>--steps</code> argument can be specified for <code>bin/tres-link-1</code> to run a range of steps (e.g. <code>--steps 2-4</code>) or steps that have not yet completed successfully (<code>--steps _missing_</code>).\nThere are different options available for the <code>--steps</code> argument so that you can run just the steps you want from a pipeline.</p>\n</div>\n<div class=\"hdlist\">\n<div class=\"title\">Syntax and Examples for <code>--steps</code></div>\n<table>\n<tr>\n<td class=\"hdlist1\">\n<code>_all_</code> \n</td>\n<td class=\"hdlist2\">\n<p>Runs all steps</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<code>_missing_</code> \n</td>\n<td class=\"hdlist2\">\n<p>Runs missing steps (i.e. steps with missing output, or those of which have not yet been run)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<code>3-5</code> \n</td>\n<td class=\"hdlist2\">\n<p>Runs:</p>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>steps 3, 4, and 5</p>\n</li>\n<li>\n<p>any prior steps in the pipeline if they resolve sources that are targets in steps 3-5 <strong>and</strong> their identifiers output is missing</p>\n</li>\n</ol>\n</div>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<code>B+</code> \n</td>\n<td class=\"hdlist2\">\n<p>Runs:</p>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>any steps that resolve B</p>\n</li>\n<li>\n<p>any prior steps in the pipeline if they resolve sources that are targets for the above steps <strong>and</strong> their identifiers output is missing</p>\n</li>\n<li>\n<p>any additional steps that target/depend on any sources that were updated in the above steps (this behavior is triggered by the use of the '+' symbol)</p>\n</li>\n</ol>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To prevent leaving the pipeline in an inconsistent state where steps have identifiers that are out-of-date, steps that target any sources getting regenerated will have their identifiers deleted.\nA pipeline is in an inconsistent state when different executions of the pipeline create different identifiers for the same Tresata ID groups.\nConsider the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Pipeline.init\n  resolve \"A\" using config1\n  resolve \"B\" using config2\n  resolve \"C\" against (\"A\", \"B\") using config3\n  resolve \"D\" against \"C\" using config4</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This pipeline has four steps: step 1 resolves A and feeds into/generates identifiers used by steps 3 and 4, step 2 resolves B and feeds into steps 3 and 4, step 3 resolves C and feeds from steps 1 and 2 and into step 4, and step 4 resolves D and feeds from step 1, 2, and 3.</p>\n</div>\n<div class=\"paragraph\">\n<p>If we specify <code>--steps B</code> (equivalent to <code>--steps 2</code>), then step 2 will run, and, to prevent against inconsistent state, the identifier tables from steps 3 and 4 will be wiped (if present), as they have become stale.\nRunning <code>--steps B+</code> will force steps 3 and 4 to be re-run as well, instead of just wiping their identifiers.\nNote that step 1 is not re-run, as its identifiers do not affect step 2.\nIf we instead specify <code>--steps C</code> (or <code>--steps 3</code>), then step 3 will run, steps 1 and 2 will run if they are missing, and the identifiers for step 4 will be wiped (if present) since they are now stale.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nResolving sources can leave later steps with stale identifiers.\nIdentifiers that become out-of-date will automatically be wiped to ensure identifiers are always in a valid state.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"level-2\">6.2. Level-2 Matching</h3>\n<div class=\"paragraph\">\n<p>As with level-1, the logic for matching a level-2 source to the graph is defined in a pipeline file or class.\nUnlike level-1, level-2 sources participate in matching independently, and can only attach to the graph by establishing zero or one edges per record.</p>\n</div>\n<div class=\"paragraph\">\n<p>A level-2 pipeline file consists of a series of <em>targeted</em> matching steps with the <code>resolve ... against ... using ...</code> syntax.\nLevel-2 sources can establish edges against any subset of level-1 sources.\nThe pipeline for level-2 is defined either in a file <code>config/l2_pipeline.scala</code>, or a compiled class <code>$CONFIG_PACKAGE.PipelineL2</code> available on the classpath (recall that the <code>CONFIG_PACKAGE</code> variable is defined in the <code>conf/settings.sh</code> file).\nAn example of a full level-2 pipeline class is shown below.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">package com.tresata.tree.demo\n\nimport com.tresata.scala.env.Env\n\nimport com.tresata.matching.core.dsl.{Config, Pipeline}\nimport com.tresata.matching.core.integrated\n\nclass PipelineL2 extends Function1[Env, Pipeline] {\n  def apply(env: Env): Pipeline = {\n    val level1 = (\"a\", \"b\")\n\n    val config1 = (Config.init\n      matchOn \"ssn\" score 1.1 using integrated.Same[String]()\n      matchOn \"first\" score 1.0 using integrated.Jaccard(thres = 0.8)\n      matchOn \"last\" score 1.0 using integrated.Jaccard(thres = 0.8)\n      matchOn \"zip\" score 0.3 using integrated.Same[String]()\n      matchIfScore 0.9\n    )\n\n    val config2 = config1\n      matchIfScore 1.2\n\n    (Pipeline.init\n      resolve (\"c\", \"d\") against level1 using config1\n      resolve \"e\" against level1 using config2\n    )\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nThe <code>Env</code> object is a construct introduced in TREE 7.0.0 that provides the <code>FileAccess</code>, a typeclass to abstract away accessing files on the local filesystem, Hadoop filesystem, or other filesystems, for example in providing model files for the Address matcher.\nIn most cases it can be ignored as just boilerplate code.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>In this example we define two targeted matching steps that match against all of level-1 (sources <code>a</code> and <code>b</code>).\nThe order of the steps here does not matter\\; level-2 matching steps are executed independently.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nA source can only be resolved in one step of a level-2 pipeline.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Level-2 matching is run using any number of calls to the <code>bin/tres-link-2</code> script.\nThis script supports an optional globbing syntax, in which TREE will run level-2 matching on a subset of tables for a source.\nFor example, in our <code>bin/run</code> script, we might have the following calls:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">bin/tres-link-2 --source c --table 2018_*\nbin/tres-link-2 --source d\nbin/tres-link-2 --source e --table table?</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example, any tables beginning with \"2018_\" that exist for source <code>c</code>, all tables in source <code>d</code>, and any tables in source <code>e</code> with a name beginning with \"table\" followed by one character will run through level-2 matching.\nRemember that all matching steps in a level-2 pipeline run on the same locked level-1 graph.\nFor each call to <code>bin/tres-link-2</code> we only run the relevant matching step from the level-2 pipeline.</p>\n</div>\n<div class=\"paragraph\">\n<p>To see the relations that are generated for a level-2 table, you can enable persisting the relations to a table by adding the following setting to the <code>conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export WRITE_RELATIONS=true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The relations tables will be written to the <code>var</code> subdirectory of the TREE path, in <code>var/relations2</code>.\nOnce level-2 matching is complete for a table, an identifiers table is written that maps each record in the table to a Tresata ID.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"canonical-mapping\">5. Canonical Mapping</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The first step in configuring TREE is defining how to map the input sources into a canonical format.\nThis canonical format provides a standard set of columns that all sources can adhere to, so that all input records can be read and reasoned on together.\nA <a href=\"#matching-configs\">matching configuration</a> describes the logic of forming edges between records using these canonical fields.</p>\n</div>\n<div class=\"paragraph\">\n<p>In a canonical config, a JSON configuration file, the <code>canonical</code> object array states what these fields will be.\nAlong with the <code>canonical</code> array, a canonical config requires a <code>sources</code> map, which describes how to map the columns in an input table into the canonical fields.\nIt may be the case that there are two (or more) entities per record in the input table.\nInside each value of the <code>sources</code> map is another map, <code>entities</code>, that lists the entities per record.\nIf there is only one entity per record, the convention is to name the entity <code>ref</code>.\nThe entry in the <code>sources</code> map should also state the primary key field (<code>pkey</code>) along with the <code>entities</code> map.</p>\n</div>\n<div class=\"paragraph\">\n<p>Here is a simple canonical config containing one source, named \"transactions\", which contains a buyer and a seller for each record in the table, and a primary key named \"txn_id\".\nNote that not all the canonical fields will necessarily have an analogous column in an input table.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{\n  \"canonical\": [ \"firstName\", \"lastName\", \"address\", \"city\", \"zip\", \"phoneNumber\" ],\n  \"sources\": {\n    \"transactions\": {\n      \"pkey\": \"txn_id\",\n      \"entities\": {\n        \"buyer\": {\n          \"mappings\": {\n            \"firstName\" : { \"fields\": [ \"first_l\" ] },\n            \"lastName\" : { \"fields\": [ \"last_l\" ] },\n            \"address\" : { \"fields\": [ \"addr_l\" ] },\n            \"city\" : { \"fields\": [ \"first_l\" ] },\n            \"zip\" : { \"fields\": [ \"zip_l\" ] }\n          }\n        },\n        \"seller\": {\n          \"mappings\": {\n            \"firstName\" : { \"fields\": [ \"first_r\" ] },\n            \"lastName\" : { \"fields\": [ \"last_r\" ] },\n            \"phoneNumber\" : { \"fields\": [ \"phone_r\" ] }\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The \"transactions\" entry in <code>sources</code> will be applied to any number of tables that are designated as being from the \"transactions\" source.\nEach record in the input table will be mapped to two separate records in the canonical table: one for the buyer and one for the seller.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nEach record that is created in the canonical tables, or <strong>prep</strong>, will have a new canonical primary key of the form <code>source/table/entity/pkey</code>, where pkey is the original table&#8217\\;s primary key.\nThese canonical primary keys will later become the Tresata IDs.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The values in the <code>mappings</code> object can contain multiple entries in the <code>fields</code> array.\nThese values are coalesced to create the canonical value\\; if the first field is null/empty, the value of the second field is used, and so on.</p>\n</div>\n<div class=\"paragraph\">\n<p>Typically, having different sources of input data signifies different ETL standards and quality of data.\nThe canonical config can also include information for how to scrub each individual input field.\nThe <a href=\"#scrubbers\">Scrubbers</a> section details the addition of scrubbing functions to the canonical config, as well as adding filters for values in the field.</p>\n</div>\n<div class=\"paragraph\">\n<p>The canonical configuration can also be split across multiple files.\nThe <code>canonical_config.json</code> is required and must contain a <code>canonical</code> and <code>sources</code> object, but the entries inside of <code>sources</code> can be placed into individual files.\nIf the \"transactions\" source entry were to be moved into a separate file, the following JSON data would need to be found in the file <code>canonical_config.d/transactions.json</code>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{\n  \"pkey\": \"txn_id\",\n  \"entities\": {\n    \"buyer\": {\n      \"mappings\": {\n        \"firstName\" : { \"fields\": [ \"first_l\" ] },\n        \"lastName\" : { \"fields\": [ \"last_l\" ] },\n        \"address\" : { \"fields\": [ \"addr_l\" ] },\n        \"city\" : { \"fields\": [ \"first_l\" ] },\n        \"zip\" : { \"fields\": [ \"zip_l\" ] }\n      }\n    },\n    \"seller\": {\n      \"mappings\": {\n        \"firstName\" : { \"fields\": [ \"first_r\" ] },\n        \"lastName\" : { \"fields\": [ \"last_r\" ] },\n        \"phoneNumber\" : { \"fields\": [ \"phone_r\" ] }\n      }\n    }\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>See the canonical configuration files in the demo for an example of splitting the configuration across individual files per source.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scrubbers\">5.1. Scrubbers</h3>\n<div class=\"paragraph\">\n<p>The canonical config provides a means for adding scrubbing functions to better transform the input columns into a uniform format.\nThe user can call built-in scrubbing functions as well as specify their own filters or <a href=\"https://en.wikipedia.org/wiki/Regular_expression\">reg-ex</a> rules.\nIn addition to calling cleaning functions within the canonical config, when TREE converts the staging tables into prep tables, it calls a custom cleaning function for that source if there is one present in the <code>cleaners</code> directory or compiled to a class available on the classpath.\nSee the section on <a href=\"#custom-cleaning\">custom cleaning</a> for more information.</p>\n</div>\n<div class=\"paragraph\">\n<p>The canonical config can be read as a <code>CleaningConfig</code> object, and the <code>mappings</code> object contains the filters and scrubbers to apply to each source.\nBelow is a sample canonical configuration file with cleaning defined over an \"accounts\" source.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{\n  \"canonical\": [ \"name\", \"account_num\" ],\n  \"sources\": {\n    \"accounts\": {\n      \"scrubAllNulls\": false,\n      \"trimLowerAll\": false,\n      \"entities\": {\n        \"ref\": {\n          \"mappings\": {\n            \"name\": {\n              \"fields\": [ \"CUST_NAME\", \"SECONDARY_NAME\" ],\n              \"filters\": [ \"n/a\", \"not found\" ],\n              \"scrubbers\": [\n                { \"type\": \"trimLower\" },\n                { \"type\": \"regexReplace\", \"regex\": \"\\\\s+\", \"replacement\": \" \" }\n              ]\n            },\n            \"account_num\": {\n              \"fields\": [ \"ACCT\" ],\n              \"scrubbers\": [\n                { \"type\": \"null\" }\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>scrubAllNulls</code> flag is a shortcut to automatically filter out any values in all (non-primary key) input fields that are equivalent to one of a variety of null representations, after trimming whitespace and lowercasing.\nSimilarly, the <code>trimLowerAll</code> flag enables lowercasing and trimming whitespace for all encountered values.\nBoth are true by default.\nNull scrubbing and trimming whitespace/lowercasing takes place before any other scrubbing or filtering defined in the config.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each canonical field has a <code>FieldMapping</code> defined inside <code>\"mappings\"</code> for each entity per row of the original staging table.\nThe <code>FieldMapping</code> object details which fields to draw the canonical value from, any filters to apply to values that are encountered, and any internal scrubbing routines to apply to the data.\nFirst, any values in the columns described by the <code>fields</code> array are filtered out if they match the values in the <code>filters</code> array after trimming whitespace and lowercasing.\nThen, any functions present in the <code>scrubbers</code> object are applied in sequence to the results.\nThe fields are then coalesced if there are multiple entries defined in <code>fields</code>, and the result is the canonical value for that entity.\nAdditionally, a field mapping can set <code>\"fallback\": true</code>, and the scrubbed values will be coalesced along with the non-scrubbed values as a fallback for when the value is filtered out by the scrubbers.</p>\n</div>\n<div class=\"paragraph\">\n<p>In the above example, the \"CUST_NAME\" and \"SECONDARY_NAME\" fields are first filtered for any values that are equal to \"n/a\" or \"not found\" after trimming whitespace and lowercasing, and then undergo two scrubbing functions.\nThe first has type \"trimLower\", which trims whitespace and converts the result to lowercase.\nValues then go through a regular expression replacement function in the second scrubber, where any instances of multiple spaces (<code>\\\\s+</code>) are replaced with a single space.\nThe result of the scrubbed \"CUST_NAME\" column is then sent to the canonical \"name\" field, unless it is null, in which case the scrubbed result of \"SECONDARY_NAME\" is used.\nThe \"ACCT\" column is cleaned using the built-in null scrubber and the result becomes the canonical \"account_num\" field.</p>\n</div>\n<div class=\"paragraph\">\n<p>Below we list the scrubber types that can be included in the <code>scrubbers</code> object, some of which can take in additional parameters.\nThe format for adding a scrubber is</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"json\">{ \"type\": \"&lt\\;scrubber-type&gt\\;\", \"parameter1\": value, \"parameter2\": value, ... }</code></pre>\n</div>\n</div>\n<div id=\"scrubber-types\" class=\"hdlist\">\n<div class=\"title\">Scrubber types</div>\n<table>\n<tr>\n<td class=\"hdlist1\">\npassThrough \n</td>\n<td class=\"hdlist2\">\n<p>Passes all values along</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnull \n</td>\n<td class=\"hdlist2\">\n<p>Filters out any string representations of null, including <code>null</code> and <code>\\\\n</code></p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntrimLower \n</td>\n<td class=\"hdlist2\">\n<p>Trims leading and trailing whitespace and lowercases the output</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsymbol \n</td>\n<td class=\"hdlist2\">\n<p>Removes punctuation and non alpha-numeric symbols</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nregexFilter \n</td>\n<td class=\"hdlist2\">\n<p>Returns the first instance of the specified regular expression in the input.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>regex</code>: Reg-ex string for which to search.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nregexReplace \n</td>\n<td class=\"hdlist2\">\n<p>Replaces instances of the specified regular expression with the replacement string.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>regex</code>: Reg-ex string to replace\\; <code>replacement</code>: substituted string.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsetFilter \n</td>\n<td class=\"hdlist2\">\n<p>Only accepts strings from a set of valid values.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>validValues</code>: (optional) array of valid strings to filter for\\; <code>url</code>: (optional) fully qualified path to file containing valid values (one-per-line).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsetFilterNot \n</td>\n<td class=\"hdlist2\">\n<p>Rejects strings from a set of invalid values.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>invalidValues</code>: (optional) array of invalid strings to filter out\\; <code>url</code>: (optional) fully qualified path to file containing invalid values (one-per-line).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nor \n</td>\n<td class=\"hdlist2\">\n<p>Returns the value after applying the first scrubber in the given list that returns a valid value.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>list</code>: array of scrubbers.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nand \n</td>\n<td class=\"hdlist2\">\n<p>Returns the value after applying all scrubbers in the given list in sequence.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>list</code>: array of scrubbers.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nmap \n</td>\n<td class=\"hdlist2\">\n<p>Replaces a set of substrings with their specified substitutions.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>values</code>: (optional) map of strings to their desired substitutions.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nsubstring \n</td>\n<td class=\"hdlist2\">\n<p>Extracts a substring of the input by specifying the number of characters to keep or drop from either end, or an index range of characters to keep.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>take</code>: (optional) number of characters to take from left\\; <code>takeRight</code>: (optional) number of characters to take from right\\; <code>drop</code>: (optional) number of characters to drop from left\\; <code>dropRight</code>: (optional) number of characters to drop from right\\; <code>startIndex</code>: (optional) index to begin extracting characters\\; <code>endIndex</code>: (optional) index to finish extracting characters.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nint \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid integers</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nfloat \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid decimal numbers</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nphone \n</td>\n<td class=\"hdlist2\">\n<p>Accepts phone numbers from the specified region, validating numbers if specified.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>region</code>: string identifying region for possible number formats (default \"US\")\\; <code>validate</code>: validates numbers when set to true (default false).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nemail \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid email addresses</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nstate \n</td>\n<td class=\"hdlist2\">\n<p>Maps commons names and abbreviations of U.S. states and territories to their 2-letter ANSI code.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nzip \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid zip codes, returning the first 5 digits.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>zip4</code>: when set to true, returns the final four digits if present (default false).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nipv4 \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid IPv4 Internet addresses</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nssn \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid social security numbers, and strips away non-numeric characters</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ntin \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid tax identification numbers.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>tinType</code>: type of tax id, possible values are \"all\" (default), \"employer\", \"individual\", \"preparer\", and \"adoption\".</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ndate \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid date-time formatted strings using any of the given formats, outputting to a specified format.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>patterns</code>: array of date-time formats to parse\\; <code>canonical</code>: output date-time format.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ndateRange \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid date-time formatted strings that fit within the given range, using the <code>yyyy-MM-dd</code> format.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>startDate</code>: (optional) earliest acceptable date\\; <code>endDate</code>: (optional) latest acceptable date.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncc \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid credit-card numbers of 13-19 digits with optional hyphens and spaces, and returns the number alone after verifying the check-digit</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nupc \n</td>\n<td class=\"hdlist2\">\n<p>Accepts GTIN-format product codes (including UPC and EAN) from 2-13 characters without check-digits, as well as 14-character product codes with a check-digit, and returns the UPC number with check-digit removed</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nurl \n</td>\n<td class=\"hdlist2\">\n<p>Returns the input string if it can be parsed as a valid URL using <code>java.net.URL</code></p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ndomain \n</td>\n<td class=\"hdlist2\">\n<p>Returns a fully qualified domain, if it exists, of the URL parsed from the input string.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>parent</code>: when set to true, returns the parent domain only, if it exists (default false).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\naddress \n</td>\n<td class=\"hdlist2\">\n<p>Returns the address line in a canonical format, not including city/zip/country.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>threshold</code>: minimum confidence value between 0.0 and 1.0 (default 0.5)\\; <code>model-type</code>: type of entity recognition model to use, either \"mallet\" (default) or \"factorie\"\\; <code>mailable</code>: when set to true, preserves the input order of street types and directionals (default false).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncountry \n</td>\n<td class=\"hdlist2\">\n<p>Maps common names and abbreviations for countries to their ISO 3166-1 alpha-3 or alpha-2 code designation.\n<em>Parameters</em>:&#8201\\;&#8212\\;&#8201\\;<code>alpha</code>: number of characters for country code, either 2 (default) or 3.</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncountryLatLong \n</td>\n<td class=\"hdlist2\">\n<p>Maps country codes to comma-separated decimal latitude and longitude values</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nfirstnameGender \n</td>\n<td class=\"hdlist2\">\n<p>Returns the gender (\"m\"/\"f\") of the first name if found with at least 95% certainty</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncompanyName \n</td>\n<td class=\"hdlist2\">\n<p>Cleans up company names by removing legal and corporate structure related parts of the name.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>aggressive</code>: when set to true, common activities (construction, consulting, investing) are also removed from the company name (default false)\\; <code>threshold</code>: minimum confidence value between 0.0 and 1.0 (default 0.5)\\; <code>model-type</code>: prediction model type, either \"rule\" (default), \"mallet\", or \"factorie\".</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnameTitleGender \n</td>\n<td class=\"hdlist2\">\n<p>Returns the gender (\"m\"/\"f\") for a given name title (\"mr\", \"mrs\", etc.)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nnameGenerationSuffix \n</td>\n<td class=\"hdlist2\">\n<p>Returns the generation number (\"1\", \"2\", &#8230\\;&#8203\\;) for the given suffix (\"sr\", \"jr\", \"iv\", etc.)</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nbic \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid 8 or 11 digit Business Identifier Codes.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>bicType</code>: component to return for a parsed BIC number, possible values include \"full\" (default), \"country\" (digits 5 and 6), and \"branch\" (digits 9-11).</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ngfcid \n</td>\n<td class=\"hdlist2\">\n<p>Accepts valid Global Finance Customer Identification numbers</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\ncredit \n</td>\n<td class=\"hdlist2\">\n<p>Rejects known invalid credit IDs</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nclass \n</td>\n<td class=\"hdlist2\">\n<p>References a Scrubber class available on the classpath.\n<em>Parameters</em>&#8201\\;&#8212\\;&#8201\\;<code>name</code>: path to a compiled class that extends <code>Scrubber</code>, available on the classpath.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"custom-cleaning\">5.2. Custom Cleaning</h3>\n<div class=\"paragraph\">\n<p>Although the canonical config allows for powerful and flexible expressions of scrubbing methodologies for each individual field, you may find the need for a more expressive cleaning procedure.\nFor this purpose we allow for manual cleaning scripts that describe a <code>DataFrame</code> to <code>DataFrame</code> transformation.\nTREE looks in two locations for a cleaner to apply to a source.\nIt first looks for a scala file <code>&lt\\;source&gt\\;_cleaner.scala</code> containing a <code>DataFrame =&gt\\; DataFrame</code> function, found in the directory specified by the <code>CLEANER_DIR</code> variable in <code>conf/settings.sh</code> (the default location is the <code>cleaners/</code> directory).\nIf that file is not found, TREE will search the classpath for a <code>Cleaner&lt\\;source&gt\\;</code> compiled class inside the package specified by the <code>CONFIG_PACKAGE</code> variable in <code>conf/settings.sh</code> (the default package is <code>com.tresata.customer.project</code>, as defined in the <code>libexec/defaults.sh</code> file).\nRecall that the <code>bin/tres-package</code> script will compile classes in the <code>package/</code> directory into a JAR file inside <code>lib/</code>, and add it to the classpath.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nCustom cleaning routines will be applied <em>before</em> any of the cleaning functions that are encoded in the canonical config are applied (but after the effect of <code>scrubAllNulls</code> and <code>trimLowerAll</code>).\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>A simple example of a custom-cleaning script (in a file called <code>A_cleaner.scala</code>) is shown below.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">cleaners/A_cleaner.scala</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import org.apache.spark.sql.DataFrame\nimport com.tresata.spark.sql.Dsl._\n\n(df: DataFrame) =&gt\\; df.fapi\n  .map(\"first\" -&gt\\; \"first\"){ s: String =&gt\\; if (s != null) s.toLowerCase else null }\n  .df</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The same cleaner placed inside the package directory would look as follows:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">package/com/tresata/customer/project/CleanerA.scala</div>\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">package com.tresata.customer.project\n\nimport org.apache.spark.sql.DataFrame\nimport com.tresata.spark.sql.Dsl._\n\nclass CleanerA extends Function1[DataFrame, DataFrame] {\n  def apply(df: DataFrame): DataFrame = df.fapi\n    .map(\"first\" -&gt\\; \"first\"){ s: String =&gt\\; if (s != null) s.toLowerCase else null }\n    .df\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The demo contains examples of packaged classes that are compiled and added to the classpath, including custom cleaner scripts.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"project\">4. Configuring a Custom TREE Application</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>A typical deployment of TREE will involve unpacking the <code>tresata-doc-tree-9.1.0-SNAPSHOT-dist.tar.gz</code> tarball archive into a directory in <code>/opt/tresata/</code>.\nThis installation folder will contain the bin scripts that run the components of tree, as well as the tree assembly jar and client-wide configuration files.\nThe installation folder will usually be closed off from write access.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nThe <code>template</code> folder inside the unpacked installation directory serves as a template for configuring your own application of TREE.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>When you are creating and configuring your own instance of TREE, you will need to copy the <code>template</code> directory (which is configured to run a workable demo) from the installation folder into a new location, such as your home.\nInside you will find five directories that we discuss here in more detail, along with a copy of the data folder used for the demo (<code>tree-demo-data</code>).</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nThe first step to configuring your custom TREE application is modifying the paths and package names found in <code>conf/settings.sh</code>.\nSee the <a href=\"#conf\">conf</a> section for details.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"bin\">4.1. bin/ Scripts</h3>\n<div class=\"paragraph\">\n<p>In the <code>bin/</code> directory you&#8217\\;ll find two scripts, <code>run</code> and <code>run-flex</code>.\nThe bin scripts invoke a series of TREE commands that run the various components of TREE and create the output tables.</p>\n</div>\n<div class=\"paragraph\">\n<p>When configuring your own instance of TREE, you can use the <code>bin/run</code> file as a template script for running your TREE jobs.\nIf you are using flex mode (see the manual on running flex mode in TREE), you can use the commands outlined in <code>bin/run-flex</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>The first step in the demo involves a call to <code>bin/tres-package</code>.\nThis script compiles all the code in the <code>package/</code> subdirectory into a jar in the <code>lib/</code> folder, and makes it accessible on the classpath for TREE.\nA call to <code>bin/tres-init</code> establishes the directory on HDFS that will store all intermediate files produced by TREE along with the correct permissions.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each input table has a table name and belongs to a particular source as defined in the canonical configuration file.\nA table must be loaded using the <code>bin/tres-load</code> script, which will load the table into prep, where it will participate in one or more <a href=\"#matching-process\">matching</a> processes, as well as possible <a href=\"#roll-up\">roll-up</a> runs.\nThe <code>bin/tres-load</code> script requires an argument <code>--source &lt\\;name-of-source&gt\\;</code> at the minimum, referencing the name of a source that has been configured in the <a href=\"#canonical-mapping\">canonical mapping</a>, and optionally a <code>--table &lt\\;glob-patterns&gt\\;</code> argument that accepts a <a href=\"https://en.wikipedia.org/wiki/Glob_(programming)\">glob pattern</a> that selects a set of tables to run that belong to that source.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <code>bin/tres-full-ids</code> command is run after all matching steps have completed, and generates the master table of level-1 Tresata IDs.\nThe resulting Tresata IDs can be joined back into the staging table with the <code>bin/tres-output</code> command, and optionally <a href=\"#master\">master</a> tables can be created by calling <code>bin/tres-master</code>.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"conf\">4.2. conf/ Configuration Settings</h3>\n<div class=\"paragraph\">\n<p>In the <code>conf/</code> directory you&#8217\\;ll find three configuration files.\nEditing these files (primarily <code>conf/settings.sh</code>) is typically the first step in configuring your own TREE application.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <code>conf/settings.sh</code> file is where you define the paths used by TREE, along with other settings such as source formats.\nFor most settings, the defaults will suffice.\nHowever, it&#8217\\;s required that you set the following three settings:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export INSTALL_DIR=/opt/tresata/tresata-tree-tresata-doc-tree-9.1.0-SNAPSHOT\nexport TREE_PATH=\"tree-demo-data\"\nexport CONFIG_PACKAGE=com.tresata.tree.demo</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>INSTALL_DIR</code> variable should be set to the location where TREE is unpacked and installed, which by default is inside of <code>/opt/tresata/</code>.\nThe <code>TREE_PATH</code> is where all the data used by TREE (from staging files to output, and any intermediate files in between) will reside on HDFS.\nThe <code>CONFIG_PACKAGE</code> is the package where all the classes that establish the pipelines, roll-up rules, and source cleaners will be defined.</p>\n</div>\n<div class=\"paragraph\">\n<p>It&#8217\\;s also required that you disable the bad-edge and resolutions tables unless you decide to use them.\nThey are enabled in the template for use in the demo, but they should only be enabled when these tables are present.\nFor a new TREE application, set the following variables to <code>false</code> in the <code>conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export ENABLE_BAD_EDGES=false\nexport ENABLE_RESOLUTIONS=false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The defaults for any of the settings that can be defined in this file are found in the <code>libexec/defaults.sh</code> file in the installation directory.\nTo re-assign a setting, use an <code>export</code> expression.\nFor example, to change the source type for the prep tables to bsvu, add the expression <code>export PREP_SOURCE_TYPE=bsvu</code>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nIf the bottom two lines of <code>conf/settings.sh</code> are left uncommented, TREE will run in local mode.\nInstead of reading <code>TREE_PATH</code> from HDFS (with paths relative to the user&#8217\\;s home directory), <code>TREE_PATH</code> will point to a location on the local filesystem (with paths relative to the project directory).\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The <code>conf/application.conf</code> file is a place to configure settings that tune the behavior of the different components of TREE.\nThis file can typically be left alone.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <code>log4j2.properties</code> can be modified to configure the logging level for any classes used in TREE.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"config\">4.3. config/ Configuration Files</h3>\n<div class=\"paragraph\">\n<p>The <code>config/</code> directory houses the <a href=\"#canonical-mapping\">canonical configuration</a> files.\nThese files state how to map records from each source to a set of canonical fields used by TREE in the matching process.\nIt&#8217\\;s comprised of a set of JSON files that are stitched together to define the mappings for each source.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to the JSON configuration files, there is also an optional directory containing YAML configuration files for level-2 sources.\nThese configurations are used in implementations that use the Tresata Integration Kit ('TIK') to allow for users at the client to define their own matching logic.\nTIK is outside the scope of this document, and more information on its configuration can be found in the tree tik guide on the internal bookmarks page.\nIf TIK is not being used, the <code>tik/</code> directory can be removed.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lib\">4.4. lib/ Jar Files</h3>\n<div class=\"paragraph\">\n<p>The <code>lib/</code> directory houses any jar files that need to be made available to TREE processes.\nThis typically only includes the <code>config.jar</code> file that is created by the <code>tres-package</code> script, which compiles any classes found in the <code>package/</code> directory.\nAny jar files included in the <code>lib/</code> folder will be included on the classpath for TREE, and so any time changes are made to the pipelines, roll-up files, or custom cleaning classes, you should re-run <code>tres-package</code> to regenerate the configuration jar in <code>lib/</code>.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"package\">4.5. package/ Scala Classes</h3>\n<div class=\"paragraph\">\n<p>The <code>package/</code> directory contains any Scala files that you wish to compile and make available for use in TREE.\nIn particular, TREE will look for three types of classes: pipeline files, roll-up rules, and custom source cleaners.\nAny classes that are added to this directory should have a package declaration that is consistent with the <code>CONFIG_PACKAGE</code> variable in <code>conf/settings.sh</code>.\nAny matching or roll-up steps will automatically use the appropriate classes for pipelines and roll-up rules.\nCleaner classes (named as <code>Cleaner&lt\\;source-name&gt\\;</code>) are optional and will be used if they are found.\nCustom cleaners take effect as the first step in the loading process, before the mapping and scrubbing defined in the canonical config is performed.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-tree\">4.6. Running TREE</h3>\n<div class=\"paragraph\">\n<p>The TREE steps are run using the <code>bin/run</code> script (or <code>bin/run-flex</code> if flex mode is used).\nBefore TREE can be run, the input tables must be placed inside the staging directory in the location specified by the <code>TREE_PATH</code> variable in <code>conf/settings.sh</code>.\nThe staging folder requires a subdirectory format similar to what&#8217\\;s found in the demo (e.g, <code>staging/source=A/table=20200101/</code>) that&#8217\\;s based on the source and table/partition names.\nThe output and any intermediate files can be found in the <code>TREE_PATH</code> as well.\nThe <code>var/</code> directory contains the intermediate files (including the relations and identifiers that constitute the generated graph) as well as any <a href=\"#manual-edges\">manual edge tables</a>, and the output and master output can be found in the <code>output</code> and <code>master</code> directories.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"demo\">3. Running the TREE Demo</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The TREE tarball package includes a runnable demo that performs entity resolution on a few basic input files.\nRun the demo to familiarize yourself with the bash scripts that run the different components of TREE.</p>\n</div>\n<div class=\"paragraph\">\n<p>After downloading the tarball distribution, unpack the archive</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">$ tar -xvzf tresata-doc-tree-9.1.0-SNAPSHOT-dist.tar.gz</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>and <code>cd</code> into the <code>tresata-doc-tree-9.1.0-SNAPSHOT</code> directory.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you wish to run the demo in local mode, which will execute much faster than on HDFS, uncomment the following two lines in the <code>template/conf/settings.sh</code> file:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"bash\">export HADOOP_CONF_DIR=local-conf\nexport ADD_SPARK_OPTS=\"${ADD_SPARK_OPTS} --conf spark.eventLog.enabled=false --master local\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>If you are running the demo on HDFS, you must first copy the TREE data folder, <code>tree-demo-data</code>, to your home directory on HDFS.</p>\n</div>\n<div class=\"paragraph\">\n<p>Executing the <code>bin/run-demo</code> script will run the demo end-to-end.\nRunning in local mode will write the output tables to the <code>tree-demo-data</code> folder inside the root package directory, and running on HDFS will write output to the <code>tree-demo-data</code> folder in your home directory on HDFS.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"io\">2. Inputs and Outputs of TREE</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>TREE operates on a series of input datasets with no restrictions on size or number of columns.\nThe input tables must be standardized on their input format, typically in bar or comma separated value files or avro tables.\nThe only column required for each table is a field that contains a valid, unique (within each table) primary key.\nWe refer to this original dump of tables as <strong>staging</strong>.</p>\n</div>\n<div class=\"paragraph\">\n<p>The first step of TREE is the cleaning and canonicalization phase, which consists of first scrubbing the data inside of columns of interest into desired standard formats, and then mapping these columns to a list of <strong>canonical fields</strong>.\nThe methodology for cleaning and mapping input columns to these canonical fields is specified in a JSON file we refer to as a <a href=\"#canonical-mapping\"><strong>canonical config</strong></a>.\nThe output of this phase of TREE is referred to as <strong>prep</strong> and it is on prep that we apply the record linking and clustering routines.\nIn many tables, such as those that describe payments between a beneficiary and ordering customer, there are two or more <em>entities</em> represented in each row.\nThe canonical config describes how to map each entity to a single record in prep.</p>\n</div>\n<div class=\"paragraph\">\n<p>The next phase of TREE is responsible for creating links between any pair of records from prep that are deemed equivalent by a set of rules described in a <a href=\"#matching-configs\"><strong>matching configuration</strong></a>.\nThese links between two records are referred to as <strong>relations</strong> and form the edges of a <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\">graph</a> representing the input data.\nThe resulting graph is then divided into connected (or loosely connected) components that each represent a single entity.\nEach of these entity groups are mapped to unique global <strong>identifiers</strong>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nThroughout this document we refer to the universal entity identifier keys, the principal output of the entity resolution process, as <strong>Tresata IDs</strong>.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Identifiers are joined back into the original staging tables as a single column (or multiple, for tables containing more than one entity per line) to form the <strong>output</strong> tables.\nAdditionally, a roll-up phase can take place that maps each entity group to a set of representative values for each of the canonical fields (for example, use the most commonly encountered name for an entity).\nRules for assigning these values are specified in a <a href=\"#roll-up-configs\"><strong>roll-up rules</strong></a> configuration file.</p>\n</div>\n<div class=\"paragraph\">\n<p>Finally a <strong>master</strong> data file can be created for each staging table that outputs a customized subset of fields from the original table, scrubbed prep table, and roll-up table(s).</p>\n</div>\n<div class=\"paragraph\">\n<p>The file types for the input and output tables are bar-separated values by default, and delta or parquet for internal and temporary tables.\nThese can be changed by modifying the values of the <code>SOURCE_TYPE</code> variables in <code>conf/settings.sh</code> with an <code>export</code> expression.\nYou can see the default file types for each table in the <code>libexec/defaults.sh</code> file.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
" \n<h2 id=\"purpose\">1. Purpose of TREE and Record Linkage</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The aim of TREE is to provide a highly effective and automated approach to the <a href=\"https://en.wikipedia.org/wiki/Record_linkage\">Record Linkage</a> problem.\nTREE can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources.\nEntity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.</p>\n</div>\n<div class=\"paragraph\">\n<p>TREE contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"advantages\">1.1. Advantages of TREE</h3>\n<div class=\"paragraph\">\n<p>TREE handles the mapping of input data to a canonical format that facilitates entity-matching across tables that may come from different data warehousing systems and ETL processes, and links records using techniques that can handle the scale of data typically seen in real-world applications.\nSome of the advantages that set TREE apart from other record linkage solutions on the market include:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Easily customizable and flexible data cleaning and canonicalization process defined for each data source</p>\n</li>\n<li>\n<p>Highly scalable record linking and graph-based algorithms for identifying entity groups</p>\n</li>\n<li>\n<p>Support for multiple identifier hierarchies (e.g. person and family level IDs)</p>\n</li>\n<li>\n<p>Processes for configuring representative records for each identifier group</p>\n</li>\n<li>\n<p>Feedback loop for feeding manual edges into future iterations</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"<h2 id=\"purpose\">1. Purpose of TREE and Record Linkage</h2>\n\t<div class=\"sectionbody\">\n\t<div class=\"paragraph\">\n\t<p>The aim of TREE is to provide a highly effective and automated approach to the <a href=\"https://en.wikipedia.org/wiki/Record_linkage\">Record Linkage</a> problem.\n\tTREE can be deployed on collections of large datasets that may lack any form of common identifier or database join key, but contain useful identifiable metadata, with the goal of creating a set of global identifiers that will connect the same entity within and across various data sources.\n\tEntity resolution has applications across several industries including health care, retail, and banking, to name a few, where there is value in identifying records that represent the same person, family unit, or corporation across data sources that stem from different data cleansing or ETL methodologies.</p>\n\t</div>\n\t<div class=\"paragraph\">\n\t<p>TREE contains components that scrub and canonicalize data across different sources, build large-scale graph structures to represent connections across records, and form a persistent set of global identifiers that identify entities across all data sources.</p>\n\t</div>\n\t<div class=\"sect2\">\n\t<h3 id=\"advantages\">1.1. Advantages of TREE</h3>\n\t<div class=\"paragraph\">\n\t<p>TREE handles the mapping of input data to a canonical format that facilitates entity-matching across tables that may come from different data warehousing systems and ETL processes, and links records using techniques that can handle the scale of data typically seen in real-world applications.\n\tSome of the advantages that set TREE apart from other record linkage solutions on the market include:</p>\n\t</div>\n\t<div class=\"ulist\">\n\t<ul>\n\t<li>\n\t<p>Easily customizable and flexible data cleaning and canonicalization process defined for each data source</p>\n\t</li>\n\t<li>\n\t<p>Highly scalable record linking and graph-based algorithms for identifying entity groups</p>\n\t</li>\n\t<li>\n\t<p>Support for multiple identifier hierarchies (e.g. person and family level IDs)</p>\n\t</li>\n\t<li>\n\t<p>Processes for configuring representative records for each identifier group</p>\n\t</li>\n\t<li>\n\t<p>Feedback loop for feeding manual edges into future iterations</p>\n\t</li>\n\t</ul>\n\t</div>\n\t</div>\n\t</div>\n\t</div>\n\t<div class=\"sect1\">"
"This post was generated using curl POST with a json body schema."
"This post was generated using curl POST with a json body schema."
"This post was generated using curl POST with a json body schema."
"This post was generated using curl POST with a json body schema."
"This post was generated using curl POST with a json body schema."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"I think the best way to do it is through organising the content and being able to search through topics."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Do we have a potential solution for the search reaching into this page?"
"I was able to access the document with the link using SSO."
"https://doc.tresata.com/9.1.0-SNAPSHOT/tresata-doc-truck-9.1.0-SNAPSHOT.html"
"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n\n</head>\n<body class=\"article toc2 toc-left\">\n<div id=\"header\">\n<h1>A Guide to Matching Configs</h1>\n<div id=\"toc\" class=\"toc2\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#overview\">1. Overview</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#rules\">1.1. Piggybacks</a></li>\n<li><a href=\"#scored-matching\">1.2. Scored Matching</a></li>\n</ul>\n</li>\n<li><a href=\"#piggybacks\">2. Piggyback Rules</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#piggybackOn\">2.1. Hard Matching on Values</a></li>\n<li><a href=\"#piggybackOnSignatureFor\">2.2. Hard Matching on Signatures</a></li>\n</ul>\n</li>\n<li><a href=\"#scored-rules\">3. Scored Matching Rules</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#same\">3.1. Equivalence Matching</a></li>\n<li><a href=\"#jaccard\">3.2. Jaccard Overlap Matching</a></li>\n<li><a href=\"#edit-distance\">3.3. Levenshtein (Edit) Distance Matching</a></li>\n<li><a href=\"#omh\">3.4. Order Min Hash (OMH) Matching</a></li>\n<li><a href=\"#phonetic\">3.5. Phonetic Matching</a></li>\n<li><a href=\"#numeric\">3.6. Numeric Distance Matching</a></li>\n<li><a href=\"#date\">3.7. Date Distance Matching</a></li>\n<li><a href=\"#term-weights\">3.8. Matching on Term-Weight Vectors</a></li>\n<li><a href=\"#or-matching\">3.9. OR Matching Rules</a></li>\n<li><a href=\"#and-matching\">3.10. AND Matching Rules</a></li>\n<li><a href=\"#set-matching\">3.11. Matching over Sets of Values</a></li>\n<li><a href=\"#optional-matches\">3.12. Matching with Optional Values</a></li>\n</ul>\n</li>\n<li><a href=\"#scoreOn\">4. Scored Rules without Signatures</a></li>\n<li><a href=\"#penalties\">5. Penalties (Negative Scores)</a></li>\n<li><a href=\"#config-options\">6. Additional Config Options</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#grouping-threshold\">6.1. Grouping Threshold</a></li>\n<li><a href=\"#skip-abort-thresholds\">6.2. Skip and Abort Thresholds</a></li>\n<li><a href=\"#standalone-mode\">6.3. Enabling Self-Edges</a></li>\n<li><a href=\"#relation-explanations\">6.4. Enabling Relation Explanations</a></li>\n</ul>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"content\">\n<div class=\"sect1\">\n<h2 id=\"overview\">1. Overview</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Matching configs define the logic for scoring and creating edges, as well as resolving clusters of records into Tresata IDs, in a step of a matching pipeline.\nThe primary task of a config is to set rules for establishing a relation between two records.\nThere are two types of matching steps in a pipeline, and they influence how a matching config is used to find relations.\nAn <strong>internal</strong> matching step allows for any two records in a set of sources to establish a relation, or edge, between them given the rules in a matching config.\nA <strong>targeted</strong> matching step allows for any record in a set of sources to establish at most one edge to a record in a set of (already resolved) target sources using the rules in a matching config.\nConsider the pipeline object below.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.matching.core.dsl.Pipeline\n\nPipeline.init\n  resolve (\"A\", \"B\") using config1 <i class=\"conum\" data-value=\"1\"></i><b>(1)</b>\n  resolve (\"C\", \"D\") against (\"A\", \"B\") using config2 <i class=\"conum\" data-value=\"2\"></i><b>(2)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<table>\n<tr>\n<td><i class=\"conum\" data-value=\"1\"></i><b>1</b></td>\n<td>This internal step allows any record in <code>A</code> or <code>B</code> to establish any number of relations to other records using the rules provided in the matching config <code>config1</code>.</td>\n</tr>\n<tr>\n<td><i class=\"conum\" data-value=\"2\"></i><b>2</b></td>\n<td>This targeted step uses the rules in the matching config <code>config2</code> to define how a record from <code>C</code> or <code>D</code> can establish a relation against at most one record from either <code>A</code> or <code>B</code>.</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>A matching config contains a set of rules for establishing an edge between two records.\nThere are two mechanisms for determining whether two records form an edge.\n<strong>Scored matching</strong> rules assign a score to a pair of records over one or more canonical fields using a given criteria, and if the total score across all such rules is greater than a threshold value, the records are determined to be a match.\n<strong>Piggyback</strong> rules circumvent the scoring mechanism and automatically establish an edge between records that have a matching value across one or more canonical fields.\nThe following is an example of a matching config that employs both.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.matching.core.dsl.Config\nimport com.tresata.matching.core.integrated\n\nConfig.init\n  piggybackOn \"ssn\" score 100.0\n  matchOn \"firstName\" score 1.0 using integrated.Same[String]\n  matchOn \"lastName\" score 1.0 using integrated.Same[String]\n  matchIfScore 1.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Two records with the same value in the <code>ssn</code> field are deemed a match automatically using the piggyback rule.\nIn addition to this, if two records share the same <code>firstName</code> and <code>lastName</code> value, they reach a score of 2.0, which is sufficient to form a match.</p>\n</div>\n<div class=\"paragraph\">\n<p>Piggybacks and scored rules work in parallel to determine the set of relations between records, and in internal matching steps these records are then resolved into clusters that are given the same Tresata ID.\nIn a targeted matching step, each record in the resolved set of sources can find any number of potential relations to records in the target sources, but only the edge with the highest score is kept.\nThe record is given the same Tresata ID as the record that it forms an edge with in the target source.</p>\n</div>\n<div class=\"paragraph\">\n<p>Matching configs can contain additional <a href=\"#config-options\">settings</a> that include parameters specific to edge scoring, options for how to resolve clusters of records to the same identifier, and optimizations for finding and testing records pairs for potential relations.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"rules\">1.1. Piggybacks</h3>\n<div class=\"paragraph\">\n<p>The simplest method of forming edges between records is using a <em>piggyback</em> rule.\nA piggyback rule will group records together based on their values in the given canonical fields, and connect all records within a group to one another.\nThey are efficient in that each piggyback runs a fully independent process in parallel with other piggybacks, and the size of groupings of records has little bearing on the run-time.\nThey are useful for when there is a set of canonical fields where an exact match across each field is indisputable evidence of a shared identity.\nConsider the following config:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  piggybackOn \"ssn\" score 100.0\n  piggybackOn (\"firstName\", \"lastName\") score 90.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>If this config is used in a matching step, then a pair of records is considered a match if either the <code>ssn</code> values are the same, or both the <code>firstName</code> and <code>lastName</code> values are the same.\nEach piggyback rule consists of two pieces: a set of one or more canonical fields and a score.\nThe score is required, but the value does not affect whether it is considered a match, but rather is used for ranking purposes.\nIt&#8217;s not necessary to assign piggybacks different scores, but only the highest-ranked edge is kept, and assigning different scores leads to a consistent selection of edges.</p>\n</div>\n<div class=\"paragraph\">\n<p>Canonical fields are not restricted to containing single values; a field may contain a set of values, and it&#8217;s possible to create a piggyback rule that establishes a match between any two records that share a value.\nFor this, we <em>piggyback on signatures</em>.\nSignatures are discussed further in the following section on <a href=\"#scored-matching\">scored matching rules</a>, but essentially we can group on each of the values in a set instead of the field&#8217;s value itself.\nConsider the following rule:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">piggybackOnSignatureFor \"names\" score 100.0 using signature.set(signature.same[String])</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>names</code> field contains a set of names, and any two records with a shared value will potentially form an edge using the piggyback rule.\nThe section on <a href=\"#piggybackOnSignatureFor\">hard matching on signatures</a> details how to create these piggyback rules.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scored-matching\">1.2. Scored Matching</h3>\n<div class=\"paragraph\">\n<p>Piggyback rules are the most efficient way to group records together on equivalent values, but in many applications we need to establish matches between records using a combination of matching criteria over several fields, using both strict and \"fuzzy\" matching.\nThis is accomplished using <em>scored rules</em>, where separate rules score pairs of records over different fields, and a pair of records is considered a match when the total score surpasses a <em>matching threshold</em> value.\nTake the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"ssn\" score 1.0 using integrated.Same[String]\n  matchOn \"name\" score 0.5 using integrated.Jaccard()\n  matchOn \"birthdate\" score 0.5 using integrated.Same[String]\n  matchIfScore 1.4</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The matching config contains two types of lines: (1) a series of scored matching rules (the lines beginning with a <code>matchOn</code> declaration), and (2) a matching threshold, set using the <code>matchIfScore</code> keyword.\nThe basic idea is that we want to score every pair of records with the sum of the matching rules that they satisfy, then if this total score surpasses the matching threshold, we emit a relation between the records, and the two records will receive the same Tresata ID.\nThis is a bit of a simplification, since not every pair of records will be checked, but the matching process is optimized to check for matches only within groupings of records that are determined to be likely matches.</p>\n</div>\n<div class=\"paragraph\">\n<p>Let&#8217;s consider the above example.\nThere are three rules with scores of 1.0, 0.5, and 0.5, that must total to a score higher than 1.4.\nThere are three ways to satisfy the matching threshold: (1) all three rules match to reach a score of 2.0, (2) rules 1 and 2 reach a score of 1.5, and (3) rules 1 and 3 reach a score of 1.5.\nThe scores themselves are not very important, but should be selected to ensure that only the desired combinations of rules can satisfy the threshold.\nFor example, we are scoring the rules to ensure that a matching name and birthdate together are not enough to signify a match, but either one in addition to a social security number is sufficient.</p>\n</div>\n<div class=\"paragraph\">\n<p>A scored matching rule typically consists of three pieces:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>One or more canonical fields to draw values from</p>\n</li>\n<li>\n<p>A score to boost a pair of records' total by if the rule is satisfied</p>\n</li>\n<li>\n<p>a <em>Matcher</em> that dictates how signatures are generated and comparisons are calculated</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>A scored matching rule uses the <code>matchOn ... score ... using ...</code> syntax to describe these three components.\nThe fields used in the rule can be any subset of the canonical fields that are mapped in TREE&#8217;s canonical configuration.\nIf, say, we had the birthdate split over multiple fields, we might use the matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"month\", \"day\", \"year\") score 0.5 using integrated.Same[(String, String, String)]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>to boost the score by 0.5 only when (non-null) values in all three fields match.\nThe score must be a positive value, but <a href=\"#penalties\">penalties</a> can be assigned as well to subtract from the total score.\nThe final piece following the <code>using</code> keyword is the <strong>Matcher</strong>, and the <a href=\"#scored-matching\">scored matching</a> section details all the commonly-used <code>Matcher</code> classes.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <code>com.tresata.matching.core.integrated</code> package contains the <code>Matcher</code> sub-classes for possible use.\nThe <code>Matcher</code> sub-class used by a scored matching rule consists of three components:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>A <code>Derive</code> function that converts the values from the given fields into the types that will be used for comparisons</p>\n</li>\n<li>\n<p>A <code>Sign</code> function that converts a value into a collection of one or more <strong>signatures</strong>, which are often stored as <code>Long</code> hash values</p>\n</li>\n<li>\n<p>A <code>Compare</code> function that returns a <code>Double</code> from 0.0 to 1.0 given two inputs, used for determining equivalence</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>The <code>Derive</code> function is typically inferred from the type, and so is usually hidden from the end-user, an exception being <a href=\"#set-matching\">matching over sets</a>.\nThe <code>Compare</code> function is relatively straightforward: a <code>Compare</code> function takes a tuple containing two values to compare and outputs a <code>Double</code>, which by convention is normalized to the range of [0.0, 1.0], with 1.0 indicating a perfect match.\nThe comparison function is typically hidden from the end-user as an implementation detail of the <code>Matcher</code> class being used, but are sometimes referenced directly, as in <a href=\"#scoreOn\">scoreOn</a> rules or <a href=\"#penalties\">penalties</a>.\nThe <code>Sign</code> function determines how records will be grouped together as potential matches, where the <code>Compare</code> function will brute-force each pairing to look for possible matches.\nThe signature-generating and comparison functions can be found in the <code>signature</code> and <code>comparison</code> objects inside the <code>com.tresata.matching.core</code> package, respectively.</p>\n</div>\n<div class=\"paragraph\">\n<p>The signature-generating (<code>Sign</code>) function has a significant effect on the run-time of TREE.\nEach <code>Matcher</code> sub-class defines a relevant signature function, which produces one or more signatures per value.\nSome matchers (such as <code>integrated.Same</code> or <code>integrated.Phonetic</code>) produce a single signature, while others (such as <code>integrated.Jaccard</code>) yield multiple signatures.\nThe signatures are generated such that any two original values that generate the same signature have a strong chance of being a match.\nTREE will group records by signatures, and perform exhaustive comparisons on record-pairs within these groupings to look for potential relations.</p>\n</div>\n<div class=\"paragraph\">\n<p>Consider the two Strings \"johnny chapman appleseed\" and \"john chapman appleseed\", and a config with one scored rule that compares names using the <code>Jaccard</code> matcher.\nBy default, the <code>Jaccard</code> matcher tokenizes a String into a set of two-letter bigrams (\"jo\", \"oh\", \"hn\", &#8230;&#8203;), and the similarity is calculated as the Jaccard index between these sets.\nFor these two Strings the Jaccard index is about 0.8636.\nIf a scored rule uses a Jaccard matcher with a threshold of 0.8, each value generates eight signatures:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scala&gt; integrated.Jaccard(thres = 0.8).sign(\"johnny chapman appleseed\")\nres0: Iterable[Long] = List(-3686951320033172223, 5307948663842712273, -8681445009939126779, 8816072295552412390, -5884928203576020930, -652560532381505810, 318603489272894691, 4636188509827489550)\n\nscala&gt; integrated.Jaccard(thres = 0.8).sign(\"john chapman appleseed\")\nres1: Iterable[Long] = List(-7667913001915710829, -3094730804111724703, -8681445009939126779, -6000810573180111384, 2901199594335974546, 8583470323732618623, 318603489272894691, 2101723635467649203)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Note how these two Strings have two signatures in common.\nTREE will group records on signatures and compare records within the same group.\nThese two records will end up in two of the same groupings, where a comparison will be calculated and an edge will result between the records.</p>\n</div>\n<div class=\"paragraph\">\n<p>The groupings of signatures in fact depend on the <em>minimal combinations of scored rules</em> that can reach the matching threshold.\nConsider the rules in the config defined at the beginning of this section:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"ssn\" score 1.0 using integrated.Same[String]\nmatchOn \"name\" score 0.5 using integrated.Jaccard()\nmatchOn \"birthdate\" score 0.5 using integrated.Same[String]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>Same</code> matcher that is used in the rules on <code>ssn</code> and <code>birthdate</code> generates a single signature, based on the hash of the trimmed and lowercased value in the field.\nThe <code>Jaccard</code> matcher (with a default threshold of 0.8) will generate eight signatures for each value in the <code>name</code> field.\nRecall that two name values with a shared signature don&#8217;t necessarily imply that the values will match, but have strong evidence for a potential match.\nThe config requires either both <code>ssn</code> and <code>name</code> to match between two records, or both <code>ssn</code> and <code>birthdate</code>.\nThus TREE will generate groupings based on every combination of ssn and name signatures (<code>1 &#x00D7; 8 = 8</code>), as well as every combination of ssn and birthdate signatures (<code>1 &#x00D7; 8 = 8</code>).\nAssuming a record has non-null values in these three fields, a record will appear in 16 different groupings with other records that share signatures across one of these pairs of fields.</p>\n</div>\n<div class=\"paragraph\">\n<p>An optimal matching configuration will strike a balance between having too many groups, and having groups with too many records.\nIf the size of a group exceeds certain limits, records will be ignored or groups tossed out altogether to avoid performing a quadratically increasing amount of calculations.\nOnce a group contains 5,000 records, the number of comparisons will swell to 25 million, where it will begin to greatly impact TREE&#8217;s performance.\nSee the section on <a href=\"#grouping-threshold\">grouping thresholds</a> for methods of controlling this number of comparisons.</p>\n</div>\n<div class=\"paragraph\">\n<p>As another example, consider the following matching configuration:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"first_name\" score 1.0 using integrated.Jaccard(thres = 0.75)\n  matchOn \"last_name\" score 1.0 using integrated.Jaccard(thres = 0.75)\n  matchOn \"address\" score 1.0 using integrated.Jaccard(thres = 0.75)\n  matchOn \"city\" score 1.0 using integrated.Jaccard(thres = 0.75)\n  matchIfScore 2.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Here we have a config that requires any combination of three rules out of the four, each with a Jaccard matcher that generates 9 signatures.\nThis results in each record appearing in <code>(4 choose 3) &#x00D7; 9<sup>3</sup> = 4 &#x00D7; 729 = 2,916</code> different groupings on signatures.\nThis example illustrates how using matchers that generate a large number of signatures can greatly impact the run-time of TREE.\nIt is possible to lower the number of groupings in this example by specifying a different grouping threshold (which by default is the same as the matching threshold).\nFor example, if it was set to <code>1.9</code> instead (by invoking <code>groupIfScore 1.9</code>), we could instead generate <code>(4 choose 2) &#x00D7; 9<sup>2</sup> = 6 &#x00D7; 81 = 486</code> groupings per record.\nSee the section on <a href=\"#grouping-threshold\">grouping thresholds</a> for more information and examples.\nIt&#8217;s important to minimize the number of groupings, but to also ensure that the groupings are not too large and full of records that have a low chance of matching, so that we don&#8217;t compare too many record-pairs that won&#8217;t yield a match.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nIt&#8217;s important to take into account the type of matching step in the pipeline a matching config is used in when considering the groupings of signatures and their size. For an <em>internal</em> matching step (<code>resolve &lt;sources&gt; using &lt;config&gt;</code>) all records participate in signature grouping. For a <em>targeted</em> matching step (<code>resolve &lt;sources&gt; against &lt;targets&gt; using &lt;config&gt;</code>), only the target sources are grouped by signatures, while the resolved sources are streamed through record-by-record.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Matching configs won&#8217;t typically need much optimization done on the grouping threshold, but it&#8217;s important to be aware of the effects that certain matchers (such as <code>Jaccard</code>) can have on the run-time of TREE, particularly when several matchers that generate multiple signatures are used together to total above the matching threshold.\nThe two factors that have the greatest impact on the run-time are (1) generating too many groups (by grouping on too many signature combinations) and (2) generating groups with too many records.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"piggybacks\">2. Piggyback Rules</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The following sub-sections show examples of commonly-used piggyback rules.\nPiggybacks can either group records on their values in the canonical fields, or on signatures generated using a given function.\nRecall that a signature-generating function maps a value to a set of 1 or more values.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"piggybackOn\">2.1. Hard Matching on Values</h3>\n<div class=\"paragraph\">\n<p>Most piggyback rules match on the values themselves.\nA piggyback rule should be used whenever there is a canonical field (or set of fields) where two records with the same values is indicative of a match.\nAny two records with the same value in the piggyback fields will be connected via relations.\nThe <code>piggybackOn ... score ...</code> syntax is used to specify the fields to piggyback on, as well as the score to assign to edges.\nThe following rule connects any two records with the same (non-null) <code>ssn</code> value:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">piggybackOn \"ssn\" score 100.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The score here is only useful in ranking edges.\nOnly the highest-ranking edge is written to relations, and so piggyback scores are typically set to a value higher than the sum of points from any scored matching rules.\nMultiple fields can be piggybacked on, and for such rules, the values in all fields must be non-null to participate in a grouping.\nThe following rule,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">piggybackOn (\"name\", \"birthdate\") score 90.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will group all records with the same name and birthdate into the same Tresata ID.\nThis is similar to if we were to use only scored matching rules:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"name\" score 1.0 using integrated.Same[String]\n  matchOn \"birthdate\" score 1.0 using integrated.Same[String]\n  matchIfScore 1.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The piggyback version is much faster, however.\nA field may be used in multiple piggybacks, so you can even replace a set of scored rules with piggybacks on the combinations of fields that result in a match.\nConsider the following config:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"ssn\" score 1.0 using integrated.Same[String]\n  matchOn \"name\" score 0.5 using integrated.Same[String]\n  matchOn \"birthdate\" score 0.5 using integrated.Same[String]\n  matchIfScore 1.4</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This config can be replaced with a config using only piggybacks:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  piggybackOn (\"ssn\", \"name\") score 100.0\n  piggybackOn (\"ssn\", \"birthdate\") score 100.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This is helpful for whenever only <code>Same</code> matchers are used in the scored rules&#8212;&#8203;the combinations of fields that yield a match can be moved into piggybacks.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"piggybackOnSignatureFor\">2.2. Hard Matching on Signatures</h3>\n<div class=\"paragraph\">\n<p>In addition to matching on the values themselves, piggyback rules can match on signatures generated using a given function.\nThe primary use case for piggybacking on signatures is matching over sets of values.\nThis is particularly helpful in hierarchical matching, where matching pipelines operate on the roll-up fields, which are typically sets of values themselves.\nConsider a scenario where a <code>names</code> field contains multiple aliases, each of which should participate in piggyback matching.\nUsing the <code>piggybackOnSignatureFor ... score ... using ...</code> syntax, we can express that a match in any of the aliases should generate a relation between records:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">piggybackOnSignatureFor \"names\" score 100.0 using signature.set(signature.same[String])</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The signature-generating function <code>signature.set</code> operates on the values in the <code>names</code> field, which have the type <code>Set[String]</code>, and returns a set of signatures, where each value is mapped to a single signature using <code>signature.same[String]</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Piggybacking on signatures should only be used when two values that share a signature are necessarily equivalent (contrast this with the signature-generating function of the <code>Jaccard</code> matcher, where a shared signature is evidence of a <em>possible</em> match).\nThe following example maps values to a phonetic reduction and piggybacks on the results.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">piggybackOnSignatureFor \"firstName\" score 100.0 using signature.doubleMetaphone</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Piggybacking on signatures is rarely used outside of matching on sets, so they typically only appear in higher tiers of matching where the inputs are most often sets of values aggregated over multiple records within a Tresata ID.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"scored-rules\">3. Scored Matching Rules</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The following sub-sections detail all the commonly-used <code>Matcher</code> sub-classes that are used in scored matching rules.\nThese classes are all found in the <code>integrated</code> package, located in <code>com.tresata.matching.core.integrated</code>.\nSee the <a href=\"#scored-matching\">scored matching</a> section for an explanation of how rules are structured, as well as how TREE optimizes the search for relations given all the rules specified in a matching config.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"same\">3.1. Equivalence Matching</h3>\n<div class=\"paragraph\">\n<p>The <code>Same</code> matcher is the go-to for matching over fields on identical values.\nIt is the most efficient matcher as it only generates a single signature per value (a hash of the value itself) and its comparison function is the given type&#8217;s equality function.\nIt can operate on Strings and primitive types, as well as collections including <code>Seq</code>, <code>Map</code>, <code>Option</code>, and <code>Tuple</code> types.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"name\" score 1.0 using integrated.Same[String]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>demonstrates the most typical use of the <code>Same</code> matcher: matching on String values.\nAs is the case with all matchers, Strings are first trimmed for whitespace and lowercased.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Generates a single signature per value, so very efficient</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Can only handle exact matches</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"jaccard\">3.2. Jaccard Overlap Matching</h3>\n<div class=\"paragraph\">\n<p>The <code>Jaccard</code> matcher is the most commonly used matcher for finding links between Strings that are not completely identical.\nUsing a given <em>tokenizer</em>, a String is mapped to a sequence of tokens, and the Jaccard index is calculated between two Strings by dividing the size of their intersection by their union.\nIf the Jaccard index is at least as high as the given threshold, the Strings are considered a match.</p>\n</div>\n<div class=\"paragraph\">\n<p>The tokens are typically bi-grams (pairs of letters) but this can be adjusted by creating a custom ngrams tokenizer (e.g. <code>tok = token.ngrams(size = 3)</code>), or a separate tokenizer altogether can be used.\nThe default threshold for accepting a match is 0.8 and can be adjusted by setting the <code>thres</code> parameter.\nA feature called <em>one permutation hashing</em> can be enabled by setting <code>onePerm = true</code> for potential speed-up, however this feature is still in testing.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"name\" score 1.0 using integrated.Jaccard(thres = 0.8)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will use the default tokenizer of size-2 n-grams (bi-grams) and accept a match between any two Strings that have a Jaccard index of at least 0.8 between their sets of bi-grams.</p>\n</div>\n<div class=\"paragraph\">\n<p>It&#8217;s important to note that decreasing the threshold for the Jaccard match will raise the numbers of signatures generated by each record, which can cause a significant increase in the number of groupings of signatures across multiple rules.\nWhen possible, keep the threshold as high as possible while still getting satisfactory results.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Reliable method to implement fuzzy matching on Strings</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Generates several signatures depending on how low a threshold is used, so may greatly increase the number of groupings, particularly when used in several rules</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"edit-distance\">3.3. Levenshtein (Edit) Distance Matching</h3>\n<div class=\"paragraph\">\n<p>Some matchers are useful for generating matches between Strings that are separated by a small number of transformations.\nIdeally, an efficient matcher will generate equivalent signatures for Strings that are similar enough, but not for String pairs that vary significantly (false positives).\nThe <code>EditDistance</code> matcher generates a match between two Strings if the Levenshtein distance between the Strings is within the given threshold.\nThe Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\nThis matcher will generate a match between \"abcd\" and \"bacd\" (one transposition), but not between \"abcd\" and \"abcdefg\" (three insertions).\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"name\" score 1.0 using integrated.EditDistance(thres = -2)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will accept a match between pairs of Strings that require no more than two edits to transform one String into the other.\nWhile this matcher has good guarantees of effectiveness in producing positive matches, it should generally be avoided as it involves some very inefficient operations.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Matcher is very accurate; will pick up matches with few typos very effectively</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Very computationally inefficient&#8212;&#8203;lower threshold means many more signatures to generate; typos aren&#8217;t terribly common in practice</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"omh\">3.4. Order Min Hash (OMH) Matching</h3>\n<div class=\"paragraph\">\n<p>The Order Min Hash (OMH) matcher is a good alternative to using the <code>EditDistance</code> matcher, as it estimates the (case-insensitive) Levenshtein distance between two Strings using locality sensitive hashing.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"full_name\" score 1.0 using integrated.OMHED.long2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will accept a match between pairs of Strings that require no more than two edits (insertions, deletions, or transpositions) to transform one String into the other, and will work optimally for Strings of size at least 12 characters.\nThe defaults available are <code>short1</code> for short Strings (6-12 characters) with up to one typo, and <code>long1</code>, <code>long2</code>, and <code>long3</code> for longer Strings (&gt;12 characters) with up to one, two, or three typos, respectively.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Much more efficient than <code>EditDistance</code>; optimized for length of String</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> In practice, small differences of 1-2 edits are not very common; relatively untested</p>\n</div>\n<div id=\"hybrid-omh\" class=\"paragraph\">\n<p>The Hybrid OMH matcher is an alternative to the <a href=\"#omh\">Order Min Hash (OMH) Matching</a> matcher made to apply the same benefits across a wider variety of input String sizes.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"full_name\" score 1.0 using integrated.OMHED.hybrid</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will accept a match between pairs of Strings with different number of edits (insertions, deletions, or transpositions) for Strings of different respective sizes:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>For Strings sized up to and including <code>low</code>, it will accept 0 edits.</p>\n</li>\n<li>\n<p>For Strings sized above <code>low</code> and up to and including <code>high</code>, it will accept no more than 1 edit (equivalent to OMHED.short1).</p>\n</li>\n<li>\n<p>For Strings sized above <code>high</code>, it will accept no more than 2 edits (equivalent to OMHED.long2).</p>\n</li>\n<li>\n<p>For String comparisons between different sizes, it defaults to behavior according to the shorter String.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The default parameters are <code>3</code> and <code>10</code> characters for the low and high boundaries respectively.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros</em></strong>: Combines the effectiveness of OMHEDs optimized for short and long Strings</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons</em></strong>: Same as <a href=\"#omh\">Order Min Hash (OMH) Matching</a>, but with potential worse performance on Strings that cut across low and high length boundaries</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"phonetic\">3.5. Phonetic Matching</h3>\n<div class=\"paragraph\">\n<p>The <code>Phonetic</code> matcher is useful for finding matches between Strings that are phonetically equivalent.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"name\" score 1.0 using integrated.Phonetic</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This matcher will accept a match between \"bobby\" and \"bobbie\", for example.\nHowever, this matcher should only ever be used on short, single-word values (such as first names), since the values are reduced to a phonetic spelling of at most four letters.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Can catch equivalences that yield too low of an overlap in n-grams; only generates a single signature, so very fast</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Cannot handle longer Strings (will truncate after first few syllables); limited usefulness in practice</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"numeric\">3.6. Numeric Distance Matching</h3>\n<div class=\"paragraph\">\n<p>The <code>Numeric</code> matcher generates matches between numerical data by checking that the difference between two values is within the specified range.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"age\" score 1.0 using integrated.Numeric(5)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This matching rule will accept a match between two integer values in the \"age\" field if they differ by <em>at most</em> 5.\nThe tolerance must be greater than zero.\nIf you require an exact match in an integer field, use <code>integrated.Same[Int]</code> instead.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Small number of signatures generated, so not computationally intensive</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Not likely to be useful in real applications</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"date\">3.7. Date Distance Matching</h3>\n<div class=\"paragraph\">\n<p>The <code>Date</code> matcher generates matches between calendar date data by checking that the difference between two epoch days is within the specified range.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"date\" score 1.0 using integrated.Date(5)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This matching rule will accept a match between two calendar dates (parsed from format <code>yyyy-MM-dd</code>) in the \"date\" field if they differ by <em>at most</em> 5 epoch days.\nThe tolerance must be greater than zero.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Small number of signatures generated, so not computationally intensive</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Conceptually different from other matchers.</p>\n</div>\n<div class=\"paragraph\">\n<p>A good example of this matcher&#8217;s use case is to find connections between events that occur within some time window,\ne.g. transactions that occur at the same bank within 2-3 days of each other.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"date\" score 1.0 using integrated.Date(5, (day: Int) =&gt; Some(day))</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"term-weights\">3.8. Matching on Term-Weight Vectors</h3>\n<div class=\"paragraph\">\n<p>Matching configs support matching over term-weight vectors, which will tokenize inputs and weight terms based on their frequency in the data.\nUsing these matchers requires first transforming the input data from Strings into a type of vector format.</p>\n</div>\n<div class=\"paragraph\">\n<p>One way to find matches on term-weight vectors is using the <code>Instance</code> matcher, which matches weighted terms using angular similarity and random projections for signatures.\nThe vectors can be pre-computed or computed on-the-fly using the <code>transform</code> syntax in the matching config.\nWe recommend using the <code>transform</code> syntax:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">transform \"address\" -&gt; \"address\" using transform.TfIdf()\nmatchOn \"address\" score 1.0 using integrated.Instance[String](thres = 0.85)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>These two lines first transform the data in the <code>address</code> field into <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf-idf</a> (term frequency-inverse document frequency) vectors, and then accept a match between two vectors if they have a similarity of at least 0.85, measured using their cosine similarity.</p>\n</div>\n<div class=\"paragraph\">\n<p>Encoding textual information as tf-idf vectors has the advantage of assigning weight to different terms based on their frequency in the corpus of input data.\nFor example, the <code>Instance</code> matcher has a much higher likelihood of discovering a match between the inputs \"st. jude hospital\" and \"saint jude\", since the weighting of the term \"jude\" will overshadow the effects of \"st\"/\"saint\" and \"hospital\" in the resulting vectors, due to its low frequency.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Pros:</em></strong> Capable of reducing the noise from common/less impactful terms</p>\n</div>\n<div class=\"paragraph\">\n<p><strong><em>Cons:</em></strong> Not thoroughly tested; generates a significant amount of signatures (more so than Jaccard), so not computationally feasible in many applications</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"or-matching\">3.9. OR Matching Rules</h3>\n<div class=\"paragraph\">\n<p>In many applications a scored matching rule is necessary for a particular piece of information, but there are multiple methods of establishing a match on that data i.e. using different fields that represent the same piece of information.\nFor example, say we want to assign a score of 1.0 to a match on a person&#8217;s name.\nHowever, there might be two fields containing the name of an individual, with their own distinct levels of sparseness or degrees of noise.\nIt&#8217;s possible to assign a score boost for satisfying a match between either one of them, but not both of them summed together.\nThese kinds of OR matching rules can be created using a <em>nested</em> matching config:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (Config.init\n  matchOn \"name\" score 1.0 using integrated.Same[String]\n  matchOn \"short_name\" score 1.0 using integrated.Same[String]\n  matchIfScore 0.9\n) score 1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example we establish a matching rule with a score of 1.0, and instead of listing the fields and supplying a matcher, we supply a matching config object that is evaluated independently.\nIf a match is found using the supplied rules and threshold, the score boost is added.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nNested matching configs can contain any number of scored matching rules, but due to how piggybacks are implemented, any piggyback rules defined inside would act as an ordinary piggyback rule, yielding an immediate match instead of simply adding to the nested config&#8217;s score. As a result, piggybacks defined inside a nested config will throw an error.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"and-matching\">3.10. AND Matching Rules</h3>\n<div class=\"paragraph\">\n<p>Oftentimes a score boost is desired for multiple rules when matched together, but individually the rules should not provide a boost to the total score.\nFor example, consider an individual&#8217;s name spread across two fields: <code>firstName</code> and <code>lastName</code>.\nOn its own, a first or last name doesn&#8217;t indicate a potential match, but taken together they provide meaningful evidence of a match.\nIn that case we may want to provide a boost to the score if both the first and last name are found to be equivalent, and none otherwise.\nWe can accomplish this using AND matching rules.</p>\n</div>\n<div class=\"paragraph\">\n<p>There are two methods for adding to the score with AND matching.\nThe first involves creating a matching rule based on a tuple containing all the fields that should be matched together:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"firstName\", \"lastName\") score 1.0 using integrated.Same[(String, String)]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example, we are establishing signatures and running comparisons using a matcher that compares tuples of two Strings.\nAs with individual Strings, the <code>Same</code> matcher run on a tuple of Strings <em>will throw out entries that contain nulls in either of the two entries</em>.\nSo in order to satisfy this rule, the data from the <code>firstName</code> and <code>lastName</code> fields of both records will need to be defined and found identical using the compare function of <code>integrated.Same</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>The other alternative to tuple matching is to use a nested matching config.\nWith a nested matching config we can establish any number of scored matching rules, and the result of meeting the nested config&#8217;s matching threshold is to apply the given score to the total.\nIn the following example, if the nested matching config satisfies both rules, it will surpass the internal threshold of 1.9, and consequently add 1.0 to the total score.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (Config.init\n  matchOn \"firstName\" score 1.0 using integrated.Same[String]\n  matchOn \"lastName\" score 1.0 using integrated.Same[String]\n  matchIfScore 1.9\n) score 1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Nested matching configs are particularly useful when the fields that should be matched together are using different comparison functions.\nThe above two methods are functionally equivalent, but the first rule (using tuples) is more efficient&#8212;&#8203;the first and last names taken together will result in finer-grained signature groups and less comparisons between records that won&#8217;t necessarily result in a match (people sharing only the same first name won&#8217;t be compared, only those who share both first and last names).\nHowever, sometimes the comparison functions are different for fields that should be counted together.\nTake the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (Config.init\n  matchOn \"address\" score 1.0 using integrated.Jaccard()\n  matchOn \"city\" score 1.0 using integrated.Same[String]\n  matchIfScore 1.9\n) score 1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In the rule for <code>address</code> we don&#8217;t wish to use the <code>integrated.Same</code> matcher that we use for city, so matching on tuples won&#8217;t work because we need separate matchers for each field.\nWe can use a nested matching config to separate their rules and be free to use whichever matcher we desire for each field.\nWe still need to satisfy both rules in order to add anything to the score; if both rules result in a match, 1.0 is added to the total.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-warning\" title=\"Warning\"></i>\n</td>\n<td class=\"content\">\nNested matching configs can contain any number of scored matching rules, but due to how piggybacks are implemented, any piggyback rules defined inside would act as an ordinary piggyback rule, yielding an immediate match instead of simply adding to the nested config&#8217;s score. As a result, piggybacks defined inside a nested config will throw an error.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"set-matching\">3.11. Matching over Sets of Values</h3>\n<div class=\"paragraph\">\n<p>Matching configs now support matching rules that draw multiple values from records, be it from separate fields, or from fields that contain multiple values, or both.\nThere are two matchers, <code>Overlap</code> and <code>Intersect</code>, that operate on collections of values.\nThese are typically used in situations where similar data is spread out over multiple canonical fields, or when fields contain sets of values, such as hierarchical matching on roll-up.</p>\n</div>\n<div class=\"paragraph\">\n<p>These two matchers differ in how they judge a match between sets of elements.\n<code>Overlap</code> expects a signature function and is used to generate a match if there is a <em>non-empty intersection between the signatures</em> for the sets of elements in both records.\nIn the simplest case, this can mean finding an exact matching value across different fields (converting the values themselves into signatures).\nSignature functions can be found in <code>com.tresata.matching.core.signature</code>.\n<code>Intersect</code> instead takes in a <code>Matcher</code> class, and generates a match between two sets if there is an element from each that are considered a match using the given matcher.\nGenerally, the <code>Intersect</code> matcher is reserved for when equivalent signatures do not necessarily imply a match, such as in rules that use the <code>Jaccard</code> matcher.</p>\n</div>\n<div class=\"paragraph\">\n<p>When using <code>Overlap</code> or <code>Intersect</code>, the inputs need to be converted to the necessary type (Options of Sets/Sequences) using the appropriate <code>Derive</code> function, such as <code>forTuple3</code> or <code>forTuple2OfSets</code>.\nSee below for example usage.</p>\n</div>\n<div class=\"paragraph\">\n<p>To match across multiple canonical fields that each contain a single value, a rule is defined on a tuple containing the fields, and the matcher is wrapped by a function that converts the inputs into the necessary type (Options of Sets/Sequences).\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"name\", \"nickname\") score 1.0 using integrated.Overlap(signature.same[String]).forTuple2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will check for an identical name across two canonical fields&#8212;&#8203;so a <code>name</code> value for one record can match a <code>nickname</code> value for another record.\nThe rule will convert the tuple of two name Strings into the necessary type using the <code>forTuple2</code> call.\nTechnically, we are boosting the total score by 1.0 when there is a non-empty overlap of signatures generated by the set of values from the <code>name</code> and <code>nickname</code> fields in either record.\nIn this case the signatures are the values themselves (<code>signature.same</code>).</p>\n</div>\n<div class=\"paragraph\">\n<p>If, say, we wanted to use a <code>Jaccard</code> matcher to find a match between values across two canonical fields, we would need to use the <code>Intersect</code> matcher wrapped around the <code>Jaccard</code> matcher (using the <code>Overlap</code> matcher would mean a single shared Jaccard signature between values would result in a match).\nThis would generate signatures for each value in the two fields, then check the Jaccard index over each pairing of sets of signatures for two records over the two fields.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"name\", \"nickname\") score 1.0 using integrated.Intersect(integrated.Jaccard()).forTuple2</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>converts the tuple of two name Strings into the type needed for <code>Intersect</code>, uses the <code>Jaccard</code> matcher&#8217;s signature-generating function to create sets of signatures, then the <code>Jaccard</code> matcher&#8217;s comparison function to look for matches across both fields.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nWhen possible, use the <code>Overlap</code> function over <code>Intersect</code>; <code>Overlap</code> will optimize the comparison function between records. It will check that there is a non-zero overlap between signatures, rather than check all pairs of values from each side for equivalence. For this reason, always use <code>Overlap(signature.same[String])</code> over <code>Intersect(integrated.Same[String])</code>.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>The syntax is similar for checking for matches across fields that contain collections of values.\nIf a canonical field contains a set of Strings, we use the <code>forSet</code> function to convert the inputs to the necessary type.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"names\" score 1.0 using integrated.Overlap(signature.same[String]).forSet</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will look for a shared value between sets of Strings in the <code>names</code> field.\nSimilarly, the matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"names\" score 1.0 using integrated.Intersect(integrated.Jaccard()).forSet</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will check for a match across sets of Strings in the <code>names</code> field between two records, using the <code>Jaccard</code> matcher.</p>\n</div>\n<div class=\"paragraph\">\n<p>It&#8217;s also possible to build sets yourself from a field that has a set of Strings encoded into a single String, for example a comma-delimited set of values.\nThis involves preparing the input with a custom <code>Derive</code> function, using the <code>composeDerive</code> method.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"names\" score 1.0 using integrated.Overlap(signature.same[String]).composeDerive{ (s: String) =&gt; Option(s.split(\",\").toSet) }</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will split the Strings in the <code>names</code> field by comma, build the set of Strings (wrapped in an Option, as required), and use the result in the <code>Overlap</code> matcher.\nSimilarly, the matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn \"names\" score 1.0 using integrated.Intersect(integrated.Jaccard()).composeDerive{ (s: String) =&gt; Option(s.split(\",\").toSet) }</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will use the <code>Jaccard</code> signatures and comparison function to establish a match.</p>\n</div>\n<div class=\"paragraph\">\n<p>Finally, there is the possibility of drawing inputs from multiple fields each containing sets themselves, and for this we use the <code>forTupleNOfSets</code> functions.\nTo look for matching signatures across two fields containing sets of names, we might use the matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"names\", \"nicknames\") score 1.0 using integrated.Overlap(signature.same[String]).forTuple2OfSets</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Similarly, we can use this function for the <code>Intersect</code> matcher if we want to provide a custom <code>Matcher</code> class:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"names\", \"nicknames\") score 1.0 using integrated.Intersect(integrated.Jaccard()).forTuple2OfSets</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"optional-matches\">3.12. Matching with Optional Values</h3>\n<div class=\"paragraph\">\n<p>In almost all applications, we do not consider two empty values, or nulls, to be considered a match.\nIn some cases, however, we could want to alter this behavior.\nThis is typically only true for rules where other fields must also match, and necessarily be non-null.\nTake the following example,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">matchOn (\"address\", \"city\", \"apt\") score 1.0 using integrated.Same[(String, String, Option[String])]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this matching rule, we are considering empty values in the <code>apt</code> field to be a match.\nThe <code>address</code> and <code>city</code> values must both be defined (non-null) and match for 1.0 to be added to the score, but the <code>apt</code> values can either both be defined (non-null) and match, or both be null.\nThe matchers will always ignore null values, so we accomplish this by using an <code>Option</code> type.\nThe value in the <code>apt</code> field will either result in a <code>Some</code> object containing a String, or a <code>None</code> object.\nNull values will be converted to a <code>None</code> object which can match against other <code>None</code> values.</p>\n</div>\n<div class=\"paragraph\">\n<p>Matching on optional values is useful when paired with required fields, where the optional data is not necessary for a match, but differing values is indicative of a mismatch.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"scoreOn\">4. Scored Rules without Signatures</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>It&#8217;s also possible to specify scored matching rules that bypass the creation of signatures, but still contribute to the total score of a record pair.\nThis is most commonly used in targeted matching steps (<code>resolve ... against ... using ...</code>), in cases where a rule should not be considered necessary for a match, but should be used to \"boost\" one edge over another.\nConsider the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">import com.tresata.matching.core.dsl.Config\nimport com.tresata.matching.core.{integrated, comparison}\n\nConfig.init\n  matchOn \"ssn\" score 1.0 using integrated.Same[String]\n  matchOn \"firstName\" score 0.2 using integrated.Same[String]\n  scoreOn \"address\" score 0.2 ifComparisonAtLeast 0.8 using comparison.jaroWinkler\n  matchIfScore 1.1</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Using the <strong>scoreOn</strong> syntax, we state that the <code>address</code> field is not considered essential for a match, and that we would prefer to bypass that rule&#8217;s signature generation.\nRecord pairs will still need to generate similar signatures on <code>ssn</code> and <code>firstName</code> in order to group together and be considered for a match.\nOnce records are grouped together and compared, the <code>address</code> rule will be used in calculating the total score.\nNote that the scoreOn rule on <code>address</code> does not need a <code>Matcher</code> class.\nRecall that a <code>Matcher</code> specifies <code>Derive</code>, <code>Sign</code> and <code>Compare</code> functions.\nInstead, a scoreOn rule can be passed <em>just a comparison function</em>, because it only needs to specify how to score a pair of values belonging to records that are already grouped together for comparison.\nIf there is a <code>Matcher</code> class that contains the desired comparison function, it can be passed in instead:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scoreOn \"name\" score 1.0 using integrated.Same[String]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This scored rule uses the equality function for comparisons, but the piece that trims and lowercases each side is moved into the <code>Derive</code> function, so that it only needs to be done once for each value (as opposed to twice for each comparison).</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-tip\" title=\"Tip\"></i>\n</td>\n<td class=\"content\">\nIf there is a <code>Matcher</code> class that implements the desired comparison operation, it should be used instead, as it will move repeated operations into a <code>Derive</code> function so that the <code>Compare</code> function is optimized.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Consider the scoreOn rule from the config above:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scoreOn \"address\" score 0.2 ifComparisonAtLeast 0.8 using comparison.jaroWinkler</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Note the use of the <code>ifComparisonAtLeast</code> keyword.\nBy convention, the comparison functions return a value from the range [0.0, 1.0], where 1.0 indicates a perfect match.\nCalling <code>ifComparisonAtLeast 0.8 using comparison.jaroWinkler</code> binarizes the output of the comparison method such that its output is converted into a success (boost of 0.2) or failure (0.0) depending on whether <code>comparison.jaroWinkler</code> returns a value above 0.8.\nBy default, a scoreOn rule accepts a match if the comparison method outputs a value of 0.5 or above.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scoreOn \"address\" score 0.2 using comparison.jaroWinkler</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>boosts a record pair&#8217;s score by 0.2 if the output of the <code>jaroWinkler</code> function given their address values is greater than 0.5.\nThe <strong>Jaro-Winkler</strong> function is a good measure of the edit-distance between two Strings:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scala&gt; comparison.jaroWinkler(\"alicia\", \"allie\")\nres0: Double = 0.76</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, the <strong>Monge-Elkan</strong> distance function is a strong choice for comparing values where one is likely to be a prefix of the other:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scala&gt; comparison.mongeElkan(\"alice anderson\", \"alice\")\nres0: Double = 1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In addition, there are the more familiar <code>same</code> and <code>jaccard</code> comparison functions.\nThe <code>same</code> comparison function will return 1.0 if the Strings are equivalent (after lowercasing and trimming whitespace), and 0.0 otherwise.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scala&gt; comparison.same[String](\"alice\", \"Alice \")\nres0: Double = 1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>jaccard</code> comparison function takes in a tokenizer (the <code>com.tresata.matching.core.token</code> object contains the built-in tokenizers found in TREE) and returns the Jaccard index between the sets of tokens for two Strings.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scala&gt; comparison.jaccard(tok = token.ngrams(size = 2))(\"alice\", \"alicia\")\nres0: Double = 0.5</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>A custom comparison function can be supplied as well.\nThe following example will provide a boost on edges that involve a preferred source:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scoreOn \"source\" score 0.2 using { tup: (String, String) =&gt;\n  val (src1, src2) = tup\n  if (src1 == \"betterSource\" || src2 == \"betterSource\") 1.0 else 0.0\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>If one of the records involved has \"betterSource\" as its value in the <code>source</code> field, the edge&#8217;s score will be boosted by 0.2.</p>\n</div>\n<div class=\"paragraph\">\n<p>A scoreOn rule accepts one or more canonical fields to draw values from, a score to boost the total by, and a comparison function (the <code>com.tresata.matching.core.comparison</code> object contains the functions typically used in a scoreOn rule) or <code>Matcher</code> sub-class.\nHowever, the score can also be left out, in which case the output of the comparison function is used as the value to boost the score.\nThe matching rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">scoreOn \"address\" using integrated.MongeElkan</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>will boost the score between two records by the output of the Monge-Elkan function given their address values (an amount between 0.0 and 1.0).</p>\n</div>\n<div class=\"paragraph\">\n<p>The scoreOn syntax is a helpful way to alter the scores between records, without complicating the grouping of records by their signatures.\nIt&#8217;s most commonly used as a \"boost\" for nudging the score of one edge over another, which is useful in <em>targeted</em> matching steps, where only the highest-ranking edge is emitted to the relations table.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"penalties\">5. Penalties (Negative Scores)</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Up to this point all scored matching rules have specified how to add to the total score between a pair of records to determine whether two records form an edge and receive the same Tresata ID.\nIn the section on <a href=\"#scoreOn\">scored rules without signatures</a> we saw how scoreOn rules can bypass the generation of signatures and boost the total score using the output of the passed-in comparison function.\n<strong>Penalties</strong> offer a way for us to deduct points from the score between two records, given a comparison function.\nTake the following matching rule:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">penalizeOn \"gender\" penalty 1.0 ifComparisonLessThan 0.5 using comparison.same[String]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This rule will subtract 1.0 if the genders are not equivalent (the output of the comparison function is below 0.5), and will have no effect if they are the same.\nNote that if the gender is not defined (null) for either record, then the penalty will <em>not</em> be applied.</p>\n</div>\n<div class=\"paragraph\">\n<p>Similarly to scoreOn rules, the <strong>penalizeOn</strong> syntax takes in a value for the deduction (<code>penalty 1.0</code>), and a <code>Compare</code> function, with an optional threshold to binarize the comparison operator.\nThe <code>ifComparisonLessThan</code> keyword can convert the comparison function&#8217;s output to either 0.0 or 1.0, depending on whether the output is greater than the given threshold (the default threshold is 0.5).\nIf we have the rule</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">penalizeOn \"name\" penalty 1.0 ifComparisonLessThan 0.8 using comparison.mongeElkan</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>then a penalty of 1.0 will be deducted from the score between two records if and only if both name values are defined, and the output of the <code>comparison.mongeElkan</code> function is less than <code>0.8</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Penalties are often useful if equivalent values do not add much information, but differing values are indicative of a mismatch.\nThis could be useful if for example a gender column was only marginally populated, and we don’t want to rely on the presence of a gender value for a match, so we allow a sum of rules that doesn’t include gender to total above the matching threshold.\nThis way only when the value in the <code>gender</code> field for two records is present and different do we allow it to impact the total score.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"config-options\">6. Additional Config Options</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The following sections detail the settings that can be modified on a matching config which don&#8217;t involve specifying a piggyback or scored matching rule.\nThese settings impact how relations and identifiers are formed, as well as how the groupings of signatures are managed and optimized.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"grouping-threshold\">6.1. Grouping Threshold</h3>\n<div class=\"paragraph\">\n<p>As mentioned in the section on <a href=\"#scored-matching\">scored matching</a>, the grouping of signatures is done based on the (minimal) combinations of rules that sum to a total score that exceeds the matching threshold.\nSo for the following example,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"ssn\" score 4.0 using integrated.Same[String]\n  matchOn \"firstName\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchOn \"lastName\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchOn \"address\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchIfScore 4.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>groupings will be based off of signatures from the <code>ssn</code> and <code>firstName</code> rules together, the <code>ssn</code> and <code>lastName</code> rules, and the <code>ssn</code> and <code>address</code> rules, as those are the combinations that reach a score of 5.0.\nIn this case, however, we may find that grouping on the ssn values is strong enough to ensure that most records in a group will be matched.\nIt would be significantly faster if we could group on only the signatures generated by the first rule (on <code>ssn</code>), and then check for pairwise comparisons amongst those records to find relations.\nFor this, we would need our <strong>grouping threshold</strong> to be set to a value like 3.9.\nWe can accomplish this with the <code>groupIfScore</code> method.\nBy default, the grouping threshold is set to the matching threshold.\nIf we modify the matching config as follows,</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  matchOn \"ssn\" score 4.0 using integrated.Same[String]\n  matchOn \"firstName\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchOn \"lastName\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchOn \"address\" score 1.0 using integrated.Jaccard(thres = 0.6)\n  matchIfScore 4.9\n  groupIfScore 3.9</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>we reduce the grouping threshold from 4.9 to 3.9, allowing TREE to only group on signatures from the <code>ssn</code> field.</p>\n</div>\n<div class=\"paragraph\">\n<p>This can be very useful in cases where we require one or more matching rules that generate multiple signatures to reach the matching threshold, but they aren&#8217;t all necessary for creating small and robust signature groups.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nRecall that for an <em>internal</em> matching step (<code>resolve &lt;sources&gt; using &lt;config&gt;</code>) all records participate in signature grouping. For a <em>targeted</em> matching step (<code>resolve &lt;sources&gt; against &lt;targets&gt; using &lt;config&gt;</code>), only the target sources are grouped by signatures, while the resolved sources are streamed through record-by-record.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"skip-abort-thresholds\">6.2. Skip and Abort Thresholds</h3>\n<div class=\"paragraph\">\n<p>As detailed in the <a href=\"#scored-matching\">scored matching</a> section, the process of finding relations in TREE consists of generating groups of signatures based off of combinations of the scored matching rules, and then performing comparisons between each pair of records in a group using the matcher&#8217;s comparison operator to find relations.\nThis second step is very sensitive to the size of the groups themselves.\nSince each pair of records is compared, the number of comparison calculations performed grows quadratically with the size of the group.\nOnce a group reaches a modest size of 1,000, we are already at 1 million calculations.\nA group with over 30,000 records starts to approach a billion calculations, and will put too large a strain on a single reducer, vastly slowing down the matching process.\nFor this reason, there are built-in limits to the size of any individual group.</p>\n</div>\n<div class=\"paragraph\">\n<p>There are two safeguards we can impose on the groups, which are referred to as <strong>skip thresholds</strong> and <strong>abort thresholds</strong>.\nA skip threshold tells TREE to ignore or \"skip\" records that it encounters within the group after the buffer reaches a certain number of records.\nAn abort threshold tells TREE to throw away groups entirely once they&#8217;ve reached a terminal value.\nConsider the following matching config:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  ...\n  skipIfGroupExceeds 2000\n  abortIfGroupExceeds 5000</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This config tells TREE to process any groups that contain up to 5,000 records&#8212;&#8203;larger groups will be discarded altogether.\nIn addition, only the first 2,000 records will be examined within a group.\nThe 2,001st through 5,000th records in a group will be discarded.</p>\n</div>\n<div class=\"paragraph\">\n<p>By default, the abort threshold is set to <code>1,000</code>, and the the skip threshold is set to <code>Int.MaxValue</code> (effectively disabled).\nIf the abort threshold and skip threshold are both set to higher than 5,000, TREE may experience some significant slowdown if there exist groups that approach that size.\nThe skip threshold is not used by default, but should be kept to a number below the abort threshold for it to have any effect.</p>\n</div>\n<div class=\"paragraph\">\n<p>It may be the case that you do not want to throw out any groups.\nIf so, you should make sure to set the skip threshold to safeguard against very large groups, and set the abort threshold to the max:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  ...\n  skipIfGroupExceeds 2000\n  abortIfGroupExceeds Int.MaxValue</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This config will keep all groups, but never process more than 2,000 records per group.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\nRecall that for an <em>internal</em> matching step (<code>resolve &lt;sources&gt; using &lt;config&gt;</code>) all records participate in signature grouping. For a <em>targeted</em> matching step (<code>resolve &lt;sources&gt; against &lt;targets&gt; using &lt;config&gt;</code>), only the target sources are grouped by signatures, while the resolved sources are streamed through record-by-record.\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"standalone-mode\">6.3. Enabling Self-Edges</h3>\n<div class=\"paragraph\">\n<p>As discussed in the TREE manual, <strong>singletons</strong> are level-1 records that do not form any connections to other records.\nWhile they still receive a Tresata ID, singleton records are the only records to participate in future steps of the pipeline if that source is resolved again.\nIn addition, singletons can be removed from the set of target sources in a targeted matching step, using the <code>withoutSingletonsIn</code> method.</p>\n</div>\n<div class=\"paragraph\">\n<p>In a matching config, we can change which records are considered singletons.\nA <code>Config</code> object can enable <code>standalone</code> mode, where records that pass the matching criteria when compared to themselves are assigned <strong>self-edges</strong>.\nWhen standalone mode is enabled in an internal matching step, the set of singletons includes records that both do not match against other records, and don&#8217;t match against themselves.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  ...\n  standalone true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This config, when used in an internal matching step, will allow records to form edges against themselves, for the sole purpose of ensuring that such records are not considered singletons in later steps.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"relation-explanations\">6.4. Enabling Relation Explanations</h3>\n<div class=\"paragraph\">\n<p>By default, every record stored in the relations table will contain the pkeys of the records involved in the edge (stored in the <code>pkey1</code> and <code>pkey2</code> fields), the type of matching used to generate the edge (an <code>algo</code> field containing either \"piggy\" for piggybacks and \"prob\" for scored matching), and the total score of the edge using the specified type of matching (<code>score</code>).\nIn addition, an <code>explanation</code> field can be inserted, which details the scoring given from all the rules in the matching config that contributed to the total score.</p>\n</div>\n<div class=\"paragraph\">\n<p>To enable the <code>explanation</code> field in the relations table, we can toggle the <strong>explain</strong> setting in a matching config.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"prettyprint highlight\"><code data-lang=\"scala\">Config.init\n  ...\n  explain true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This setting causes TREE to insert a full explanation for the score of an edge. For example, an explanation with the value \"c0=0.0,c1=2.5\" implies that the edge was created using scored matching rules, with the second scored matching rule contributing 2.5 points. Similarly, an explanation with the value \"p0=100.0,p1=0.0\" means that the edge came from the first piggyback rule.</p>\n</div>\n<div class=\"paragraph\">\n<p>The overhead for providing explanations in the relations table is small, so performance shouldn&#8217;t be affected by this setting.\nIt&#8217;s a useful field to have for debugging or calculating statistics on the relations table, and can show you specifically which rules are making an impact on the creation of edges.</p>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169; 2011-2022 Tresata, Inc. All rights reserved.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footer\">\n<div id=\"footer-text\">\nLast updated 2022-10-01 18:39:27 -0400\n</div>\n</div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js\"></script>\n</body>\n</html>"
"For example:\n```\n---  \ncanonical: [name, address, phone, fullName]\nsources: \n  default:\n    pkey: pkey\n    transforms:\n      - type: name\n        from: name\n        to: [name]\n      - type: address\n        from: address\n        to: [address]\n      - type: expression\n        expression: \"join(' ', $countrycode, $phone)\"\n        to: phone\n      - type: scrubber\n        from: phone\n        to: phone\n        scrubber:\n          type: phone\n      - type: expression\n        expression: \"join(' ', $name.firstName, $name.lastName)\"\n        to: fullName\n    mapping: \n      name:\n        field: name\n      address:\n        field: address\n      phone:\n        field: phone\n      fullName:\n        field: fullName\n```\n\nthe exact syntax is documented in twig snapshot documentation (currently at https://server02.tresata.com:8084/artifactory/list/libs-local/com/tresata/tresata-doc/9.1.0-SNAPSHOT/tresata-doc-twig-9.1.0-SNAPSHOT.html but soon in discourse)"
"Great introduction! How can we download the package?"
"Great introduction, how can we proceed to download the package?"
"# Introduction to Tresata ORION\n\n### What is Tresata ORION?\n\nORION is a complex visualization tool that you can use to see relationship networks between the entities identified in TREE. You can build ORION graphs off of your processed data and link in key attributes to build a unique graph you can filter and query.\n\n\n### At Tresata, we use ORION to:\n\nVisualize complex connections. It's easy to imagine a person who knows a person, but being able to put a 7-degrees-of-separation-Kevin-Bacon graph up for every entity in your data set - well, that's revolutionary. ORION's primary use is in AML and fraud detection: it's easy to see money being sent and sent back and forth or find patterns. It's also useful to see an entity's network, whether that's a patient or provider, a bank, a store, or just about anything else. How does it all connect? ORION helps us and clients see and understand.\n\n![image|690x388](upload://50lg4Etsyv99jvrECpZeoPyoGOU.png)"
"# Introduction to Tresata TIDES\n\n### What is Tresata TIDES?\n\nTIDES (Tresata Intelligence Discovery & Exploration Starship) is Tresata’s software for visualizing cleansed and linked data after it’s been through the Tresata Data Factory to be ingested, cleansed, linked, and transformed in the necessary ways. TIDES is where we transition to using our Data Asset, full of all of the complex, high-volume data we’ve just processed (some companies will call this the Data Lake). TIDES takes a cleansed Data Asset and uses it to display graphs and visualizations. What sets TIDES apart from other big data visualization tools is its capacity to query all of the data instead of just a sample. If there are billions of records, TIDES can use every single one of them to show a holistic picture of the Data Asset.\n\n### At Tresata, we use TIDES to…\n\nUse the data! TIDES is the fun part of building a story using data to answer complex use-case scenarios and questions. Any problem a business user (the consumer of the data) wants to investigate can be done with TIDES graphs, aggregations, metrics, and charts. Because TIDES can query billions of data points, it’s easy to get down to a segment of one to inquire about a single entity, or to look at overall trends and make broad predictions and generalizations based on trends. This helps business users actually consume the data and customize it to their needs. The Tresata team will often make dashboards in a PoV or during a project to demonstrate results.\n\n# TIDES High-level Overview\n\n### Visualizing Data at Scale\n\nAfter creating a Data Asset, the next challenge any big enterprise will face is developing actionable insights from the data. Data visualization is one of the best ways to explore and generate value from your data. Enterprises use a variety of tools to visualize their data, but, typically, these tools operate poorly at-scale, thus requiring the use of sampling.\n\nTo truly understand your data at the unique entity level, or segment of one, requires scalable visualization software that reflects your entire Data Asset without compromising speed and efficiency.\n\nTresata TIDES is built precisely for the visualization of your at-scale Data Asset. TIDES allows data analysts and engineers to derive insights from all of their data quickly and efficiently. Through a complete view of their data with TIDES, enterprises will make better business decisions, while saving valuable time and resources.\n\n### About TIDES\n\nTresata TIDES is Tresata’s Intelligence Discovery & Exploration Starship, the core user interface for Tresata’s advanced analytics software that enables real-time discovery and visualization of intelligence of the Data Asset through a web interface.\n\nTIDES enables users to interactively explore and uncover actionable intelligence and relationships within the Data Asset. TIDES offers flexible data models, full-text search, and aggregation capabilities to discover and compute insights from Data Assets of any size, shape, and speed. The web interface enables users to interactively explore their Data Asset and create visualizations. Due to its distributed computing framework, TIDES can execute queries against large datasets in parallel and can scale linearly. TIDES’ app framework allows customers to quickly create custom reporting and analytics user facing apps on the web interface.\n\nTIDES is the perfect platform for data analysts and businesses to discover, visualize, and consume the actionable intelligence computed with our Digital Business Platform."
"Distribute is the final phase of the data analytics life cycle at Tresata, where we export the final single source of truth from raw and disparate data sources after Ingesting and Enriching them. This data asset could be migrated to any cloud platforms or used to power reporting tools to create visualisations and dashboards. Our key differentiator is the capability of the software to use Network Theory for knowledge distribution to generate directed acyclic Graphs."
"![academy icons - tresata.png\\ 28x28](upload://AttBaP3zbNlGIG9xKbLoBH5Cvk8.png) Tresata tree ng - 2021\n\n\n## TREE Workflow\n\n*This chapter provides a high level overview of the processes that TREE goes through before, during, and after Record Linkage*\n\n## High Level WorkFlow\n\nIn any implementation of TREE there is a standardized process that must be adhered to. This process ensures consistency in each application of TREE regardless of the industry or vertical it is being applied in. The steps in this process have been outlined in order below -\n\n![TREE Workflow|839x198](upload://msthFgBuZgwYaqS4nNnDxzqyWcB.png)\n\nTable 1. KEY|***Color***|***Description***|\n| --- | --- |\n|Blue|Tresata only step|\n|White|Anyone can perform|\n|Yellow|Need implementation|\n|Grey|Data Feeds|\n\n### R**ecord Linkage**\n\nDuring this step, TREE will link records together across the input tables and assign them a unique Tresata ID signifying their relationship.\n\nThis part of the process is proprietary to Tresata and as a result must be configured by one of it’s engineers. It’s during this step that the most can go wrong due to the sensitivity of the thresholds and parameters that must be tuned to link records together properly.\n\n### **Quality Assurance (QA)**\n\nOnce Tresata IDs have been generated for each identified entity, it’s important to go through and QA the results to ensure they line up with expectations and identify areas of improvement that can be added into the next run. This step will involve writing some custom scripts to generate metrics that aid in understanding the results from TREE. This process is typically iterative and should be performed after each successful TREE run.\n\n### **Iteration**\n\nOnce the results from TREE are stable and running in a production environment, sometimes minor issues can come up that require an update to the configuration. For example, there could be a change in the default value for a canonical field that is causing a hairball. Given we will not want this sort of linkage to continue, we can generate metrics from the current run, compare this to previous QA, identify the cause of this new linkage, and make an update to the existing configuration to correct the mistake moving forward. This sort of issue does not happen frequently, but it can be very detrimental to our process when it does. For this we must strive to have consistent, lightweight and standardized QA scripts to deploy between runs for all TREE implementations.\n\n### **Output Enrichment**\n\nTREE also provides roll-up/master for customizing the fields/values displayed in finalized tables. TREE has a built-in capability to allow users to specify a set of 'rules' that dictate which value best represents each Tresata ID for all the canonical fields. We define these rules in roll-up logic and master output. For example, we can configure 'best-ofs', which might deem the best name for an identified patient to be the most common one throughout history or the one used on their most recent doctor’s visit. This depends on whether we want to protect against rare typos or want to capture people’s preferred name or married name. This can apply to any field including, but not limited to, first name, line of work, phone number, etc.\n\n*This entity on the top would might roll up into the following:*\n\n*![rollup.png|419x230](upload://oVBtdKbSorezIIj3kQ6BA5Y4Wzo.png)*"
"\n# **TREE Glossary**\n\nBelow is a list of the technical terms used throughout this course. It is recommended to take a screenshot of this page so that it is easily accessible as you go through the course. Not all definitions will make sense until you have completed the course.\n\n### **Terms**\n\n* **Tresata ID -** A 32-digit code assigned by TREE to each record during record linkage. If two records are linked, they will both have the same Tresata ID. Tresata IDs identify which records are associated with a given *'entity'* . Always remember, Tresata IDs are unique only to an *entity* , NOT necessarily a data record (see: *Primary Key* ).\n* **Entity -** An entity can refer to a single person (i.e. a customer, patient, provider, *et cetera* ), a single company, or a single product, depending on the goal or scope of record linkage. TREE assigns a unique Tresata ID to each entity and adds that TID to all the records associated with that entity in order to link the records. Entities and Tresata IDs have a 1-to-1 relationship, and we often refer to an entity by its Tresata ID.\n* **Entity Resolution** - Synonym for *Record Linkage* and *Data Matching* ; The process of finding records in a data set that refer to the same entity, potentially across different data sources, and assigning them the same *Tresata ID* .\n\n  * Identification of which fields shall be used when comparing records.\n  * Laying out logic to determine if the values within a field should match (see: *Fuzzy* *Matching*)\n* **Matching Logic** - A set of rules by which TREE performs Entity Resolution. Matching Logic primarily consists of the following:\n  * Weighting the importance of these fields when linking records.\n  * Setting thresholds that a match must meet before records are linked and an entity is defined.\n* **Matching Config** - A Scala file containing *Matching Logic* for TREE. *Matching configs* always reference *canonical* field names (see: *canonical* ). These files are sensitive and proprietary, meaning they are often worked on outside of any client environment and packaged in such a way that only TREE can read them once they are finalized.\n* **Tuning** - The process of iteratively changing *Matching Logic* until TREE produces an optimal output.\n* **Level 1 Matching** - Also known as ‘L1’, this is the first step in TREE. This compares records in marked “Level-1 Tables” and determines what records should be linked, therefore becoming part of the same *entity* and assigned the same *Tresata ID.* This is the only step that can create entities, and as such Level 1 defines all Tresata IDs in a *Data Asset.*\n* **Level 2 Matching** - Also known as ‘L2’, this step in TREE occurs after Level-1 matching, and only transforms marked “Level-2” tables. The records in ‘L2’ tables are compared against the entities defined during ‘L1’. If they can be linked according to the provided *Matching Logic* , the record is assigned the TresataID for that entity (just as the records in L1 were). Records in L2 tables that cannot be *resolved* to any L1 entity are known as *Orphan Records* (which occasionally and daringly ask “Please sir, I want some more,” are promptly sent to the Governor, and shall rue the day somebody named them b1b572f67f2f4408871aed8a8844bab7). (see: *Orphan*\n* **Reference Data** - Data which would be expected to contain one record per entity with all available information about that entity (although this is often not the case). Examples would be patient rosters, customer loyalty tables, and bank account holder information. Reference tables are typically run in level 1 of TREE. (see: *Level-1 Matching* )\n* **Transactional Data** - Data which contains records of interactions between entities. Transactional data is expected to contain one record per interaction (although this is not always the case) and usually two entities per record. Examples include purchase history, money transfers, and hospital visits.\n* **Hierarchies of Identity** - The multiple levels of entities that can be resolved through record linkage. For example, one tier might resolve all the records that link to a single person, whereas the next tier resolves all the records associated with a single household (but will contain multiple people). Each hierarchy (or \"tier\") will have its own Tresata ID, meaning a record will have a tresata id for every tier.\n\n![hiearchiesOfIdentity.png|653x105](upload://8gZFQ59EG9CdgqxpvdiH5gWOgKG.png)\n\n* **Edge** - A connection between two records. TREE creates *edges* when it *links* two records. Typically, *reference data* can have multiple *edges* between records, whereas *transactional data* is only linked to a single *entity* found in reference data.\n* **Canonical** - This is a field mapping traditionally contained in a JSON format. It maps the field names from input data into a common naming convention that TREE uses internally in the *matching logic* .\n\nExample: We are doing L1 matching on a patient and provider table. The patient table has a field “last-name” and the provider table has a field “PROV_SURNAME”. For this, our canonical would likely map both “last-name” and “PROV_SURNAME” to a common field, such as “surname”.\n\n* **Staging -** The name of the folder on HDFS containing all input data that TREE will *resolve* .\n\n* **Output** - The name of the folder on HDFS containing all the tables in *Staging* , now with a *Tresata ID* associated with each record. Note that TIDs are not unique identifiers in *output* tables, as multiple records can be deemed part of the same *entity* and receive the same TID.\n\n* **Ingestion** - The act of taking raw data (in any format) and performing whatever transformations, aggregations, or conversions necessary for it to be compatible with our software. This step creates the *staging* tables.\n* **Primary Key** - Also called “pkey”, a unique key generated before running data through record linkage. It is useful to distinguish records after resolution, as a given Tresata ID will occur in every record associated with an entity and may not be unique in a TREE output table.\n\n![pkeys.png|576x126](upload://omjfXfJonz0QwTsFnNxi9UfEn0U.png)\n\n* **Data Enrichment** - The act of adding information to a record or entity that was not present before.\n\nFor example, data enrichment can be as simple as *denormalizing* tables, the process of adding new fields to a table (i.e. adding a customer’s “favorite color” to a customer loyalty table after they took an online survey.) *Entity resolution* is a much fancier kind of *enrichment* .\n\n* **Roll-up** - The process of taking the *output* of TREE (recall that in *output* tables, a TID is not unique to a record) and condensing each *entity* (and all its associated records) into one record per *entity* . Note that for the tables produced after *roll-up* , a TID will uniquely identify a single record.\n* **QA** - Short for “Quality Assurance”, this is the process by which the quality of the *output* of TREE is evaluated and and iteratively *tuned* . This is usually done by generating helpful statistics (number of entities containing multiple names, number of records per entity, etc.) and/or using a truth file, which contains several hand-linked entities and judges whether TREE produced a similar result.\n* **Singletons** - After *L1 matching* , where each record receives a Tresata ID, any that do not share a TresataID with any other records are known as *singletons* .\n* **Orphan** - After *L2 matching,* any records that do not resolve to an entity formed during L1 are not assigned a Tresata ID and are referred to as *orphan records* .\n* **Under Compression** - An issue in record linkage where the matching logic is too strict and therefore it is too difficult for records to match with one another. This can usually be found when seeing a high number of entities and a low number of records per entity, or when a known entity from a truth file has multiple tresata IDs after record linkage.\n* **Over Compression** - An issue in record linkage where the matching logic is too loose and therefore too many records improperly link with one another. This can usually be found when seeing a low number of entities and a high number of records per entity, or when several known entities from a truth file have a single Tresata ID after record linkage.\n* **Hairball** - A form of severe over compression in which loose matching logic allows many records to collapse into one entity because they share a common feature such as a phone number or address (often, “filler” values such as 123 Main St. and 123-456-7890). Hairballs significantly slow run times and should be identified during the QA process.\n* **Fuzzy Matching** - An umbrella term for any rules in matching logic that allow entities to be linked even if values differ slightly (i.e. company_name = “chik-fil-a” matches to company_name = “chick-fil-a”). Common types of *fuzzy matching* include Jaccard Similarity and OMHED."
"# TRUCK\n\nQ: I want to run a scala script on my dataset to remove unwanted newline characters. Can I do this in the spark shell? Or do I need to modify the script and run it using TRUCK tres-script?\n\nA: Either of these approaches will work, as tres-script is for one-off spark jobs when you don’t want to start a spark shell.\n\nQ: Is anyone familiar with the .kml extension? It's interchangeable with XML, and I want to convert it to a .bsv or .json format, but am not sure how to.\n\nA: You can use a tres-source-convert, as XML is an acceptable format. This should be in the TRUCK documentation on Tresata Academy for acceptable file formats.\n\nQ: If I want to convert my dataset into a json file, what would be the steps to do that?\n\nA: Use a tres-source-convert from the TRUCK toolkit.\n\nQ: How do I unzip a tar.gz file?\n\nA: You can use this command: tar -xf <file-name>.tar.gz"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"No more need to resort to freeform name parser for LAST, FIRST format. We now have one parser for both FIRST LAST and LAST, FIRST."
"In YAML:\n```\n{ \"weight\": 1.0, \"field\": \"w\", \"type\": \"name\", \"threshold\": 0.6, \"nicknames\": true }\n```\n\nin Scala:\n```\nmatchOn \"name\" score 1.0 using integrated.Name().withNicknames\nmatchOn \"name\" score 1.0 using integrated.NameParsed().withNicknames\nmatchOn \"name\" score 1.0 using integrated.NameFreeform().withNicknames\n```"
"Updates in the latest product snapshots, so hot off the press and before it makes it into a release and becomes generally available."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?\n"
"Yes for accessing the TREK installation on our internal servers you need a VPN"
"Do we need VPN to access TREK UI?"
"What are the data formats which TRUCK supports?"
"![Qc2jF3NlTiCwaWjlK4fi_access_trej2|video](upload://m7qG9RpMxNajJF5KuEHENo0Vzn3.mp4)\n\nyoutube.com/watch?v=v0zZ2u678aI"
"# What is Tresata's tool kit for data ingestion?\n\nIngesting data is a core function of any data management system. At Tresata the tool kit for ingesting data starts with TRUCK (Tresata Record Understanding and Collection Kit), Tresata's collection of software for Extracting, Loading, and Transforming massive amounts of data. Natively built for Hadoop, TRUCK gives engineers first-rate capability for building sophisticated data migration processes. TRUCK scales to ever-growing data inputs, allowing data engineers to run ELT operations that would previously take hours or days in a matter of minutes.\n### At Tresata, we use TRUCK to...\n\nExtract, load, and transform truly colossal data on a daily basis. You will quickly find that TRUCK becomes one of your most frequently used pieces of software, so take the time to familiarize yourself with it. Whether it's viewing a sample of a dataset stored on Hadoop, writing data to S3 or Elasticsearch, or automating a Spark job, TRUCK is an essential part of the data engineer's toolkit.\n\n# Overview\n\nTRUCK is a proprietary component of Tresata's tech stack that automates the preparation and ingestion of input data into Hadoop. TRUCK is extensively used by data teams for extracting, parsing, formatting, and preparing as-is data inputs that are then fed to Tresata's software products. TRUCK simplifies the process of extracting, merging, formating, serializing, and loading data sources. With the use of TRUCK’s declarative programing framework, data analysts can build complex ELT processes in a matter of hours which would otherwise take weeks and advanced skills.\n\n### Key Features\n\n* **EXTRACT** - Pull or push data in any Hadoop compatible formats, i.e. events, streams, files\n* **PROCESS** - Real-time, batch, or selective execution modes to ingest data at the desired rate\n* **BUILD** - Declarative data processing for analysts to develop custom data ingestion flows with ease\n* **SCALE** - Computing power of Hadoop to transform and ingest ever-growing data inputs\n* **AUTOMATE** - Create automated data pipelines by integrating TRUCK tools as workflow steps\n* **SECURE** - Works entirely in Hadoop cluster in 100% isolation, ensuring full enterprise security\n\n### TRUCK’s key features & functions enable you to:\n\n1. Dramatically quicken both the building process and the execution time for data transformations and ingestion pipelines\n2. Provide a robust and consistent data processing framework that can enforce standards and improve the overall quality of your deliverables\n3. Quickly extract and utilize vertical or horizontal subsets from any dataset in any format available on Hadoop\n4. Convert data to or from a variety of Hadoop compatible formats (Avro, BSV, CSV, JSON, and many more) using simple commands\n5. Read and write datasets to or from a variety of sources (like HDFS, Elasticsearch, Amazon S3, Hive, etc.)\n6. Automate manual and repetitive tasks which are resource intensive and error prone\n\n### Use Cases\n\n* Migrating data from a legacy database to a Hadoop data warehouse\n* Converting raw data sources into a format that is optimized for Hadoop, such as Avro or Parquet\n* Writing custom scripts to clean or transform inconsistent or poor quality data into a useable format\n* Inspecting the underlying architecture of data from any structured data"
"# TRUCK Tools\n\nTRUCK is comprised of a collection of tools that are used for ELT. This manual will walk through how to use each tool and give a better understanding of how each of these tools can be used to make data usable for application and insights.\n\nThe tools we will be covering in this manual are:\n\n1. tres-source-cat\n2. tres-source-convert\n3. tres-print-schema\n4. tres-script\n\nYou can find your installation of TRUCK in the folder `/opt/tresata`. This manual references TRUCK version 2.0.0\n\n### TRUCK Documentation\n\nThis module may be out of synch with respect to whichever version of TRUCK you are using. Be sure to check the artifact server, or the documentation hub for the most up to date tool names, arguments, and usages for the tresata TRUCK tooling.\n\n### Input and Output: The Source API\n\nMost, if not all, of our backend data transformation tool, use what we call the \"Source API\", this is a syntax of writing URLs and filenames that provides some configuration for how to read and write to and from these locations.\n\nBefore continuing to learn about this tool, please familiarize yourself, if you have not already, with the Source API documentation, access to which has been outlined in your learning path's resource manager.\n# tres-source-cat\n\ntres-source-cat allows the user to read in a file and send it to standard output using a variety of options to select the data. Data may be sampled vertically or horizontally. The resulting output includes a header and selected data. By default, the data will be printed in bsv format.\n\nThe general usage of tres-source-cat is\n\n```\ntres-source-cat --input <file-type>%<input-data-path> --[options]\n```\n\nFor example:\n\n```\ntres-source-cat --input csv%somefile.csv --json --limit 50\n```\n\n### Required Parameters\n\n1. **input**:\n\n  1. The pathname to the target file. Note that the input field begins with the file extension type followed by a % before the pathname.\n  2. The input selection may also include additional queries in the URL, formatted ?<field>=<value>.\n  3. The input section may also include date selectors, formatted __<selection>__.\n\n### Optional Parameters\n\n1. **csv**: Prints comma-separated output\n\n2. **json**: Prints JSON-per-line output.\n3. **delim \"<char>\"**: Determines delimiter between fields.\n4. **limit <int>**: Prints the specified number of lines starting from the head of the data to standard output.\n5. **sample <float>**: Accepts a number between 0 and 1 to nondeterministically sample a specific percentage of rows from the data.\n6. **project <field(s)>**: Includes all designated fields in the resulting output file. If left unspecified, the default selection is \"all.\" Any number of fields may be included, separated by commas; if more than one --project parameter is added the second will be ignored. If a field that does not exist is passed in, the process will fail.\n7. **today <YYYY-MM-DD>**: Runs the program as though the specified date were the current date. This is helpful for use in conjunction with date selectors.\n# tres-source-convert\n\ntres-source-convert reads from a data source and writes output to a file, converting the original file to the format type specified in the output path. tres-source-convert also has the capabilities to be able to convert files correctly, even if there are quoted fields using the delimiter of the original file. A good example would be a csv with a description field containing a value \"item 1, item 2, item 3\". tres-source-convert reads in the quoted field value in order to read in the whole dataset correctly.\n\nThe general usage of tres-source-convert is\n\n```\ntres-source-convert --input <file-type>%<input-data-path> --output <file-type>%<output-data-path> --[options]\n```\n\nFor example:\n\n```\ntres-source-convert --input bsv%/some-directory/some-file/__latest__ --output parq%my_output?save_mode=append --today 2017_03_31 --discard field1,field2\n```\n\n### Required Parameters\n\n1. **input**:\n  1. The pathname to the target file.\n  2. The input selection may also include additional queries in the URL, formatted ?<query>=<key>.\n  3. The input section may also include date selectors, formatted __<selection>__.\n2. **output**\n  1. The pathname to the exported file.\n  2. Special query keys can be used in conjunction with the input to specify parameters such as num_shards and save_mode.\n    1. **num_shards=<int>** breaks up the file into the number of specified partitions. num_shards=1 would create a single part file.\n    2. **save_mode=<selection>** specifies whether to append, overwrite, error_if_exists, or ignore.\n\n### Optional Parameters\n\n1. **sample <float>**: Accepts a number between 0 and 1 to nondeterministically sample a specific percentage of rows from the input data.\n2. **project <field(s)>**: Includes all designated fields in the resulting output file. If left unspecified, the default selection is \"all.\" Any number of fields may be included, separated by commas; if more than one --project parameter is added the second will be ignored. If a field that does not exist is passed in, the process will fail.\n3. **discard <field(s)>**: Eliminates unwanted fields from the resulting output file. Any number of fields may be discarded, separated by commas. The process will not fail if a field that does not exist is passed in.\n4. **today <YYYY-MM-DD>**: Runs the program as though the specified date were the current date. This is helpful for use in conjunction with date selectors.\n# tres-print-schema\n\ntres-print-schema allows the user to examine the schema of a dataset on hdfs.\n\nThe general usage of tres-print-schema is\n\n```\ntres-print-schema --input <file-type>%<input-data-path>\n```\n\nFor example:\n\n```\ntres-print-schema --input csv%somefile.csv\n```\n\n### Required Parameters\n\n1. **input**:\n  1. The pathname to the target file. Note that the input field begins with the file extension type followed by a % before the pathname.\n  2. The input section may also include date selectors, formatted __<selection>__.\n# tres-script\n\ntres-script generates a Spark job from a Scala script without necessitating a full sbt project run or loading a script into the Spark shell.\n\ntres-script is used primarily on HDFS to run one-off Spark jobs using Scala scripts that are not tied to full Scala projects. tres-script makes use of a standard set of core libraries; if you wish to use a set of custom libraries, a full Scala project will need to be configured. tres-script can be configured locally if needed but risks being partitioned into multiple output files and thus is not often used outside of HDFS.\n\nThe general usage of tres-script is\n\n`tres-script <scala_script> --input <file-type>%<file/path> --output <file-type>%<file/path>`\n\nFor example:\n\n`tres-script renameFields.scala --input bsv%/pathname/input_file.bsv --output bsv%/pathname/output_file.bsv --today 2011_04_28`\n\n### Example Script\n\n`import com.twitter.scalding.Args import com.tresata.scala.args.RichArgsimport com.tresata.spark.sql.Jobimport com.tresata.spark.sql.source.Source(args: Args) => new Job(args) {  import spark.implicits._     override def run: Unit = {    Source.forArg(\"input\").read.fieldsApi      .project('Src, 'Lat, 'Lon)      .rename(('Src, 'Lat, 'Lon) -> ('Source, 'Latitude, 'Longitude))      .write(Source.forArg(\"output\"))   } }`\n\nThe above script demonstrates the format of a Scala script used to change the names of column headers. The terminal command used to run this script would be structured:\n\n```\ntres-script renameFields.scala --input bsv%quake.bsv --output bsv%final-quake.bsv\n```\n\nThe terminal command has to contain the same words used in the Scala script; because \"input\" and \"output\" are used in the Scala script, they must also be used in the terminal command. In this instance, the input is read in from HDFS and the output is written to HDFS.\n\n### Personalized Arguments\n\nAnywhere the Source() command is used, the user can pass in custom parameters to query the data based on what data types exist in the file(s).\n\nAll commands in the terminal command following the Scala script begin with two dashes and the argument name.\n\nIf an argument is called more than once in the terminal command, only the first instance will be processed and all others will be ignored. However, if an argument is included in the Scala script but not called in the terminal command, the program will exit with an error.\n\nArguments cannot include capital letters; words are separated by dashes instead (\"my-argument\" instead of \"myArgument\").\n\n* **input and output files** begin with the file extension type followed by a % before the pathname.\n* **type selectors** can be passed into the script with the form `val variable: <type> = args.<type>(\"some-name\")`. For example, if a user wanted to find all instances of the number 45, they would enter `val variable: Long = args.long(\"my-long\")` and call it in the run script `--my-long 45`.\n* **optional arguments** can be included using args.optional(). Be sure to specify a default in the scala script in case an optional argument is not passed.\n`val today = localDate(args.optional(\"today\")).toString(\"YYYY-MM-DD\")` In this example, localDate will default to today’s date if no optional argument is given.\n\n### Tying this together with Bash\n\nA common pattern we use at Tresata for one off scripts is to combine tres-script, a scala script (containing a job), and a bash wrapper script. Taking the example above, rather than writing out the command above every time, we can group those into a wrapper script called \"rename_fields.sh\".\n\n`#! /bin/bashTODAY=\"$(date +%Y-%m-%d)\"INPUT=\"bsv%/pathname/input_file.bsv\"OUTPUT=\"bsv%/pathname/output_file.bsv\"tres-script renameFields.scala --input $INPUT --output $OUTPUT --today $TODAY`"
"### ingest\n\nbefore you can use your data, you need to know what you’ve got and where it is. **DBP corrals all of your data automatically, no matter where it’s stored** — in legacy databases and/or in the cloud / lake/ lake house/ warehouse/ fabric/ mesh, or whatever we’re calling it this week (we can’t even agree on a common language…it’s no wonder these environments have become complex).\n\nthe alphabet soup of acronyms (you know which ones we mean) involved means you’re wasting time getting formats standardized, data cleansed, and getting data ready. you need to go all-in with your data — which means you need to put every last bit of it to work, but this is unfeasible at scale without automation."
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?"
"(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)\n\nUse the following paragraphs for a longer description, or to establish category guidelines or rules:\n\n- Why should people use this category? What is it for?\n\n- How exactly is this different than the other categories we already have?\n\n- What should topics in this category generally contain?\n\n- Do we need this category? Can we merge with another category, or subcategory?"
"[tresata-truck-3.0.0-SNAPSHOT.pdf|attachment](upload://5nlTL4ZSiCUo8PDwReBU2yHbBKt.pdf) (342.7 KB)"
"Ingesting data is a core function of any data management system. At Tresata the tool kit for ingesting data starts with TRUCK (Tresata Record Understanding and Collection Kit), Tresata's collection of software for Extracting, Loading, and Transforming massive amounts of data. Natively built for Hadoop, TRUCK gives engineers first-rate capability for building sophisticated data migration processes. TRUCK scales to ever-growing data inputs, allowing data engineers to run ELT operations that would previously take hours or days in a matter of minutes."
"That is good. What all methods, Input / Outputs are supported by Tresata Data Ingestion Software?"
"Data ingestion is **the process of importing large, assorted data files from multiple sources into a single, cloud-based storage medium** —a data warehouse, data mart or database—where it can be accessed and analyzed."
"Data ingestion is **the process of importing large, assorted data files from multiple sources into a single, cloud-based storage medium** —a data warehouse, data mart or database—where it can be accessed and analyzed."
"TOPICS RELATED TO DATA MIGRATION FROM DIFFERENT SOURCES TO TARGET WORKSPACE AND DATA CLASSIFICATION"
"Thanks for trying Discourse!\r\n\r\n@ashwani.sharma, here are some tips for launching a successful discussion community:\r\n\r\n### What is the \"elevator pitch\" for your community?\r\n\r\nThe first thing people will ask: **what is this place?** How would you describe your community to someone you just met in a 60 second elevator ride? Go back and look at your site's [Welcome Topic](/t/welcome-to-our-community/7), and edit it until until you are happy with it. \r\n\r\n### Build some interesting discussions to launch with\r\n\r\n - :bulb: What comes up often in your internal emails? Are there common themes that tend to come up again and again with your fans, customers, users, patrons, subscribers? Try moving those discussions out of private email silos into your Discourse community.\r\n\r\n - :sunglasses: If you find a cool link you want to discuss, quickly start a new topic by **pasting a link into the topic title**. Try it!\r\n\r\n- * :speech_balloon: Have some open-ended getting to know you topics for people to share their opinions, experiences, stories, or pictures. An “introduce yourself” topic is always fun, and you should go first!\r\n\r\n - :hugs: What topics do you *want* your community to create? Imagine what a model user you would love to see on your site would do – and then try doing that yourself. Create multiple accounts if you need to; post example topics and reply to them so visitors can browse the existing conversations to discover what your community is about. \r\n\r\n### Get the right people in the room\r\n\r\n- :email: [Send personal invitations](/my/invited) to your staff, power users, or friends. Ask them to help you by logging in and replying to your initial topics to generate activity. You can also create invite links and share them to many people at once.\r\n\r\n- :heart: Generously **like** any and every post you enjoy! What type of content gets liked is a major part of your community's culture. Set an example by frequently liking posts in the early days of your forum. Seeing liked posts also encourages people to reciprocate in kind, and come back for more.\r\n\r\n - :muscle: Actively seek the help of power users and early adopters in your community. There's a built in [feedback category](/c/site-feedback) for discussing organization and governance. Let your most avid users have a say in what your community does, how the site works, and what your community becomes.\r\n\r\n### How do people find your community?\r\n\r\n - :link: Where can you place links to your community so that people will naturally discover it? In the header or footer of your website? Where else?\r\n\r\n - :mega: Promote your community. Add a note to your mailing lists or email newsletters, put up a notice on your website, or make a blog entry about your new community.\r\n\r\n - :trophy: What rewards, perks, contests, or incentives can you give people for signing up, for posting, for replying? Check [your user directory](/u) to see engagement statistics, and shower your best users with attention to encourage them.\r\n\r\nFor additional advice, see our [blog post on how to build engaging Discourse communities](http://blog.discourse.org/2014/08/building-a-discourse-community/) or watch our [three part video series on getting started with Discourse](https://blog.discourse.org/2022/02/getting-started-with-discourse-by-jono-bacon-part-1/). \r\n\r\nGood luck! Building a community takes patience and persistence. :sweat_smile: If we can help, email team@discourse.org any time.\r\n\r\nDiscourse Team\r\n<https://discourse.org>"
"Hello and welcome to your 14 day free trial of Discourse! \r\n\r\nAs a [standard hosted customer](https://discourse.org/buy), you'll get:\r\n\r\n- The [full Discourse feature set](https://discourse.org/about)\r\n- High speed hosting\r\n- **100k** monthly page views\r\n- **20GB** storage\r\n- **100k** monthly emails\r\n- Unlimited members\r\n- 5 staff users\r\n- Single sign-on\r\n- Global CDN\r\n- [Standard plugins](https://www.discourse.org/plugins#standard)\r\n- Regular updates and backups\r\n\r\nThere are some good tips on basic configuration in [READ ME FIRST: Getting Started](/t/read-me-first-getting-started).\r\n\r\nYou can manage your trial, billing contact info, and payment method from [your admin dashboard](/admin).\r\n\r\nFeel free to ask us any time if you have questions, **email us at team@discourse.org**. We're happy to assist.\r\n\r\nThanks and enjoy. :smile: \r\n\r\nJeff Atwood"
"Thanks for signing up for a [Discourse](https://www.discourse.org) hosting plan!\n\n![](upload://cjD43yJ8E3Pc8gpntdEahwP83qq.jpeg)\n\n### Getting started\n\nIf you haven't already, [complete the Setup Wizard](/wizard) and go through the steps to configure your site. You can run the wizard as many times as you want, it's completely safe!\n\nThen the first thing you want to do is [edit your Welcome Topic](/t/welcome-to-our-community/7), the first thing new members will read when they arrive on your site. Think of it as your “elevator pitch” or “mission statement.” Let everyone know who this community is for, what they can expect to find here, and what you’d like them to do first.\n\nNow you're ready to invite your founding members! Go to [your invites page](/my/invited) and look for the <kbd>+ Invite</kbd> button to create invite links you can share yourself or directly send by email to everyone you want to have in your community. **Be sure to follow up to make sure they join and start participating!** \n\n### Need help?\n\n- Reach out to us any time at [team@discourse.org](mailto:team@discourse.org) with questions about making the most of Discourse. We also are very responsive to feedback and feature suggestions! \n\n- Join [meta.discourse.org](https://meta.discourse.org/), our official community, to discuss features, bugs, hosting, development, and general support with other Discourse users.\n\n### Admin dashboard\n\nExercise your admin superpowers any time via the [admin dashboard](/admin). You can also access it via the :wrench: admin link on the menu. Admin functions are generally marked with the wrench :wrench: icon, so look for that.\n\n### Staff\n\nStaff members are official representatives of this community. There are two kinds of Staff:\n\n1. **Admins**, who can do anything and configure anything on this site.\n2. **Moderators**, who can edit all posts and users, but cannot add categories or change any site settings.\n\nPromoting members of your community is easy: \n\n- select :wrench: admin wrench on their user page\n- look for the <kbd>Grant Admin</kbd> and <kbd>Grant Moderator</kbd> buttons there\n\n### Categories\n\nYou have three default categories:\n\n1. [General](/category/general) – create topics here that don’t fit into any other existing category.\n2. [Site Feedback](/category/site-feedback) – Discussion about this site, its organization, how it works, and how you and your community can improve it. [It's important!](https://meta.discourse.org/t/5249)\n3. [Staff](/category/staff) – Visible only to staff (admins and moderators)\n\n**Don't create too many initial categories**, as you can overwhelm your community. You can always add more categories later, and easily bulk recategorize topics. You and your members will have a better experience if you figure out the organization as you go rather than assuming you'll get it all right from the beginning.\n\nTo add a category, select the :wrench: admin wrench on the [categories page](/categories). You can set security per-category so only certain groups of users can see topics in that category.\n\nEvery category has an initial \"About this category\" topic which you will want to edit to suit your needs. This topic will be pinned to the top of the category, and the description you enter in the first paragraph will appear throughout. Be sure to give your new category a good, clear description, so people understand what belongs there!\n\nIn addition to categories, Discourse allows you to organize topics with tags. Tags offer a flexible alternative to categories. Create tags when editing topics. \n\n### Pinned topics and banners\n\nNote how pinning topics works in Discourse:\n\n- Once someone reads to the bottom of a pinned topic, it is automatically unpinned for them specifically. They can change this via the personal pin controls at the bottom of the topic.\n- When staff pins a topic, they can pin it globally to all topic lists, or just within its category.\n\nIf a pin isn't visible enough, you can also turn one single topic into a **banner**. The banner topic floats on top of all topics and all primary pages. Users can permanently dismiss this floating banner by clicking the &times; in the upper right corner.\n\nTo make (or remove) a pin or banner, use the topic :wrench: admin wrench.\n\n### New user restrictions and the trust system\n\nDiscourse has a [trust level system](https://blog.discourse.org/2018/06/understanding-discourse-trust-levels/) where users earn the trust of the community over time and gain abilities to assist in governing their community. The trust level system is designed to offer safe defaults for communities, even for public sites with no active moderation.\n\n> **0 (new) &rarr; 1 (basic) &rarr; 2 (member) &rarr; 3 (regular) &rarr; 4 (leader)**\n\nOn all of our plans, users you invite directly will start at trust level 1 (basic), and earn more trust as they use the site.\n\nOn the Standard and Business plan, your site will begin in “bootstrap mode” until you have 50 users. While in bootstrap mode, users will join as trust level 1 (basic). Afterwards, new users will join as trust level 0 (new) and must read for 15 minutes before being promoted to remove some initial restrictions on what they can post. You can turn off bootstrap mode via the [admin settings](https://dev.discourse.org/admin/site_settings/category/all_results?filter=bootstrap).\n\n### More guidance on building your community\n\nBuilding communities can take some effort. At the outset, be sure to:\n\n1. Define your community's purpose in the site Welcome Topic.\n2. Seed the discussion with interesting topics.\n3. Commit to visiting and participating regularly.\n4. Invite everyone you want to have in your community, and follow up to make sure they join. \n5. For public sites (plans Standard and up) link your community everywhere and promote it so people can find it.\n\nThere's more advice at [Building a Discourse Community](http://blog.discourse.org/2014/08/building-a-discourse-community/).\n\n### Advanced options\n\nHere are some other features that you might want to use:\n\n- Your invites page also includes advanced methods of [sending bulk invites](https://meta.discourse.org/t/sending-bulk-user-invites/16468), and [inviting users into groups](https://meta.discourse.org/t/invite-individual-users-to-a-group/15544).\n\n- Embed Discourse [in your WordPress install](https://github.com/discourse/wp-discourse), or [on your static HTML site](https://meta.discourse.org/t/embedding-discourse-comments-via-javascript/31963)\n\nUsers can log in with traditional local username and password accounts. You may want to add:\n\n- [Google logins](https://meta.discourse.org/t/configuring-google-oauth2-login-for-discourse/15858)\n- [Twitter logins](https://meta.discourse.org/t/configuring-twitter-login-for-discourse/13395)\n- [Facebook logins](https://meta.discourse.org/t/configuring-facebook-login-for-discourse/13394)\n- [GitHub logins](https://meta.discourse.org/t/configuring-github-login-for-discourse/13745)\n\nYou can also [set up single-sign on](https://meta.discourse.org/t/official-single-sign-on-for-discourse/13045), or even [build your own login method](https://meta.discourse.org/t/login-to-discourse-with-custom-oauth2-provider/14717)."
"Edit the first post in this topic to change the contents of the Privacy Policy page."
"<a name=\"collect\"></a>\n\n## [What information do we collect?](#collect)\n\nWe collect information from you when you register on our site and gather data when you participate in the forum by reading, writing, and evaluating the content shared here.\n\nWhen registering on our site, you may be asked to enter your name and e-mail address. You may, however, visit our site without registering. Your e-mail address will be verified by an email containing a unique link. If that link is visited, we know that you control the e-mail address.\n\nWhen registered and posting, we record the IP address that the post originated from. We also may retain server logs which include the IP address of every request to our server.\n\n<a name=\"use\"></a>\n\n## [What do we use your information for?](#use)\n\nAny of the information we collect from you may be used in one of the following ways:\n\n* To personalize your experience &mdash; your information helps us to better respond to your individual needs.\n* To improve our site &mdash; we continually strive to improve our site offerings based on the information and feedback we receive from you.\n* To improve customer service &mdash; your information helps us to more effectively respond to your customer service requests and support needs.\n* To send periodic emails &mdash; The email address you provide may be used to send you information, notifications that you request about changes to topics or in response to your user name, respond to inquiries, and/or other requests or questions.\n\n<a name=\"protect\"></a>\n\n## [How do we protect your information?](#protect)\n\nWe implement a variety of security measures to maintain the safety of your personal information when you enter, submit, or access your personal information.\n\n<a name=\"data-retention\"></a>\n\n## [What is your data retention policy?](#data-retention)\n\nWe will make a good faith effort to:\n\n* Retain server logs containing the IP address of all requests to this server no more than 90 days.\n* Retain the IP addresses associated with registered users and their posts no more than 5 years.\n\n<a name=\"cookies\"></a>\n\n## [Do we use cookies?](#cookies)\n\nYes. Cookies are small files that a site or its service provider transfers to your computer's hard drive through your Web browser (if you allow). These cookies enable the site to recognize your browser and, if you have a registered account, associate it with your registered account.\n\nWe use cookies to understand and save your preferences for future visits and compile aggregate data about site traffic and site interaction so that we can offer better site experiences and tools in the future. We may contract with third-party service providers to assist us in better understanding our site visitors. These service providers are not permitted to use the information collected on our behalf except to help us conduct and improve our business.\n\n<a name=\"disclose\"></a>\n\n## [Do we disclose any information to outside parties?](#disclose)\n\nWe do not sell, trade, or otherwise transfer to outside parties your personally identifiable information. This does not include trusted third parties who assist us in operating our site, conducting our business, or servicing you, so long as those parties agree to keep this information confidential. We may also release your information when we believe release is appropriate to comply with the law, enforce our site policies, or protect ours or others rights, property, or safety. However, non-personally identifiable visitor information may be provided to other parties for marketing, advertising, or other uses.\n\n<a name=\"third-party\"></a>\n\n## [Third party links](#third-party)\n\nOccasionally, at our discretion, we may include or offer third party products or services on our site. These third party sites have separate and independent privacy policies. We therefore have no responsibility or liability for the content and activities of these linked sites. Nonetheless, we seek to protect the integrity of our site and welcome any feedback about these sites.\n\n<a name=\"coppa\"></a>\n\n## [Children's Online Privacy Protection Act Compliance](#coppa)\n\nOur site, products and services are all directed to people who are at least 13 years old or older. If this server is in the USA, and you are under the age of 13, per the requirements of COPPA ([Children's Online Privacy Protection Act](https://en.wikipedia.org/wiki/Children%27s_Online_Privacy_Protection_Act)), do not use this site.\n\n<a name=\"online\"></a>\n\n## [Online Privacy Policy Only](#online)\n\nThis online privacy policy applies only to information collected through our site and not to information collected offline.\n\n<a name=\"consent\"></a>\n\n## [Your Consent](#consent)\n\nBy using our site, you consent to our web site privacy policy.\n\n<a name=\"changes\"></a>\n\n## [Changes to our Privacy Policy](#changes)\n\nIf we decide to change our privacy policy, we will post those changes on this page.\n\nThis document is CC-BY-SA. It was last updated May 31, 2013."
"Edit the first post in this topic to change the contents of the FAQ/Guidelines page."
"<a name=\"civilized\"></a>\n\n## [This is a Civilized Place for Public Discussion](#civilized)\n\nPlease treat this discussion forum with the same respect you would a public park. We, too, are a shared community resource &mdash; a place to share skills, knowledge and interests through ongoing conversation.\n\nThese are not hard and fast rules. They are guidelines to aid the human judgment of our community and keep this a kind, friendly place for civilized public discourse.\n\n<a name=\"improve\"></a>\n\n## [Improve the Discussion](#improve)\n\nHelp us make this a great place for discussion by always adding something positive to the discussion, however small. If you are not sure your post adds to the conversation, think over what you want to say and try again later.\n\nOne way to improve the discussion is by discovering ones that are already happening. Spend time browsing the topics here before replying or starting your own, and you’ll have a better chance of meeting others who share your interests.\n\nThe topics discussed here matter to us, and we want you to act as if they matter to you, too. Be respectful of the topics and the people discussing them, even if you disagree with some of what is being said.\n\n<a name=\"agreeable\"></a>\n\n## [Be Agreeable, Even When You Disagree](#agreeable)\n\nYou may wish to respond by disagreeing. That’s fine. But remember to _criticize ideas, not people_. Please avoid:\n\n* Name-calling\n* Ad hominem attacks\n* Responding to a post’s tone instead of its actual content\n* Knee-jerk contradiction\n\nInstead, provide thoughtful insights that improve the conversation.\n\n<a name=\"participate\"></a>\n\n## [Your Participation Counts](#participate)\n\nThe conversations we have here set the tone for every new arrival. Help us influence the future of this community by choosing to engage in discussions that make this forum an interesting place to be &mdash; and avoiding those that do not.\n\nDiscourse provides tools that enable the community to collectively identify the best (and worst) contributions: bookmarks, likes, flags, replies, edits, watching, muting and so forth. Use these tools to improve your own experience, and everyone else’s, too.\n\nLet’s leave our community better than we found it.\n\n<a name=\"flag-problems\"></a>\n\n## [If You See a Problem, Flag It](#flag-problems)\n\nModerators have special authority; they are responsible for this forum. But so are you. With your help, moderators can be community facilitators, not just janitors or police.\n\nWhen you see bad behavior, don’t reply. Replying encourages bad behavior by acknowledging it, consumes your energy, and wastes everyone’s time. _Just flag it_. If enough flags accrue, action will be taken, either automatically or by moderator intervention.\n\nIn order to maintain our community, moderators reserve the right to remove any content and any user account for any reason at any time. Moderators do not preview new posts; the moderators and site operators take no responsibility for any content posted by the community.\n\n<a name=\"be-civil\"></a>\n\n## [Always Be Civil](#be-civil)\n\nNothing sabotages a healthy conversation like rudeness:\n\n* Be civil. Don’t post anything that a reasonable person would consider offensive, abusive, or hate speech.\n* Keep it clean. Don’t post anything obscene or sexually explicit.\n* Respect each other. Don’t harass or grief anyone, impersonate people, or expose their private information.\n* Respect our forum. Don’t post spam or otherwise vandalize the forum.\n\nThese are not concrete terms with precise definitions &mdash; avoid even the _appearance_ of any of these things. If you’re unsure, ask yourself how you would feel if your post was featured on the front page of a major news site.\n\nThis is a public forum, and search engines index these discussions. Keep the language, links, and images safe for family and friends.\n\n<a name=\"keep-tidy\"></a>\n\n## [Keep It Tidy](#keep-tidy)\n\nMake the effort to put things in the right place, so that we can spend more time discussing and less cleaning up. So:\n\n* Don’t start a topic in the wrong category; please read the category definitions.\n* Don’t cross-post the same thing in multiple topics.\n* Don’t post no-content replies.\n* Don’t divert a topic by changing it midstream.\n* Don’t sign your posts &mdash; every post has your profile information attached to it.\n\nRather than posting “+1” or “Agreed”, use the Like button. Rather than taking an existing topic in a radically different direction, use Reply as a Linked Topic.\n\n<a name=\"stealing\"></a>\n\n## [Post Only Your Own Stuff](#stealing)\n\nYou may not post anything digital that belongs to someone else without permission. You may not post descriptions of, links to, or methods for stealing someone’s intellectual property (software, video, audio, images), or for breaking any other law.\n\n<a name=\"power\"></a>\n\n## [Powered by You](#power)\n\nThis site is operated by your [friendly local staff](/about) and *you*, the community. If you have any further questions about how things should work here, open a new topic in the [site feedback category](/c/site-feedback) and let’s discuss! If there’s a critical or urgent issue that can’t be handled by a meta topic or flag, contact us via the [staff page](/about).\n\n<a name=\"tos\"></a>\n\n## [Terms of Service](#tos)\n\nYes, legalese is boring, but we must protect ourselves &ndash; and by extension, you and your data &ndash; against unfriendly folks. We have a [Terms of Service](/tos) describing your (and our) behavior and rights related to content, privacy, and laws. To use this service, you must agree to abide by our [TOS](/tos)."
"Edit the first post in this topic to change the contents of the Terms of Service page."
"These terms govern use of the Internet forum at <http://tresata1.discourse.group>. To use the forum, you must agree to these terms with Tresata, Inc., the company that runs the forum.\n\nThe company may offer other products and services, under different terms. These terms apply only to use of the forum.\n\nSkip to:\n\n- [Important Terms](#heading--important-terms)\n- [Your Permission to Use the Forum](#heading--permission)\n- [Conditions for Use of the Forum](#heading--conditions)\n- [Acceptable Use](#heading--acceptable-use)\n- [Content Standards](#heading--content-standards)\n- [Enforcement](#heading--enforcement)\n- [Your Account](#heading--your-account)\n- [Your Content](#heading--your-content)\n- [Your Responsibility](#heading--responsibility)\n- [Disclaimers](#heading--disclaimers)\n- [Limits on Liability](#heading--liability)\n- [Feedback](#heading--feedback)\n- [Termination](#heading--termination)\n- [Disputes](#heading--disputes)\n- [General Terms](#heading--general)\n- [Contact](#heading--contact)\n- [Changes](#heading--changes)\n\n<h2 id=\"heading--important-terms\"><a href=\"#heading--important-terms\">Important Terms</a></h2>\n\n***These terms include a number of important provisions that affect your rights and responsibilities, such as the disclaimers in [Disclaimers](#heading--disclaimers), limits on the company's liability to you in [Limits on Liability](#heading--liability), your agreement to cover the company for damages caused by your misuse of the forum in [Responsibility for Your Use](#heading--responsibility), and an agreement to arbitrate disputes in [Disputes](#heading--disputes).***\n\n<h2 id=\"heading--permission\"><a href=\"#heading--permission\">Your Permission to Use the Forum</a></h2>\n\nSubject to these terms, the company gives you permission to use the forum. Everyone needs to agree to these terms to use the forum.\n\n<h2 id=\"heading--conditions\"><a href=\"#heading--conditions\">Conditions for Use of the Forum</a></h2>\n\nYour permission to use the forum is subject to the following conditions:\n\n1. You must be at least thirteen years old.\n\n2. You may no longer use the forum if the company contacts you directly to say that you may not.\n\n3. You must use the forum in accordance with [Acceptable Use](#heading--acceptable-use) and [Content Standards](#heading--content-standards).\n\n<h2 id=\"heading--acceptable-use\"><a href=\"#heading--acceptable-use\">Acceptable Use</a></h2>\n\n1. You may not break the law using the forum.\n\n2. You may not use or try to use another's account on the forum without their specific permission.\n\n3. You may not buy, sell, or otherwise trade in user names or other unique identifiers on the forum.\n\n4. You may not send advertisements, chain letters, or other solicitations through the forum, or use the forum to gather addresses or other personal data for commercial mailing lists or databases.\n\n5. You may not automate access to the forum, or monitor the forum, such as with a web crawler, browser plug-in or add-on, or other computer program that is not a web browser. You may crawl the forum to index it for a publicly available search engine, if you run one.\n\n6. You may not use the forum to send e-mail to distribution lists, newsgroups, or group mail aliases.\n\n7. You may not falsely imply that you're affiliated with or endorsed by the company.\n\n8. You may not hyperlink to images or other non-hypertext content on the forum on other webpages.\n\n9. You may not remove any marks showing proprietary ownership from materials you download from the forum.\n\n10. You may not show any part of the forum on other websites with `<iframe>`.\n\n11. You may not disable, avoid, or circumvent any security or access restrictions of the forum.\n\n12. You may not strain infrastructure of the forum with an unreasonable volume of requests, or requests designed to impose an unreasonable load on information systems underlying the forum.\n\n13. You may not impersonate others through the forum.\n\n14. You may not encourage or help anyone in violation of these terms.\n\n<h2 id=\"heading--content-standards\"><a href=\"#heading--content-standards\">Content Standards</a></h2>\n\n1. You may not submit content to the forum that is illegal, offensive, or otherwise harmful to others. This includes content that is harassing, inappropriate, abusive, or hateful conduct.\n\n2. You may not submit content to the forum that violates the law, infringes anyone's intellectual property rights, violates anyone's privacy, or breaches agreements you have with others.\n\n3. You may not submit content to the forum containing malicious computer code, such as computer viruses or spyware.\n\n4. You may not submit content to the forum as a mere placeholder, to hold a particular address, user name, or other unique identifier.\n\n5. You may not use the forum to disclose information that you don't have the right to disclose, like others' confidential or personal information.\n\n<h2 id=\"heading--enforcement\"><a href=\"#heading--enforcement\">Enforcement</a></h2>\n\nThe company may investigate and prosecute violations of these terms to the fullest legal extent. The company may notify and cooperate with law enforcement authorities in prosecuting violations of the law and these terms.\n\nThe company reserves the right to change, redact, and delete content on the forum for any reason. If you believe someone has submitted content to the forum in violation of these terms, [contact us immediately](#heading--contact).\n\n<h2 id=\"heading--your-account\"><a href=\"#heading--your-account\">Your Account</a></h2>\n\nYou must create and log into an account to use some features of the forum.\n\nTo create an account, you must provide some information about yourself. If you create an account, you agree to provide, at a minimum, a valid e-mail address, and to keep that address up-to-date. You may close your account at any time by e-mailing <<ins>contact_email</ins>>.\n\nYou agree to be responsible for all action taken using your account, whether authorized by you or not, until you either close your account or notify the company that your account has been compromised. You agree to notify the company immediately if you suspect your account has been compromised. You agree to select a secure password for your account, and keep it secret.\n\nThe company may restrict, suspend, or close your account on the forum according to its policy for handling copyright-related takedown requests, or if the company reasonably believes that you've broken any rule in these terms.\n\n<h2 id=\"heading--your-content\"><a href=\"#heading--your-content\">Your Content</a></h2>\n\nNothing in these terms gives the company any ownership rights in intellectual property that you share with the forum, such as your account information, posts, or other content you submit to the forum. Nothing in these terms gives you any ownership rights in the company's intellectual property, either.\n\nBetween you and the company, you remain solely responsible for content you submit to the forum. You agree not to wrongly imply that content you submit to the forum is sponsored or approved by the company. These terms do not obligate the company to store, maintain, or provide copies of content you submit, and to change it, according to these terms.\n\nContent you submit to the forum belongs to you, and you decide what permission to give others for it. But at a minimum, you license the company to provide content that you submit to the forum to other users of the forum. That special license allows the company to copy, publish, and analyze content you submit to the forum.\n\nWhen content you submit is removed from the forum, whether by you or by the company, the company's special license ends when the last copy disappears from the company's backups, caches, and other systems. Other licenses you apply to content you submit, such as [Creative Commons](https://creativecommons.org) licenses, may continue after your content is removed. Those licenses may give others, or the company itself, the right to share your content through the forum again.\n\nOthers who receive content you submit to the forum may violate the terms on which you license your content. You agree that the company will not be liable to you for those violations or their consequences.\n\n<h2 id=\"heading--responsibility\"><a href=\"#heading--responsibility\">Your Responsibility</a></h2>\n\nYou agree to indemnify the company from legal claims by others related to your breach of these terms, or breach of these terms by others using your account on the forum. Both you and the company agree to notify the other side of any legal claims for which you might have to indemnify the company as soon as possible. If the company fails to notify you of a legal claim promptly, you won't have to indemnify the company for damages that you could have defended against or mitigated with prompt notice. You agree to allow the company to control investigation, defense, and settlement of legal claims for which you would have to indemnify the company, and to cooperate with those efforts. The company agrees not to agree to any settlement that admits fault for you or imposes obligations on you without your prior agreement.\n\n<h2 id=\"heading--disclaimers\"><a href=\"#heading--disclaimers\">Disclaimers</a></h2>\n\n***You accept all risk of using the forum and content on the forum. As far as the law allows, the company and its suppliers provide the forum as is, without any warranty whatsoever.***\n\nThe forum may hyperlink to and integrate forums and services run by others. The company does not make any warranty about services run by others, or content they may provide. Use of services run by others may be governed by other terms between you and the one running service.\n\n<h2 id=\"heading--liability\"><a href=\"#heading--liability\">Limits on Liability</a></h2>\n\n***Neither the company nor its suppliers will be liable to you for breach-of-contract damages their personnel could not have reasonably foreseen when you agreed to these terms.***\n\n***As far as the law allows, the total liability to you for claims of any kind that are related to the forum or content on the forum will be limited to $50.***\n\n<h2 id=\"heading--feedback\"><a href=\"#heading--feedback\">Feedback</a></h2>\n\nThe company welcomes your feedback and suggestions for the forum. See the [Contact](#heading--contact) section below for ways to get in touch with us.\n\nYou agree that the company will be free to act on feedback and suggestions you provide, and that the company won't have to notify you that your feedback was used, get your permission to use it, or pay you. You agree not to submit feedback or suggestions that you believe might be confidential or proprietary, to you or others.\n\n<h2 id=\"heading--termination\"><a href=\"#heading--termination\">Termination</a></h2>\n\nEither you or the company may end the agreement written out in these terms at any time. When our agreement ends, your permission to use the forum also ends.\n\nThe following provisions survive the end of our agreement: [Your Content](#heading--your-content), [Feedback](#heading--feedback), [Your Responsibility](#heading--responsibility), [Disclaimers](#heading--disclaimers), [Limits on Liability](#heading--liability), and [General Terms](#heading--general).\n\n<h2 id=\"heading--disputes\"><a href=\"#heading--disputes\">Disputes</a></h2>\n\nNorth Carolina law will govern any dispute related to these terms or your use of the forum.\n\nYou and the company agree to seek injunctions related to these terms only in state or federal court in Charlotte, North Carolina. Neither you nor the company will object to jurisdiction, forum, or venue in those courts.\n\n***Other than to seek an injunction or for claims under the Computer Fraud and Abuse Act, you and the company will resolve any dispute by binding American Arbitration Association arbitration. Arbitration will follow the AAA's Commercial Arbitration Rules and Supplementary Procedures for Consumer Related Disputes. Arbitration will happen in Charlotte, North Carolina. You will settle any dispute as an individual, and not as part of a class action or other representative proceeding, whether as the plaintiff or a class member. No arbitrator will consolidate any dispute with any other arbitration without the company's permission.***\n\nAny arbitration award will include costs of the arbitration, reasonable attorneys' fees, and reasonable costs for witnesses. You and the company may enter arbitration awards in any court with jurisdiction.\n\n<h2 id=\"heading--general\"><a href=\"#heading--general\">General Terms</a></h2>\n\nIf a provision of these terms is unenforceable as written, but could be changed to make it enforceable, that provision should be modified to the minimum extent necessary to make it enforceable. Otherwise, that provision should be removed.\n\nYou may not assign your agreement with the company. The company may assign your agreement to any affiliate of the company, any other company that obtains control of the company, or any other company that buys assets of the company related to the forum. Any attempted assignment against these terms has no legal effect.\n\nNeither the exercise of any right under this Agreement, nor waiver of any breach of this Agreement, waives any other breach of this Agreement.\n\nThese terms embody all the terms of agreement between you and the company about use of the forum. These terms entirely replace any other agreements about your use of the forum, written or not.\n\n<h2 id=\"heading--contact\"><a href=\"#heading--contact\">Contact</a></h2>\n\nYou may notify the company under these terms, and send questions to the company, at <<ins>contact_email</ins>>.\n\nThe company may notify you under these terms using the e-mail address you provide for your account on the forum, or by posting a message to the homepage of the forum or your account page.\n\n<h2 id=\"heading--changes\"><a href=\"#heading--changes\">Changes</a></h2>\n\nThe company last updated these terms on July 12, 2018, and may update these terms again. The company will post all updates to the forum. For updates that contain substantial changes, the company agrees to e-mail you, if you've created an account and provided a valid e-mail address. The company may also announce updates with special messages or alerts on the forum.\n\nOnce you get notice of an update to these terms, you must agree to the new terms in order to keep using the forum."
"Create topics here that don’t fit into any other existing category."
"Private category for staff discussions. Topics are only visible to admins and moderators."
